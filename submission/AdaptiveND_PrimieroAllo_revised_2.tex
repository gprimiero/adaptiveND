\documentclass[]{article}

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{syntax}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathpartir}
\usepackage{bussproofs}
\usepackage{wasysym}
\usepackage{xcolor}
\usepackage{mdwlist}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand{\TurnADND}[2]
    { {#1}\vdash_{\textbf{\sf AdaptiveND}}  {#2}}

\newcommand{\TurnMinusn}[2]
            { {#1}\vdash_{\textbf{\sf s-n}}  {#2}}

\newcommand{\Turn}[2]
    { {#1}\vdash_{\textbf{\sf s}}  {#2}}
    \newcommand{\TurnNext}[2]
        { {#1}\vdash_{\textbf{\sf s+1}}  {#2}}

            \newcommand{\TurnMinusOne}[2]
                    { {#1}\vdash_{\textbf{\sf s-1}}  {#2}}

\newcommand{\TurnNextn}[2]
        { {#1}\vdash_{\textbf{\sf s+n}}  {#2}}
\newcommand{\TurnNextm}[2]
        { {#1}\vdash_{\textbf{\sf s+m}}  {#2}}
\newcommand{\TurnNextmPrime}[2]
        { {#1}\vdash_{\textbf{\sf s'+m'}}  {#2}}
\newcommand{\TurnNextnn}[2]
        { {#1}\vdash_{\textbf{\sf s+n+1}}  {#2}}
\newcommand{\TurnNextNext}[2]
    { {#1}\vdash_{\textbf{\sf s+2}}  {#2}}

\newcommand{\TurnNextNextNext}[2]
    { {#1}\vdash_{\textbf{\sf s+3}}  {#2}}

\newcommand{\TurnPrime}[2]
    { {#1}\vdash_{\textbf{\sf s'}}  {#2}}

\newcommand{\TurnPrimePrime}[2]
    { {#1}\vdash_{\textbf{\sf s''}}  {#2}}

    \newcommand{\TurnPrimePrimePlusOne}[2]
        { {#1}\vdash_{\textbf{\sf s''+1}}  {#2}}

        \newcommand{\TurnPrimePrimePlusTwo}[2]
            { {#1}\vdash_{\textbf{\sf s''+2}}  {#2}}


\newcommand{\TurnOne}[2]
    { {#1}\vdash_{\textbf{\sf 1}}  {#2}}
\newcommand{\TurnMarked}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox}}  {#2}}

\newcommand{\TurnMarkedREL}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox R}}  {#2}}
\newcommand{\TurnMarkedMA}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox MA}}  {#2}}
\newcommand{\TurnMarkedMAflex}[3]
    { {#2}\vdash_{\textbf{\sf {#1}\XBox MA}}  {#3}}
\newcommand{\TurnPlusOneMarkedMA}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox MA}}  {#2}}


\newcommand{\TurnChecked}[2]
    { {#1}\vdash_{\textbf{\sf \checked}}  {#2}}
\newcommand{\TurnMarkedNext}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox}}  {#2}}
\newcommand{\TurnMarkedprime}[2]
    { {#1}\vdash_{\textbf{\sf s'\XBox}}  {#2}}
    \newcommand{\TurnPrimePrimeMarked}[2]
        { {#1}\vdash_{\textbf{\sf s''\XBox}}  {#2}}


\newcommand{\TurnMaxPlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s')+1}}  {#2}}

\newcommand{\TurnMaxTwoPlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s')+1}} {#2}}

\newcommand{\TurnMaxThreePlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s',s'')+1}}  {#2}}

\newcommand{\TurnMarkedNextREL}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox R}}  {#2}}
\newcommand{\TurnMarkedNextNextREL}[2]
    { {#1}\vdash_{\textbf{\sf s+n+1\XBox R}}  {#2}}
\newcommand{\TurnMaxPlusOneREL}[2]
    { {#1}\vdash_{\textbf{\sf max(s,s')+1\XBox R}}  {#2}}

\newcommand{\TurnMarkedNextMA}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox MA}}  {#2}}
\newcommand{\TurnMarkedNextNextMA}[2]
    { {#1}\vdash_{\textbf{\sf s+n+1\XBox MA}}  {#2}}
\newcommand{\TurnMaxPlusOneMA}[2]
        { {#1}\vdash_{\textbf{\sf max(s,s')+1\XBox MA}}  {#2}}
        \newcommand{\TurnMaxTwoPlusOneMA}[2]
                { {#1}\vdash_{\textbf{\sf max(s,s',s'')+1\XBox MA}}  {#2}}


%\newcommand{\TurnOne}[2]
%   { {#1}\vdash_{\textbf{\sf 1}}  {#2}}
\newcommand{\TurnTwo}[2]
    { {#1}\vdash_{\textbf{\sf 2}}  {#2}}
\newcommand{\TurnThree}[2]
    { {#1}\vdash_{\textbf{\sf 3}}  {#2}}
\newcommand{\TurnFour}[2]
    { {#1}\vdash_{\textbf{\sf 4}}  {#2}}
\newcommand{\TurnFive}[2]
    { {#1}\vdash_{\textbf{\sf 5}}  {#2}}
\newcommand{\TurnSix}[2]
    { {#1}\vdash_{\textbf{\sf 6}}  {#2}}

\newcommand{\TurnSeven}[2]
    { {#1}\vdash_{\textbf{\sf 7}}  {#2}}

\newcommand{\TurnMarkedSevenREL}[2]
    { {#1}\vdash_{\textbf{\sf 7\XBox R}}  {#2}}

\newcommand{\TurnMarkedEightREL}[2]
    { {#1}\vdash_{\textbf{\sf 8\XBox R}}  {#2}}


\newcommand{\TurnEight}[2]
    { {#1}\vdash_{\textbf{\sf 8}}  {#2}}


\newcommand{\TurnMarkedFiveMA}[2]
    { {#1}\vdash_{\textbf{\sf 5\XBox MA}}  {#2}}
\newcommand{\TurnMarkedEightMA}[2]
    { {#1}\vdash_{\textbf{\sf 8\XBox MA}}  {#2}}

\newcommand{\TurnNine}[2]
    { {#1}\vdash_{\textbf{\sf 9}}  {#2}}


\newcommand{\TurnTen}[2]
    { {#1}\vdash_{\textbf{\sf 10}}  {#2}}

\newcommand{\TurnEleven}[2]
        { {#1}\vdash_{\textbf{\sf 11}}  {#2}}


\newcommand{\TurnMarkedElevenREL}[2]
    { {#1}\vdash_{\textbf{\sf 11\XBox R}}  {#2}}

\newcommand{\TurnTwelve}[2]
        { {#1}\vdash_{\textbf{\sf 12}}  {#2}}

\newcommand{\TurnMarkedTwelveREL}[2]
    { {#1}\vdash_{\textbf{\sf 12\XBox R}}  {#2}}

\newcommand{\TurnMarkedThirteenREL}[2]
{ {#1}\vdash_{\textbf{\sf 13\XBox R}}  {#2}}


\newcommand{\TurnThirteen}[2]
        { {#1}\vdash_{\textbf{\sf 13}}  {#2}}

\newcommand{\TurnFourteen}[2]
        { {#1}\vdash_{\textbf{\sf 14}}  {#2}}

\newcommand{\TurnFifteen}[2]
        { {#1}\vdash_{\textbf{\sf 15}}  {#2}}


\newcommand{\TurnSixteen}[2]
        { {#1}\vdash_{\textbf{\sf 16}}  {#2}}

\newcommand{\TurnSeventeen}[2]
        { {#1}\vdash_{\textbf{\sf 17}}  {#2}}


\newcommand{\TurnEighteen}[2]
        { {#1}\vdash_{\textbf{\sf 18}}  {#2}}

\newcommand{\TurnMarkedEighteenREL}[2]
    { {#1}\vdash_{\textbf{\sf 18\XBox R}}  {#2}}

\newcommand{\TurnNineteen}[2]
        { {#1}\vdash_{\textbf{\sf 19}}  {#2}}

\newcommand{\TurnTwenty}[2]
{ {#1}\vdash_{\textbf{\sf 20}}  {#2}}

\newcommand{\TurnTwentyOne}[2]
{ {#1}\vdash_{\textbf{\sf 21}}  {#2}}

\newcommand{\TurnTwentyTwo}[2]
{ {#1}\vdash_{\textbf{\sf 22}}  {#2}}

\newcommand{\TurnTwentyThree}[2]
{ {#1}\vdash_{\textbf{\sf 23}}  {#2}}

\newcommand{\TurnTwentyFour}[2]
{ {#1}\vdash_{\textbf{\sf 24}}  {#2}}

\newcommand{\TurnTwentyFive}[2]
{ {#1}\vdash_{\textbf{\sf 25}}  {#2}}

\newcommand{\Sf}{\ensuremath{\mathrm{Sf}}}
\newcommand{\At}{\ensuremath{\mathrm{At}}}
\newcommand{\Unrel}{\ensuremath{\mathsf{UnRel}}}

%\newcommand{\TurnT}[2]
%   { \Delta_0;{#1}\vdash  {#2}}
%\newcommand{\TurnTT}[2]
%   { \Delta_0;{#1}\vdash_{\sf JC_1}  {#2}}
%\newcommand{\Turnj}[1]
%   { \Delta_0\vdash_{\sf J_0}  {#1}}
%\newcommand{\Turnjc}[3]
%    { {#1};{#2}\vdash_{\textbf{\sf JC}}  {#3}}


%opening
\title{Annotated Natural Deduction for Adaptive Reasoning}
\author{Patrick Allo\\
Oxford Internet Institute\\
University of Oxford\\
 \and Giuseppe Primiero\\
 Department of Computer Science\\
 Middlesex University London}
\date{}


\begin{document}

\maketitle

\begin{abstract}
We present a multi-conclusion natural deduction calculus characterizing the dynamic reasoning typical of Adaptive Logics. The resulting system {\sf AdaptiveND} is sound and complete with respect to the propositional fragment of adaptive logics based on  \textbf{CLuN}. This appears to be the first tree-format presentation of the standard linear dynamic proof system typical of Adaptive Logics. It offers the advantage of full transparency in the formulation of locally derivable rules, a connection between restricted inference rules and their adaptive counterpart, and the formulation of abnormalities as a subtype of well-formed formulas. These features of the proposed calculus allow us to clarify the relation between defeasible and multiple-conclusion approaches to classical recapture.
\end{abstract}

\section{Introduction}

In this paper we outline a multiple-conclusion natural deduction calculus in which the dynamics of standard (Fitch-style) dynamic proofs of Adaptive Logics \cite{batens07} can be reconstructed. Adaptive logics are a family of logics that can be used to formalise a wide range of defeasible reasoning forms. Their consequence-relations rely on the standard idea of interpreting premises as normally as possible through the selection of models of its premises, but it is only at the level of its proof-theory that its distinctive approach comes to the fore. Adaptive logics, namely, reconstruct defeasible reasoning patterns as dynamic proofs; proofs in which steps performed earlier may later be retracted when the assumptions they were based on no longer hold.
%The specific system we describe here is for
In particular, an inconsistency adaptive logic
%: this is a logic that
captures paraconsistent reasoning avoiding triviality in the face of inconsistency, while trying to make up for its deductive weakness by provisionally applying classical inference-rules when there is no explicit indication that inconsistencies are involved in that inference.

The dynamics of retracting earlier lines in a proof can be captured in a rather natural way in linear proof-formats, including standard axiomatic and Fitch-style natural deduction proofs, but is much less straightforward in a tree-like proof-format. Consider, for instance, the following retraction in an application of \emph{Ex Contradictione Quodlibet}:

\begin{figure}[ht!]
\centering
    \begin{tabular}{cllcl}
        (1) & $p$ & Prem & $\emptyset$\\
        (2) & $p \vee q$ & Addition & $\emptyset$\\
        (3) & $\neg p$ & Prem & $\emptyset$\\
        (4) & $q$ & DS & $\{p\}$ & $\XBox^5$\\
        (5) & $p \wedge \neg p$ & Adjunction & $\emptyset$
    \end{tabular}
\end{figure}
\noindent Here, at line (4) disjunctive syllogism (DS) is applied on the condition that $p$ behaves normally, i.e.\ that the contradiction $p \wedge \neg p$ hasn't been derived. When this contradiction is effectively derived at line (5), line (4) is marked (here and in the following by $\XBox$) and is from then on no longer assumed to be part of the proof. This type of reasoning illustrates the idea of provisional applications of classical inference-rules to paraconsistent logics that reject the disjunctive syllogism, but in which the restricted form $\phi \vee \psi, \neg \phi / \psi \vee (\phi \wedge \neg \phi)$ is retained.

Contrast this, now, with the following attempt to reconstruct a similar reasoning-process in a Gentzen-Prawitz-style proof-tree:
%\footnote{In the proposed system we will not make use of the standard discharging method for assumptions in the marking of derivation judgements.}

\begin{prooftree}
    \AxiomC{$\Gamma \vdash p$}
    \RightLabel{$\vee$I}
    \UnaryInfC{$\Gamma \vdash p \vee q$}
    \AxiomC{$\Gamma \vdash \neg p$}
    \AxiomC{$\Gamma \not\vdash p \wedge \neg p$}
    \LeftLabel{DS$^*$}
    \TrinaryInfC{$\Gamma \vdash q$}
    \AxiomC{$\Gamma \vdash p$}
    \AxiomC{$\Gamma \vdash \neg p$}
    \RightLabel{$\wedge$I}
    \BinaryInfC{$\Gamma \vdash p \wedge \neg p$}
    \BinaryInfC{?}
\end{prooftree}
When in this proof an explicit contradiction is derived in the right-hand branch, the assumption of its invalidity (stated explicitly in the left-hand branch) no longer holds. In this format, however, the order used to construct the proof cannot be read off the proof itself (an issue that could easily be fixed). But also, more importantly, it isn't even clear what it might mean to retract the line where $q$ is derived, since the result of removing that line from the proof is in itself no longer a well-formed proof.

The proof-format we propose solves this problem by making two changes: first, we add indices to judgements to keep track of stages in the construction of a proof; and second, we exploit the fact that judgements that are `marked' at a certain stage do not have to be removed, because there is simply no need to prevent their implicit re-use since every assumption or premise should explicitly be written down in the place it is used. Instead, it is the derivation of the same judgement at a later stage that is (or may be) blocked, because the original assumption that led to its initial derivation probably no longer holds. We therefore provide, for the first time, an appropriate Natural Deduction translation of adaptive reasoning, whose proofs have so far always been presented in a linear format. We do so by formulating a general approach to Adaptive Reasoning which accommodates both strategies standardly used to retract previously made judgements. While the reformulation of the Reliability Strategy is a much easier task, Minimal Abnormality is a more daunting one given the complexity of its procedural translation in a proof tree. Nonetheless, we show that it can in principle be done within our logic, although more efficient procedures might be devised.

Because this system uses multiple-conclusion judgements, it also explicitly captures the connection between unconditional derivations of certain disjunctions in the paraconsistent logic and the conditional deductions of one of their disjuncts in the adaptive logic. Moreover, the choice of modelling inconsistency adaptive reasoning brings us closer to the original motivations for the development of adaptive logic \cite{Batens:ParaconsistentLogicEssaysOnTheInconsistent:1989}, but also allows us to engage with current philosophical debates of relevance to Graham Priest's work and in particular how one should best approach the question of \textit{classical recapture} in paraconsistent logics. The latter problem can be summarised as follows. When one adopts a logic that is strictly weaker than classical logic, the question of how one should account for epistemically useful classical inference-forms that are invalidated by one's preferred logic almost immediately arises. In the case of paraconsistent logic, this question is often deemed urgent, as the practical and epistemic usefulness of the inference-forms that are lost, like the disjunctive syllogism, is almost undisputed. Inconsistency-adaptive logics present one possible answer to this challenge under the form of defeasible inference-forms that allow one to use classical inference-steps on the condition that certain assumptions are not violated. It is also a response that Graham has endorsed \cite{GP:LPm} \cite[Ch.\ 16]{Priest:InContradiction:2006}. His specific proposal on how this should be implemented has, in recent years, become the focus of a renewed interest in the problem of how dialetheists should account for classical recapture. We contend that the combination of a multiple-conclusion calculus with the reconstruction of the defeasible dynamics of adaptive proofs can further clarify this debate.

The paper is structured as follows. We introduce in Section \ref{sec:lower} a basic natural deduction system called {\sf minimalND}, which acts as the Lower Limit Logic of our adaptive system. In Section \ref{sec:adaptive}, we extend the system to account for adaptive reasoning through the definition of an appropriate abnormal form of expressions and appropriate adaptive rules; the new system is called {\sf AdaptiveND}. In Section \ref{sec:marking} we define marking strategies to identify derivation steps that can no longer be assumed to hold in the tree. In Section \ref{sec:meta} we define basic meta-theoretical properties. We return to comment on the challenge of classical recapture in Section \ref{sec:recap}.


\section{{\sf minimalND}}\label{sec:lower}

We start by defining the type universe for the $\{\neg, \rightarrow, \wedge, \vee\}$ fragment of intuitionistic propositional logic corresponding to minimal logic. We call this logic {\sf minimalND} and use it as the equivalent of a Lower Limit Logic---the paraconsistent logic that governs the unconditional steps in a proof. Contrary to what is standard in an intuitionistic setting, we do not allow the deduction of $\bot$ from an explicit contradiction. Whereas $\bot$ can be eliminated via \emph{Ex Falso Quodlibet}, there is no introduction-rule for $\bot$, and this is what makes our base-logic paraconsistent. It is only when the assumption of consistency is introduced that the connection between negation-inconsistency and absolute inconsistency can provisionally be recreated.

We start by defining the syntax of our language:

\begin{definition}[{\sf minimalND}]

 Our starting language for {\sf minimalND} is defined by the following grammar:

\begin{displaymath}
\begin{array}{l}
{\sf Type}:={\sf Prop}\\
{\sf Prop}:= A \mid \bot \mid \neg \phi \mid \phi_{1} \rightarrow \phi_{2} \mid \phi_{1} \wedge \phi_{2} \mid \phi_{1} \vee \phi_{2}\\
\Gamma := \{\phi_{1}, \dots, \phi_{n}\}\\
\Delta := \{\phi_{1}, \dots, \phi_{n}\}

\end{array}
\end{displaymath}
\end{definition}

%
The type universe of reference is the set of propostions {\sf Prop}, construed by atomic formulas closed under negation, implication, conjunction, disjunction and allowing $\bot$ to express absolute contradictions. Formula formation rules are given in Figure \ref{fig:formulaconstructions}.

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=Atom] { } {A \in {\sf Prop}}
\and
\infer*[right=$\bot$] { } {\bot \in {\sf Prop}}
\and
\infer*[right=$\neg$] {\phi \in {\sf Prop} } {\neg \phi \in {\sf Prop}}
\and
\infer*[right=$\rightarrow$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\rightarrow\phi_2\in {\sf Prop}}
\and
\infer*[right=$\wedge$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\wedge\phi_2\in {\sf Prop}}
\and
\infer*[right=$\vee$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\vee\phi_2\in {\sf Prop}}
\end{mathpar}
\caption{Formula Formation Rules}\label{fig:formulaconstructions}
\end{figure}



\begin{definition}[Judgements]
A multiple conclusion {\sf minimalND}-judgement is of the form $\Gamma;\cdot \vdash_{\sf s} \Delta$, where: $\Gamma$ is the usual set of assumptions, $\Delta$ is a set of formulas of the language and {\sf s} is a positive integer.
\end{definition}
The set $\Gamma$ on the left-hand side of the derivability sign is to be read conjunctively. Similarly for the semi-colon symbol, which is introduced here but is only used in Section \ref{sec:adaptive} to separate standard assumptions in $\Gamma$ from conditions (in the adaptive sense). At this stage, the symbol $\cdot$ following the semi-colon is used to express an empty set of adaptive conditions. The set $\Delta$ and the comma (if it occurs) on the right-hand side of the derivability sign are both to be read disjunctively. This characterizes our calculus as multiple-conclusion. Standard context formation rules are, as usual in a proof-theoretic setting, inductively given for both left and right-hand side set of formulas, as shown in Figure \ref{fig:contextrules}, see e.g.\ \cite[pp.5-6]{Troelstra:2000:BPT:351148} and \cite[sec.2.3]{pfenning} for a formulation closer to ours. {\sf Nil} establishes the base case of a valid empty context, we use {\sf wf} as an abbreviation for `well-formed'; $\Gamma${\sf -Formation} allows extension of contexts by propositions; {\sf Prem} establishes derivability of formulas contained in context and this rule, in particular, defines the equivalent of the adaptive Premise rule.


\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=Nil] { } {\cdot\Turn {} {\sf wf}}
\and
\infer*[right=$\Gamma$-formation] {{\Turn {\Gamma; \cdot} {\sf wf} } \\ {\phi \in {\sf Prop}}} {\TurnNext {\Gamma , \phi; \cdot} {\sf wf}}
\end{mathpar}


\begin{mathpar}
\infer*[right=Prem] {{\Turn {\Gamma; \cdot} {\sf wf}}\\ {\phi \in \Gamma}}{\TurnNext {\Gamma; \cdot} {\phi}}
\end{mathpar}
\caption{Context Formation Rules}\label{fig:contextrules}
\end{figure}

The derivability sign is enhanced with a signature {\sf s} that corresponds to a counter of the ordered derivation steps executed to obtain the corresponding ND-formula in a tree. This annotation only comes to use in the next extension of the calculus in Section \ref{sec:adaptive}.

The semantics of connectives is given in the standard proof-theoretic way by Introduction and Elimination Rules in Figure \ref{fig:connectives}. Introduction of $\rightarrow$ corresponds to conditional proof, while its elimination formalises Modus Ponens. Rules for $\wedge$ are standard; notice that $\vee$-Elimination makes the disjunctive reading of the comma on the right hand-side of the turnstile explicit. $\bot$ can be eliminated by \emph{Ex Falso}, but cannot be introduced. Dually, our paraconsistent negation $\neg$ can be introduced, but not eliminated.
%
%
\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=$\rightarrow$I] {\Turn {\Gamma, \phi_1; \cdot} {\Delta, \phi_2}} {\TurnNext {\Gamma; \cdot} {\Delta, \phi_1\rightarrow \phi_2}}
\and
\infer*[right=$\rightarrow$E] {\Turn {\Gamma; \cdot} {\Delta, \phi_1\rightarrow\phi_2}\\{\TurnPrime {\Gamma';\cdot} {\Delta', \phi_1}}} {\TurnMaxPlusOne {\Gamma; \Gamma'} {\Delta, \Delta', \phi_2}}
\end{mathpar}

\begin{mathpar}
\infer*[right=$\wedge$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_1}\\{\TurnPrime {\Gamma'; \cdot} {\Delta', \phi_2}}} {\TurnMaxPlusOne {\Gamma, \Gamma';\cdot} {\Delta, \Delta', \phi_1\wedge \phi_2}}
\and
\infer*[right=$\wedge$E]
{\Turn {\Gamma;\cdot} {\Delta, \phi_1\wedge\phi_2}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_{i \in \{1,2\}}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=$\vee$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_1}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_1\vee \phi_2}}
\and
\infer*[right=$\vee$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_2}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_1\vee \phi_2}}
\and
\infer*[right=$\vee$E]
{\Turn {\Gamma;\cdot} {\Delta, \phi_1\vee\phi_2}}{\TurnNext {\Gamma;\cdot} {\Delta, \phi_{1},\phi_{2}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=$\bot$E] {\Turn {\Gamma; \cdot}{\Delta, \bot} }{\Turn {\Gamma;\cdot} {\Delta, \phi}}
%\end{mathpar}
\and
%\begin{mathpar}
\infer*[right=$\neg$I] {\Turn {\Gamma; \phi}{\Delta, \psi}}{\TurnNext {\Gamma; \cdot}{\Delta, \psi, \neg \phi}}
\end{mathpar}

\caption{Rules for I/E of connectives}\label{fig:connectives}
\end{figure}

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=Wl] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}} } {\TurnNext {\Gamma, \phi_{2}; \cdot} {\Delta, \phi_{1}}}
\and
\infer*[right=Cl] {\Turn {\Gamma, \phi_{1}, \phi_{1}; \cdot} {\Delta, \phi_{2}} } {\TurnNext {\Gamma, \phi_{1}; \cdot} {\Delta, \phi_{2}}}
\and
\infer*[right=El] {\Turn {\Gamma, \phi_{1}, \phi_{2}; \cdot} {\Delta, \phi_{3}} } {\TurnNext {\Gamma, \phi_{2}, \phi_{1}; \cdot} {\Delta, \phi_{3}}}
\end{mathpar}
\begin{mathpar}

\infer*[right=Cut] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}} \\ {\TurnPrime {\Gamma', \phi_{1}; \cdot} {\Delta', \phi_{2}}}} {\TurnMaxPlusOne {\Gamma; \Gamma'; \cdot} {\Delta, \Delta', \phi_{2}}}
\end{mathpar}
\begin{mathpar}

\infer*[right=Cr] {\Turn {\Gamma; \cdot} {\Delta, \phi, \phi}} {\TurnNext {\Gamma; \cdot} {\Delta, \phi} }
\and
\infer*[right=Er] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}, \phi_{2}} } {\TurnNext {\Gamma; \cdot} {\Delta, \phi_{2}, \phi_{1}}}
\end{mathpar}
\caption{Structural Rules}\label{fig:structural}
\end{figure}

Finally, we introduce  in Figure \ref{fig:structural} a set of rules to enforce structural properties.  {\sf WL} is a Weakening on the left-hand side of the judgement: it allows the monotonic extension of assumptions preserving already derivable formulas. Notice that this rule requires an empty set of formulas $; \cdot$ following $\Gamma$. As will become clear in the next section, this means that weakening is only valid when the set of \textit{adaptive conditions} is empty, that is, when no provisional assumptions are made that depend on the premises. We do not need to formulate a {\sf WR} rule for weakening of the set $\Delta$ of derivable formulas, as this can be obtained by a detour of $\vee$-Introduction and Elimination. {\sf CL} for Contraction on the left allows elimination of repeated assumptions and {\sf EL} for Exchange on the left is valid just by set construction, as there is no order. {\sf CR} and {\sf ER} do a similar job on the right-hand side of the judgement. Finally, {\sf Cut} (also known as {\sf Substitution} in some Natural Deduction Calculi) guarantees that derivations can be pasted together, and in general it requires that there are no clashes of free variables in $\Gamma, \Gamma'$.

The resulting system is equivalent to the propositional fragment of \textbf{CLuN}, the logic obtained by adding Excluded Middle to the positive fragment of classical logic. This is a very weak paraconsistent (but not paracomplete) logic that does not validate any of the usual De Morgan rules \cite{Batens:LogiqueAnalyse:1980}, and which has been used as the Lower Limit Logic of one of the first adaptive logics.

\begin{theorem}\label{thm:clun}
    {\sf minimalND} is sound and complete w.r.t. to the propositional fragment of \textbf{CLuN}.
\end{theorem}
\noindent\textsl{Proof.} Soundness can be shown as usual, with the key step verifying that ($\neg$I) is sound in view of the completeness-clause for negation
\[
   \text{If } v(\phi) = \mathrm{False} \text{, then } v(\neg \phi) = \mathrm{True}\tag{C$\neg$}\label{eq:negclause}
\]
Completeness follows from the provability of all \textbf{CLuN}-axioms. Below, we only give the proofs for Excluded Middle and Peirce's Law.
    \begin{mathpar}
    \infer*[left=Cr]
        {\infer*[right=$\vee$I]
            {\infer*[right=$\vee$I]
                {\infer*[right=$\neg$I]
                    {\infer*[left=Prem]
                        { }
                        {\TurnOne {p; \cdot} {p}}}
                    {\TurnTwo {\emptyset; \cdot} {p, \neg p}}}
                {\TurnThree {\emptyset; \cdot} {p \vee \neg p, \neg p}}}
            {\TurnFour {\emptyset; \cdot} {p \vee \neg p, p \vee \neg p}}}
        {\TurnFive {\emptyset; \cdot} {p \vee \neg p}}

    \and

    \infer*[right=$\to$I]
        {\infer*[right=Cr]
            {\infer*[right=$\to$E]
                {\infer*[left=$\to$I]
                    {\infer*[left=Wr]
                        {\TurnOne {p;\cdot} {p}}
                        {\TurnTwo {p;\cdot} {p, q}}}
                    {\TurnThree {;\cdot} {p, p \to q}} \\
                \infer*[right=Prem]
                    { }
                    {\TurnFour {(p \to q) \to p; \cdot} {(p \to q) \to p}}}
                {\TurnFive {(p \to q) \to p; \cdot} {p, p}}}
            {\TurnSix {(p \to q) \to p; \cdot} {p}}}
        {\TurnSeven { ;\cdot} {((p \to q) \to p) \to  p}}
    \end{mathpar}
\qed


\section{{\sf AdaptiveND}}\label{sec:adaptive}


We now extend {\sf minimalND} to characterize a new logic called {\sf AdaptiveND} to allow for inconsistency adaptive reasoning. To this aim one needs:
%
\begin{enumerate}
\item the explicit formulation of an $\Omega$ set of propositions;
\item the formulation of judgements including an \textit{adaptive condition};
\item the formulation of a rule that allows the derivation of new formulas independent from such an adaptive condition;
\item the formulation of a rule that allows the derivation of new formulas that depend on such an adaptive condition.
\end{enumerate}
%
We offer accordingly new definitions for the syntax of this logic and the related form of judgements.

\begin{definition}[{\sf AdaptiveND}]
The language of {\sf AdaptiveND} is as follows:


\begin{displaymath}
\begin{array}{l}
{\sf Type}:={\sf Prop}\\
{\sf Prop}:= A \mid \bot \mid \neg \phi \mid \phi_{1} \rightarrow \phi_{2} \mid \phi_{1} \wedge \phi_{2} \mid \phi_{1} \vee \phi_{2}\\
\Gamma := \{\phi_{1}, \dots, \phi_{n}\}\\
\Delta := \{\phi_{1}, \dots, \phi_{n}\}\\
\Omega := \{\phi \wedge \neg \phi\mid \phi\in Prop\}\\
\end{array}
\end{displaymath}
\end{definition}




\begin{definition}[Judgements]
An {\sf AdaptiveND}-judgement is of the form $\Gamma; \Theta^{-}\vdash_{s} \Delta$, where:

\begin{enumerate}
\item the left-hand side of $\vdash_{\sf s}$ has $\Gamma$ as in {\sf minimalND};
\item the semicolon sign on the left-hand side of $\vdash_{\sf s}$ is conjunctive;
\item $\Theta$ refers to a finite subset of $\Omega$, i.e. a set of formulas of a specific inconsistent logical form; we write $\phi$ instead of $\{\phi\}$ when $\Theta$ is the singleton $\{\phi\}$; below we introduce an appropriate $\Omega$-formation rule;
\item the last place of the left-hand side context is always reserved for negated formulas of type $\Omega$; we shall use $\phi^{-}$ to refer to the negation of $\phi$, and $\Theta^-$ for $\{\phi^- \mid \phi \in \Theta\}$;
\item the right-hand side is in disjunctive form.
\end{enumerate}
\end{definition}
%
When the second place on the left-hand side of $\vdash$ is empty, we shall write $\Gamma;\cdot\vdash$, thus reducing to the form of a {\sf minimalND}-judgement.
Moreover, in {\sf AdaptiveND}, the annotation on the proof stage {\sf s} is optionally followed by one of the following two marks:

\begin{itemize}
    \item[]  $\XBox$ to mark that at the current stage some previously derived formula is retracted; the meaning of this annotation is given for the Reliability Strategy by a corresponding rule \XBox R, presented in Section \ref{sec:markrel}; for the Minimal Abnormality Strategy, its meaning is given by two rules \XBox M1 and \XBox M2, presented in Section \ref{sec:ma};
\item[] $\checked$ to mark that at the current stage some previously derived formula is now finally derived, i.e. will no longer be marked by \XBox; the use of this annotation is formally given below in Definition \ref{def:finalder}.
\end{itemize}

We now introduce the rules for {\sf AdaptiveND}. In Figure \ref{fig:omega}, we describe the formation and use of formulas $\phi \in \Omega$. By $\Omega${\sf-Formation}, the explicit contradiction $\phi \wedge \neg \phi$, with $\phi$ any proposition, is a formula of the $\Omega$ type. In the Adaptive tradition a formula of type $\Omega$ is called an \textit{abnormality} or \textit{abnormal formula}. By {\sf Adaptive Condition Formation}, given a valid context $\Gamma$ and a formula
$\phi$ of the $\Omega$ type, a context $\Gamma$ followed by the Adaptive Condition that expresses the defeasible assumption that $\phi$ \textit{is false}, is a well-formed context. This corresponds to the use of syntactic restrictions that are applied to the use of conditions as additional elements of a proof line in the standard linear format of adaptive logics. By {\sf Adaptive Condition Extension}, a newly constructed formula of type $\Omega$ can be added to an existing non-empty Adaptive Condition.

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=$\Omega$-formation] {{\phi \in {\sf Prop}}}{(\phi \wedge \neg \phi) \in \Omega}
\and
\infer*[right=Adaptive Condition-formation] {{\Turn {\Gamma;\cdot} {\sf wf} } \\ {\phi \in {\sf \Omega}}} {\TurnNext {\Gamma ; \phi^{-}} {\sf wf}}
\and
\infer*[right=Adaptive Condition-extension] {{\Turn {\Gamma;\Theta^-} {\sf wf} } \\ {\phi \in {\sf \Omega}}} {\TurnNext {\Gamma ; (\Theta \cup \{\phi\})^-} {\sf wf}}
\end{mathpar}
\caption{$\Omega$ Formation rules}\label{fig:omega}
\end{figure}


In Figure \ref{fig:adaptiverules}, the calculus is extended by introducing the conditional rule {\sf RC}, which states that if a disjunction $\psi,\phi$ is derivable from $\Gamma$, with $\phi$ an abnormal formula, then $\psi$ can also be derived alone under $\Gamma$ and the Adaptive Condition that $\phi$ be false. Because the application of {\sf RC} can be delayed by keeping formulae of type $\Omega$ on the right hand-side of the turnstile, the role of the unconditional rules of the standard calculus is subsumed under the {\sf Cut} rule. The single and multi-premise versions of the unconditional rules displayed in Figure \ref{fig:adaptiverules2} can thus be treated as derived rules as shown by the procedures for rewriting a succession of RC and RU applications as a succession of Cut and RC applications in Figure \ref{fig:condasderived}.

The marking of a judgement in an \textsf{AdaptiveND} proof signals that the marked judgement cannot be used as a premise for any other rule. In that sense, the marking of judgements express a dead-end in a proof-tree. To preserve information about retracted judgements within a proof, and to facilitate the practice of cutting and pasting proofs together without having to renumber the judgements, we extend the standard formalisation of proofs as trees, and define adaptive proofs as sequences of proof-trees.

\begin{definition}[Proof Tree]
A well-formed {\sf AdaptiveND} tree is a finite proof tree obtained by deriving {\sf AdaptiveND} judgements from other {\sf AdaptiveND} judgements where
\begin{enumerate}

\item the top leaves of the tree are instances of the {\sf Prem} rule and
\item each next step is obtained by applying one of the {\sf minimalND} proof rules or one of the {\sf AdaptiveND} proof rules.
\end{enumerate}
\end{definition}

\begin{definition}[Adaptive Proof]
An {\sf AdaptiveND} proof is a sequence $\langle T_i\rangle_{i \in I}$ of {\sf AdaptiveND} trees with each $i \in I$ equal to the highest numbered judgement in $T_i$.
\end{definition}

As a notational short-hand, we will sometimes include marked judgements as unused premises to incorporate dead-ends in a single proof-tree, and include final judgements of an earlier proof tree as top leaves of a new proof tree.

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=RC] {\Turn {\Gamma;\Theta^{-}} {\psi,\phi}\\ {\phi \in \Omega}} {\TurnNext {\Gamma; (\Theta\cup\{\phi\})^{-}} {\psi}}
\end{mathpar}

\caption{Conditional Rule}\label{fig:adaptiverules}
\end{figure}

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right=RU] {\Turn {\Gamma; \Theta^{-}} {\phi_{1}}\\ {\TurnPrime {\phi_{1};\cdot} {\phi_{2}}}}
{\TurnMaxTwoPlusOne {\Gamma; \Theta^{-}} {\phi_{2}}}
\and
\infer*[right=RU2] {\Turn {\Gamma; \Theta^-} {\phi_{1}}\\ {\TurnPrime {\Gamma';\Theta'^-} {\phi_2}} \\ {\TurnPrimePrime {\phi_1, \phi_2;\cdot} {\phi_3}}}
{\TurnMaxThreePlusOne {\Gamma, \Gamma'; (\Theta \cup \Theta')^-} {\phi_{3}}}
\end{mathpar}

\caption{Unconditional Rules}\label{fig:adaptiverules2}
\end{figure}

\begin{figure}
\centering
    \begin{mathpar}
        \infer*[right=RU]{\infer*[right=RC]{\TurnOne{\Gamma; \cdot}{\phi_1, \phi} \\ \phi \in \Omega}{\TurnTwo{\Gamma; \phi^-}{\phi_1}} \\ \TurnThree{\phi_1; \cdot}{\phi_2}}{\TurnFour{\Gamma; \phi^-}{\phi_2}}
    \end{mathpar}

$\Big\Downarrow$

\begin{mathpar}
\infer*[right=RC]{
                        \infer*[right=Cut]{\TurnOne {\Gamma; \cdot} {\phi_{1}, \phi}\\ {\TurnTwo {\phi_{1};\cdot} {\phi_{2}}}}
                        {\TurnThree {\Gamma; \cdot} {\phi_{2}, \phi} \\ \phi \in \Omega}
                        }
                         {\TurnFour {\Gamma; \phi^-}{\phi_2}}
    \end{mathpar}
\vspace{1cm}
\begin{mathpar}
    \infer*[right=RU2]{
        \infer*[right=RC]
        {\TurnOne{\Gamma; \cdot}{\phi_1, \phi} \\ \phi \in \Omega}
        {\TurnTwo{\Gamma; \phi^-}{\phi_1}}
        \\
        \infer*[right=RC]
        {\TurnThree{\Gamma'; \cdot}{\phi_2, \phi'} \\ \phi' \in \Omega}
        {\TurnFour{\Gamma'; \phi'^-}{\phi_2}}
        \\
        \TurnFive{\phi_1, \phi_2}{\phi_3}
    }{\TurnSix{\Gamma, \Gamma'; \{\phi, \phi'\}}{\phi_3}}
\end{mathpar}

$\Big\Downarrow$

    \begin{mathpar}
\infer*[right=RC]{
        \infer*[right=RC]{
        \infer*[right=Cut]{{\TurnFour {\Gamma; \cdot} {\phi_{1}, \phi}}
            \infer*[right=Cut]{\TurnOne {\Gamma;\cdot} {\phi_2, \phi'}\\ {\TurnTwo {\phi_1, \phi_2;\cdot} {\phi_3}}}
            {\TurnThree {\Gamma', \phi_1 ;\cdot}{\phi_3, \phi'}}
        }
        {\TurnFive {\Gamma, \Gamma'; \cdot} {\phi_{3}, \phi, \phi'} \\ \phi \in \Omega}}
    {\TurnSix {\Gamma, \Gamma'; \phi^-} {\phi_{3}, \phi'} \\ \phi' \in \Omega}}
{\TurnSeven {\Gamma, \Gamma'; \{\phi, \phi'\}^-} {\phi_{3}}}
    \end{mathpar}
    \caption{Redundancy of unconditional rules}\label{fig:condasderived}
\end{figure}

The Adaptive strategies developed in the next Section have the aim of establishing which abnormal formulas can no longer be safely considered as conditions in the application of the Conditional Rule {\sf RC}, in turn requiring the retraction of the previously derived formulas. To this aim, three elements need to be introduced:

\begin{enumerate}
\item minimal disjunctions of formulas of type $\Omega$, denoted by $\bigvee(\Delta)^{min}$, with $\Delta \subset \Omega$;
\item the union of all abnormalities that occur as a disjunct of some $\bigvee(\Delta)^{min}$ derived up to a certain stage from $\Gamma$, denoted by $\Unrel(\Gamma)$;
\item the set of choice sets of a set of sets $\Delta_1, \ldots, \Delta_n$, corresponding to all $\bigvee(\Delta_i)^{min}$ derived up to a certain stage from $\Gamma$, denoted by $\Phi(\Gamma)$.
\end{enumerate}

The rules in Figure \ref{fig:mindab} establish these three constructions.

Rule {\sf MinDab} says that a disjunctive formula of the $\Omega$ type derived at some stage {\sf s} of a derivation can be considered minimal at stage {\sf s'} if at no previous stage {\sf t} $<$ {\sf s'} a shorter one was derived under the same context $\Gamma$. Note that the superscript $min$ works as an annotation indicating that the $\Delta$ in the first premise of the {\sf MinDab} rule satisfies the side condition of the same rule, but it does not indicate that $\Delta^{min}$ and $\Delta$ are two different sets.

Rule {\sf UnRel} says that given minimal disjunctions of abnormalities, each derived according to {\sf MinDab} at stages {\sf s-n} up to {\sf s-1}, the union of all such formulas can be derived at stage {\sf s}, defined as follows:

\begin{definition}[Set of unreliable formulas]
$\Unrel(\Gamma)$ derived at stage {\sf s} from $\Gamma$ denotes the conjunction of the union of some $\Delta_{1}, \dots, \Delta_{n}$ derived from $\Gamma$ up to stage {\sf s-1} according to {\sf MinDab}.
\end{definition}
Keep in mind that $\Unrel(\Gamma)$ denotes a conjunction and not a set of formulae. As such, when it occurs on the right-hand side of the turnstile it should not be read as a disjunction of abnormalities (which is the intended reading for a set $\Delta$ of abnormalities in that position). The side condition for the Rule {\sf UnRel} ensures that each $\Delta_{i}^{min}$ used as a premise is still minimal at stage {\sf s-1}. Contrary to how unreliable formulae at a stage are defined in standard adaptive proofs, this rule does not require that all minimal disjunctions of abnormalities derived at a given stage need to be used as its premises, but the side-condition of this rule does refer to all disjunctions of abnormalities that occur at earlier stages of the proof.

Choice sets provide selections of abnormalities that might turn out to be true at a stage {\sf s}, defined as follows:

\begin{definition}[Choice set of $\{\Delta_{1}, \dots, \Delta_{n}\}$]
$\Phi(\Gamma)$ derived at stage {\sf s} from $\Gamma$ denotes the set of choice sets of some $\{\Delta_{1}, \dots, \Delta_{n}\}$ derived from $\Gamma$ up to stage {\sf s-1} according to {\sf MinDab}. We denote each such choice set with ${\sf choice_{i}}(\{\Delta_{1}, \dots, \Delta_{n}\})$.
\end{definition}

The choice set of an empty set of minimal disjunction of abnormalities is empty. This notion of choice set is procedurally implemented by the rule {\sf Choice} in Figure \ref{fig:mindab}, where again each premise must be intended as the conclusion of a {\sf MinDab} rule and the side condition ensures that each $\Delta_{i}^{min}$ used as a premise is still minimal at stage {\sf s-1}. Here too, it is not required to use all minimal $\Delta$'s that are already derived as premises. The Rule {\sf MinChoice} allows the selection of a \textit{minimal} choice set out of $\Phi(\Gamma)$, denoted by ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$.






\begin{figure}[t]
\begin{mathpar}
\infer*[right=MinDab] {\Turn {\Gamma; \cdot} {\Delta}\\ {\Delta \subset \Omega} \\ {\mbox { with no }\Delta'\subseteq \Delta \mbox{ s.t. } \Gamma; \cdot \vdash_{\sf t<s'} \Delta'}} {\TurnPrime {\Gamma; \cdot} {\Delta^{min}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=UnRel]{\TurnMinusn{\Gamma;\cdot}{\Delta_{1}^{min}}\\
\ldots\\
\TurnMinusOne{\Gamma;\cdot}{\Delta_{n}^{min}}\\ {\mbox { with no }\Delta_{i}^{min}\subset \Delta_{j}^{min}} }
{\Turn{\Gamma;\cdot}{\Unrel(\Gamma)}}
\end{mathpar}

\begin{mathpar}
\infer*[right=Choice]{\TurnMinusn{\Gamma;\cdot}{\Delta_{1}^{min}}\\
\ldots\\
\TurnMinusOne{\Gamma;\cdot}{\Delta_{n}^{min}}\\ {\mbox { with no }\Delta_{i}^{min}\subset \Delta_{j}^{min}}}
{\Turn{\Gamma;\cdot}{\Phi(\Gamma)}}
\end{mathpar}



\begin{mathpar}
	\infer*[right=MinChoice]{{\TurnMinusOne{\Gamma;\cdot}{\Phi(\Gamma)}\\ {\mbox { with no }{\sf choice_{j}}(\{\Delta_{1},\ldots, \Delta_{n}\}) \in \Phi(\Gamma), \mbox{ s.t. } {\sf choice_{j}}\subset {\sf choice_{i}}}}}
	{\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}}
\end{mathpar}


\caption{Deriving Minimal Disjunctions of Abnormalities, Unreliable Formulae, and Minimal Choice Sets of Abnormalities}\label{fig:mindab}
\end{figure}


The derivation of a minimal disjunction of abnormalities and the formation of their union and choice set is a process that occurs along with the development of the proof-tree, and the marking procedure depends on the derivation of these types of judgements. Unlike for the standard definitions of unreliable formulae and minimal choice-sets, the conclusions of the rules \textsc{UnRel} and \textsc{Choice} do not necessarily coincide with the minimal choice-sets and sets of unreliable formulae as they are used in the standard proof-format. The following propositions guarantee that this does not lead to further complications: if an abnormality occurs in $\Unrel(\Gamma)$, but is not unreliable according to $\Gamma$, then $\Unrel(\Gamma)$ was derived from a $\Delta_i$ that isn't minimal; similarly, if an abnormality occurs in some ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, but isn't verified by a minimally abnormal model, then it was derived from a $\Delta_i$ that isn't minimal. The key to these results is that the side-conditions that apply to the premises of \textsc{UnRel} and \textsc{Choice} refer to all disjunctions of abnormalities that occur unconditionally in the proof, and not only to those used as premises.

\begin{proposition}\label{prop:unrel}
    If $\Turn{\Gamma;\cdot}{\Unrel(\Gamma)}$ occurs in a proof and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are minimal at $s$, then each conjunct of $\Unrel(\Gamma)$ is in $\bigcup(\Lambda)$.
\end{proposition}
\noindent\textsl{Proof.}~Immediate from the fact that if $\Gamma; \cdot \vdash_{s - n  + (i-1)} \Delta_i^{min}$ is a premise used to derive $\Turn{\Gamma;\cdot}{\Unrel(\Gamma)}$ then (i) $\Delta_i^{min} \in \Lambda$, and (ii) $\Delta_i^{min}$ is minimal at stage $s-1$.\qed

\begin{proposition}\label{prop:choice}
    If $\Turn{\Gamma;\cdot}{\Phi(\Gamma)}$ occurs in a proof and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are minimal at $s$, then each $\phi \in \Phi(\Gamma)$ is a subset of some choice-set from $\Lambda$.
\end{proposition}
\noindent\textsl{Proof.}~If $\Gamma; \cdot \vdash_{s - n  + (i-1)} \Delta_i^{min}$ is a premise used to derive $\Turn{\Gamma;\cdot}{\Phi(\Gamma)}$, adding a premise $\Gamma; \cdot \vdash_{s - n  + (j-1)} \Delta_j^{min}$ for some $\Delta_j$ that was already derived at some stage $s - (n + m)$ would not lead to the violation of the side-condition for the application of \textsc{Choice}. Let $\Turn{\Gamma;\cdot}{\Phi'(\Gamma)}$ be the conclusion obtained by adding this premise, and note that each $\phi' \in \Phi'(\Gamma)$ can be obtained by extending a $\phi \in \Phi(\Gamma)$ with some member of $\Delta_j$.\qed

\begin{proposition}\label{prop:minchoice}
    If $\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}$ is derived from $\Gamma; \cdot \vdash_{\mathsf{s-1}}\Phi(\Gamma)$ and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are still minimal at $s$, then $\sf{choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$ is a subset of some minimal choice-set from $\Lambda$, and for each minimal choice-set ${\sf choice_{j}}(\{\Delta_{1},\ldots, \Delta_{n}, \ldots, \Delta_{n+m}\})^{min}$ from $\Lambda$ there is a ${\sf choice_{k}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \in \Phi(\Gamma)$ such that ${\sf choice_{k}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \subseteq {\sf choice_{j}}(\{\Delta_{1},\ldots, \Delta_{n}, \ldots, \Delta_{n+m}\})^{min}$.
\end{proposition}
\noindent\textsl{Proof.}~Assume $\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}$ is obtained by an application of \textsc{MinChoice} to a judgement with $\Phi(\Gamma)$ as a consequent. Let $\Delta_{n+1}$ be minimal at stage $s$, and assume that $\Phi'(\Gamma)$ is derivable from judgements with
$\Delta_1, \ldots \Delta_{n+1}$ as consequent. By Proposition \ref{prop:choice} we know that ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$ is a subset of one or more members of $\Phi'(\Gamma)$. Let $\psi_1, \ldots, \psi_m$ be an enumeration of the members of
$\Delta_{n+1}$. (i) If some $\psi_i \in \Delta_{n+1}$ is a member of ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, then the latter is also minimal in $\Phi'(\Gamma)$. (ii) If no $\psi_i \in \Delta_{n+1}$ is a member of
${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, then, because $\Delta_{n+1}$ is not included in any $\Delta_i$, any ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \cup \{\psi_i\}$ must be minimal in $\Phi'(\Gamma)$.\qed

\subsection{A simple example}\label{sec:example}

We present here a simple derivation in {\sf AdaptiveND}, where $\Gamma=\{(\neg p \vee q),p,(p\rightarrow q), (p \to \neg p)\}$:



\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(\neg p \vee q)}}}{\TurnTwo {\Gamma;\cdot}{\neg p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{p}}}{\TurnFour {\Gamma; \cdot}{(p\wedge \neg p), q}}\\
{(p \wedge \neg p)\in \Omega}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{q}}
\end{mathpar}
\bigskip
%%
In the above derivation, all judgements up to stage $4$ are obtained by {\sf minimalND} rules. Stage $5$ derives a formula on condition of the abnormality $(p\wedge \neg p)$ being false.  This corresponds to changing a multiple conclusion judgement at stage $4$ into a single conclusion one at stage $5$ by turning one of the conclusions into an adaptive condition. This move is justified by the syntactical form of the abnormality, stated as the side condition $(p \wedge \neg p)\in \Omega$ for the application of the {\sf RC} rule.



\section{Adaptive Proofs and Rules for Marking}\label{sec:marking}

In standard Adaptive Logics, one introduces strategies to tell which applications of the {\sf RC} rule should be retracted in view of the Minimal Disjunction of Abnormalities that have been derived. Adaptive Logics come with marking mechanisms that allow such retractions, according to different possible strategies. The two `standard' strategies and their rationale are \cite{batens01}:

\begin{itemize}
    \item \textit{Reliability}: once $\Unrel(\Gamma)$ is derived at some stage {\sf s}, \textit{every} formula $\psi$ derived at some prior stage {\sf s'} on the assumption that some $\phi\in \Unrel(\Gamma)$ is false, needs to be retracted;

\item \textit{Minimal Abnormality}: once $\Phi(\Gamma)$ is derived at some stage {\sf s}, a formula $\psi$ is marked if either (i) it is derived at some prior stage {\sf s'} on an assumption $\Theta$ which intersects with every minimal choice-set ${\sf choice_{i}}(\{\Delta_{i}, \ldots, \Delta_{n}\})\in \Phi(\Gamma)$; or (ii) for some minimal choice-set ${\sf choice_{i}}(\{\Delta_{i}, \ldots, \Delta_{n}\})\in \Phi(\Gamma)$, there is no derivation of $\psi$ on another condition $\Theta'$, such that the intersection of $\Theta'$ with ${\sf choice_{i}}$ is empty.
\end{itemize}
%



\subsection{Marking Rule for Reliability}\label{sec:markrel}

Reliability is the adaptive strategy that takes the most cautious interpretation of abnormalities: any formula that in view of the premises might behave abnormally, because it occurs in a minimal disjunction of abnormalities, is deemed unreliable and should not be assumed to behave normally. This means in practice that a formula $\psi$ derived on the assumption that $\phi$ behaves normally will be `marked' as soon as the unreliability of $\phi$ is established. The result of this marking is that $\psi$ should no longer be treated as a formula that was derived.

In Figure \ref{fig:markR}, we define a new rule $\XBox$R that depends on the derivation of a set of unreliable formulas.

\begin{figure}[h]
\begin{mathpar}
\infer*[right=$\XBox$R]{\Turn{\Gamma;\Theta^{-}}{\psi}\\ \TurnPrime{\Gamma; \cdot}{\Unrel(\Gamma)} \\ {\Theta \cap \Unrel(\Gamma) \ne \emptyset}}
{\TurnMaxPlusOneREL{\Gamma;\Theta^{-}}{\psi}}
\end{mathpar}
\caption{Marking for Reliability}\label{fig:markR}
\end{figure}


\subsection{Extending the example}\label{sec:example2}

Let us now extend the example from Section \ref{sec:example} with a new branch to illustrate the derivation step obtained by the Marking Rule $\XBox$R. Let $\mathbb{D}$ be the derivation from our initial example that ended with the derivation at stage {\sf 5} of $q$ in context $\Gamma$ and with $(p \wedge \neg p)^-$ as a condition. We extend it now as follows:

\begin{mathpar}
\infer*[right=\XBox R]{
{
\infer*[]{
\mathbb{D}\qquad}{\TurnFive{\Gamma; (p \wedge \neg p)^{-}}{q}}}\\
\infer*[right=$\wedge$ I]
{
\infer*[right=$\rightarrow$ E]{{\TurnSix{\Gamma;\cdot}{p}}\\{\TurnSeven{\Gamma; \cdot}{p\rightarrow \neg p}}}
{\TurnEight {\Gamma; \cdot}{\neg p}}\\{\TurnNine {\Gamma; \cdot}{p}}}{\TurnTen {\Gamma;\cdot}{p\wedge\neg p}}}
{\TurnMarkedElevenREL {\Gamma;\cdot }{q}}
\end{mathpar}
\bigskip

In this derivation a new abnormality is derived at stage {\sf 10}, namely the same that is assumed to be false at stage {\sf 5}. Note that we avoid the superflous inference step from the formula $(p \wedge \neg p)$ to the corresponding singleton set of unreliable formulas; moreover, it is essential that this abnormality be derived under an empty condition, i.e. under context $\Gamma;\cdot$, as explained above for the required strict condition on {\sf WL}. A difference between standard (i.e. linear) adaptive proofs and the proposed tree-style Natural Deduction derivation proof format becomes evident here. In the former, a marking rule implies the need to proceed backwards on the derivation, to mark all lines that were derived on an assumption that is shown to be violated and thus can no longer be considered derived. In the latter, on the other hand, there is no need to remove formulas because the result obtained at stage {\sf 5} cannot be reused in an extension of this proof.  Instead a new derivation step is performed (stage {\sf 11}), where the conclusion $q$ is marked. Moreover, if we were ever to get again $\Gamma; (p \wedge \neg p)^{-} \vdash_{\sf i} q$, it would be obtained as the result of some new derivation $\mathbb{D}^{\prime}$ and be the conclusion at some stage ${\sf i>11}$, where an additional step would again be required to mark it at a later stage.


\subsection{Marking Rules for Minimal Abnormality}\label{sec:ma}
%
%
Minimal abnormality is the marking strategy that reflects the following condition: a formula $\psi$ derived on a condition $\Theta^-$ is retracted if, either (i) every minimal choice-set includes some condition in $\Theta$, or (ii) there is a minimal choice-set ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})$ such that every derivation of $\psi$ is based on a condition that is shown to be violated by ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})$. We offer rules for this strategy in Figure \ref{fig:MA}.

\begin{figure}[ht!]
\begin{mathpar}
\infer*[right={\scriptsize$\XBox$M}]{\Turn{\Gamma;\Theta^{-}}{\psi}\\
\TurnPrime{\Gamma; \cdot}{\Phi(\Gamma)} \\ (\dag)}
{\TurnMaxPlusOneMA{\Gamma;\Theta^{-}}{\psi}}
\end{mathpar}

with
\[\Theta \cap {\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \ne \emptyset \tag{\dag}\]
for each ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \in \Phi(\Gamma)$.

\begin{mathpar}
{\small
\infer*[right={\scriptsize$\XBox$M2}]{\Turn{\Gamma;\Theta_1^{-}}{\psi}\\ \ldots \\\TurnNextn{\Gamma;\Theta_n^{-}}{\psi}\\
\TurnPrime{\Gamma; \cdot}{\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})^{min}}\\ (\dag) \\ (\ddag)}
{\TurnMarkedMAflex{max(s+n, s')+1}{\Gamma; \Theta^{-}}{\psi}}}
\end{mathpar}

with
\begin{align*}
    \begin{split}
    \Theta_1 & \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})^{min} \ne \emptyset,\\
    \vdots&\\
    \Theta_n & \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})^{min} \ne \emptyset,
    \end{split}
    \tag{\dag}
\end{align*}

and
\[
    \text{there is no } \Gamma; \Theta' \vdash_{\mathsf{t < s}} \psi \text{ with } \Theta' \not\in \{\Theta_1, \ldots, \Theta_n\}. \tag{\ddag}}
\]


\caption{Marking for Minimal Abnormality}\label{fig:MA}
\end{figure}

The first marking rule $\XBox$M reflects the following condition: if a formula $\psi$ is derived at stage $s$ under an adaptive condition $\phi$ which is part of all minimal choice-sets in $\Phi(\Gamma)$ at stage $s'$, then at the next stage the formula $\psi$ can be retracted.

The second marking rule $\XBox$M2 reflects the following condition: if a formula $\psi$ is derived always under an adaptive condition that is part of the same minimal choice-set in $\Phi(\Gamma)$, then at the next stage the formula $\psi$ can be retracted. Here the adaptive condition of the conclusion should be intended as saying that the retraction applies to all stages where the derivation of $\psi$ was obtained under one of the listed conditions.


\begin{figure}[ht!]
\centering
\begin{mathpar}
{\scriptsize
\infer*[right=$\XBox$M1]{\Turn{\Gamma; \Theta^-}{\psi} \\
\infer*[right=\textsc{MinChoice}]{\infer*[right=\textsc{Choice}]{\Pi}{\TurnNext{\Gamma, \cdot}{\Phi(\Gamma)}}}
{\TurnNextNext{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta_1, \ldots, \Delta_n\})^{min}}}\\ \ldots\\ \infer*{\Pi'}{\vdots}\\\Theta \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedNextNextMA{\Gamma;\Theta^-}{\psi}}}
\end{mathpar}




\begin{mathpar}
{\scriptsize
\infer*[right=$\XBox$M2]{\Turn{\Gamma; \Theta_1^-}{\psi} \\ \TurnNextn{\Gamma; \Theta_n^-}{\psi} \\
\infer*[right=\textsc{MinChoice}]{\infer*[right=\textsc{Choice}]{\Pi}{\TurnPrimePrime{\Gamma, \cdot}{\Phi(\Gamma)}}}
{\TurnPrimePrimePlusOne{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta_1, \ldots, \Delta_n\})^{min}}}\\
\Theta_1, \ldots, \Theta_n \cap \mathsf{choice_i} \ne \emptyset\\ \ddag} 
{\TurnMarkedMAflex{max(s+n, s''+1)+1}{\Gamma;\Theta_1^-}{\psi}
}
\end{mathpar}

\caption{Marking-trees for Minimal Abnormality}\label{fig:exmarkMA}
\end{figure}

In Figure \ref{fig:exmarkMA} we illustrate with two trees the intended use of the marking-rules for Minimal Abnormality. In the first tree, it is shown how the derivation of a formula $\psi$ under an adaptive condition $\Theta$ is followed by a series of derivations $\Pi$ for all possible choice sets of minimally abnormal formulas; if the side condition that requires $\Theta$ to occur in each such set holds, then marking can be applied. In the second (dual) tree, it is shown how a formula $\psi$ derived under an adaptive condition $\Theta$ is marked if either (i) no alternative conditional derivation of $\psi$ occurs in the proof, or (ii) every alternative conditional derivation of $\psi$ that occurs in the proof depends on a condition $\Theta'$ that intersects with the same minimal choice-set $\mathsf{choice_1}(\{\Delta_1, \ldots, \Delta_n\})^{min}$.

\subsection{An example with $\bigvee(\Delta^{min})$-selection}\label{subsec:sel-example}


The previous example is rather simple, in that it shows a formula that is first derived under an adaptive condition (referring to an abnormal formula assumed to be false), and then retracted after that condition is validated again.

Let us consider now a slightly more complex example. We want to show a situation in which a disjunction of two abnormalities can be derived: accordingly, there might be more than one formula to be marked. Let us start with a premise set $\Gamma=\{(p \vee r),\neg p,(p\vee q), \neg q, (\neg p \rightarrow q)\}$. Now consider the following derivation, dubbed $\mathbb{D}$:



\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(p \vee r)}}}{\TurnTwo {\Gamma;\cdot}{p, r}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg p}}}{\TurnFour {\Gamma; \cdot}{(p\wedge \neg p), r}}\\
{(p \wedge \neg p)\in \Omega}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{r}}
\end{mathpar}
\bigskip

At stage {\sf 4} a disjunction of an abnormality with $r$ is derived, and by {\sf RC} at stage {\sf 6} the formula $r$ is derived alone, assuming the relevant abnormality to be false. Consider now a second derivation, dubbed $\mathbb{D}^{'}$:

\begin{mathpar}
\infer*[right=UnRel]{
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnSix {\Gamma; \cdot}{(p \vee q)}}}{\TurnSeven {\Gamma;\cdot}{p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnEight {\Gamma; \cdot}{\neg p}}}{\TurnNine {\Gamma; \cdot}{(p\wedge \neg p), q}}\\
\infer*[]{\phantom{xxxx}}{\TurnTen{\Gamma; \cdot}{\neg q}}}{\TurnEleven {\Gamma;\cdot}{((p \wedge \neg p), (q\wedge \neg q))^{min}}}}
{\TurnTwelve {\Gamma; \cdot}{\Unrel(\{p\wedge \neg p,q\wedge \neg q\})}}
\end{mathpar}
\bigskip

Here the abnormality $(p \wedge \neg p)$ that was previously assumed to be false is unconditionally derived in disjunctive form with a new abnormality $(q \wedge \neg q)$ at stage {\sf 11}, where the latter is obtained by $\wedge$I from stages {\sf 7-9}. If we join now the two branches $\mathbb{D,D^{\prime}}$ to form $\mathbb{E}$, we can apply the marking-rule (where $\Unrel(\Gamma)$ is the set $\{(p \wedge \neg p), (q\wedge \neg q)\}$):

\begin{mathpar}
\infer*[right=\XBox R]{
{\infer*[]{\mathbb{D}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{r}}}\\{\infer*[]{\mathbb{D^{\prime}}}{\TurnTwelve {\Gamma; \cdot}{\Unrel(\{p\wedge \neg p,q\wedge \neg q\})}}}\\{(p \wedge \neg p)\in \bigcup(\Delta(\Gamma))}
}{\TurnMarkedThirteenREL{\Gamma; (p\wedge \neg p)^{-}}{r}}
\end{mathpar}
\bigskip

At stage {\sf 13} the formula $r$ is no longer valid, because its adaptive condition is in the set of unreliable formulas derived at stage \textsf{12}. Now we can provide a further extension of this derivation dubbed $\mathbb{D^{\prime\prime}}$:

\begin{mathpar}
\infer*[right=$\wedge$ I]{
\infer*[right=$\rightarrow$ I]{
\infer*[right=PREM]{\phantom{xxx}}{\TurnFourteen{\Gamma;\cdot}{\neg p}}\\ \infer*[right=PREM]{\phantom{xxx}}{\TurnFifteen{\Gamma;\cdot}{\neg p\rightarrow q}}}
{\TurnSixteen{\Gamma; \cdot}{q}}\\{
\infer*[right=PREM]{\phantom{xx}}{\TurnSeventeen{\Gamma;\cdot}{\neg q}}}}{\TurnEighteen {\Gamma; \cdot}{(q \wedge \neg q)}}

\end{mathpar}
\bigskip

$\mathbb{D^{\prime\prime}}$ derives a single abnormality at stage {\sf 18}, for which, as before, we skip the redundant step of deriving the singleton set of unreliable formulas. This also means that if we obtain a copy of derivation $\mathbb{D}$, where each step is re-numbered consecutively, and join it to $\mathbb{D^{\prime\prime}}$ and $\mathbb{E}$, it is possible to deduce $r$ anew with $(p \wedge \neg p)$ as its adaptive condition and accordingly leave this judgement unmarked at stage {\sf 20}:


\begin{mathpar}
\infer*[right=RC*]{
\infer*[]{\mathbb{E}}{\TurnMarkedThirteenREL{\Gamma; (p \wedge \neg p)^{-}}{r}}\\
\infer*[]{\mathbb{D^{\prime\prime}}}{\TurnEighteen {\Gamma; \cdot}{(q \wedge \neg q)^{min}}}\\
\infer*[]{\mathbb{D}}{\TurnNineteen {\Gamma;\cdot}{(p\wedge \neg p), r}}
}
{\TurnTwenty {\Gamma;(p \wedge \neg p)^{-}}{r}}

\end{mathpar}
\bigskip

\noindent where $*$ is the side condition that $(p\wedge \neg p)\in \Omega$.

\subsection{Another Example}

In the following example, the abnormalities are even more connected. For the sake of brevity, we do not derive a set of unreliable formulas when there is a single minimal disjunction of abnormalities. Let

\begin{align*}
\Gamma = & \{p \vee q, \neg q,\\
& (q \wedge \neg q) \vee (r \wedge \neg r), (q \wedge \neg q) \to (r \wedge \neg r),\\
& (q \wedge \neg q) \vee (s \wedge \neg s), (q \wedge \neg q) \to (s \wedge \neg s)\}\\
\end{align*}

Consider first the derivation $\mathbb{D}$:

\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(p \vee q)}}}{\TurnTwo {\Gamma;\cdot}{p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg q}}}{\TurnFour {\Gamma; \cdot}{(q \wedge \neg q), p}}\\
{(q \wedge \neg q)\in \Omega}}{\TurnFive {\Gamma;(q \wedge \neg q)^{-}}{p}}
\end{mathpar}

Which we extend as follows to form $\mathbb{E}$:

\begin{mathpar}
\infer*[right=\XBox R] {
    {
\infer*[] {\mathbb{D}} {\TurnFive {\Gamma;(q \wedge \neg q)^{-}}{p}}
    }\\
{
\infer*[right=$\vee$E] {
\infer*[right=PREM] {~} {\TurnSix {\Gamma} {(q \wedge \neg q) \vee (r \wedge \neg r)}}}
                    {\TurnSeven {\Gamma;\cdot}{((q \wedge \neg q), (r \wedge \neg r))^{min}}}}\\
{(q \wedge \neg q)\in \Delta^{min}}
                    }
{\TurnMarkedEightREL{\Gamma; (q \wedge \neg q)^-}{p}}
\end{mathpar}

We then construct $\mathbb{F}$ to show that a shorter disjunction of abnormalities can be derived:

\begin{mathpar}
\infer*[right=Cr]{
    \infer*[right=$\to$E]{\infer*[right=$\vee$E] {\infer*[right=PREM] {~} {\TurnNine {\Gamma; \cdot} {(q \wedge \neg q) \vee (r \wedge \neg r)}}} {\TurnTen {\Gamma; \cdot} {(q \wedge \neg q), (r \wedge \neg r)}} \\ \infer*[right=PREM] {~} {\TurnEleven {\Gamma; \cdot} {(q \wedge \neg q) \to (r \wedge \neg r)}}}
    {\TurnTwelve {\Gamma; \cdot} {(r \wedge \neg r), (r \wedge \neg r)}}
                 }
                 {
\TurnThirteen {\Gamma; \cdot} {r \wedge \neg r}
                 }

\end{mathpar}

As in the previous example, we put these branches together (and re-use a renumbered copy of $\mathbb{D}$) to obtain $\mathbb{G}$ and re-derive $p$ on condition $(q \wedge \neg q)^-$:

\begin{mathpar}
\infer*[right=RC*]{
    \infer*[]{\mathbb{E}}{\TurnMarkedEightREL{\Gamma; (q \wedge \neg q)^-}{p}}\\
    \infer*[]{\mathbb{F}}{\TurnThirteen {\Gamma; \cdot}{(r \wedge \neg r)^{min}}}\\
\infer*[]{\mathbb{D}}{\TurnFourteen {\Gamma;\cdot}{(q\wedge \neg q), p}}
}
{\TurnFifteen {\Gamma;(q \wedge \neg q)^{-}}{p}}

\end{mathpar}

But consider then the following variant of $\mathbb{E}$, denoted by $\mathbb{E}^*$:

\begin{mathpar}
\infer*[right=\XBox R] {
    {
\infer*[] {\mathbb{G}} {\TurnFifteen {\Gamma;(q \wedge \neg q)^{-}}{p}}
    }\\
{
\infer*[right=$\vee$E] {
\infer*[right=PREM] {~} {\TurnSixteen {\Gamma} {(q \wedge \neg q) \vee (s \wedge \neg s)}}}
                    {\TurnSeventeen {\Gamma;\cdot}{((q \wedge \neg q), (s \wedge \neg s))^{min}}}}\\
{(q \wedge \neg q)\in \Delta^{min}}
                    }
{\TurnMarkedEighteenREL{\Gamma; (q \wedge \neg q)^-}{p}}
\end{mathpar}

and then a variant $\mathbb{F}^*$ of $\mathbb{F}$:

\begin{mathpar}
\infer*[right=Cr]{
    \infer*[right=$\to$E]{\infer*[right=$\vee$E] {\infer*[right=PREM] {~} {\TurnNineteen {\Gamma; \cdot} {(q \wedge \neg q) \vee (s \wedge \neg s)}}} {\TurnTwenty {\Gamma; \cdot} {(q \wedge \neg q), (s \wedge \neg s)}} \\ \infer*[right=PREM] {~} {\TurnTwentyOne {\Gamma; \cdot} {(q \wedge \neg q) \to (s \wedge \neg s)}}}
    {\TurnTwentyTwo {\Gamma; \cdot} {(s \wedge \neg s), (s \wedge \neg s)}}
                 }
                 {
\TurnTwentyThree {\Gamma; \cdot} {s \wedge \neg s}
                 }

\end{mathpar}

which can once more be used to re-derive $p$:

\begin{mathpar}
\infer*[right=RC*]{
    \infer*[]{\mathbb{E}^*}{\TurnMarkedEighteenREL{\Gamma; (q \wedge \neg q)^-}{p}}\\
    \infer*[right=UnRel]{
    \infer*[]{\mathbb{F}}{\TurnThirteen {\Gamma; \cdot}{(r \wedge \neg r)}}\\
    \infer*[]{\mathbb{F}^*}{\TurnTwentyThree {\Gamma; \cdot}{(s \wedge \neg s)}}}{\TurnTwentyFour {\Gamma;\cdot}{\Unrel(\{r \wedge \neg r, s \wedge \neg s\})}}\\
\infer*[]{\mathbb{D}}{\TurnFourteen {\Gamma;\cdot}{(q\wedge \neg q), p}}
}
{\TurnTwentyFive {\Gamma;(q \wedge \neg q)^{-}}{p}}

\end{mathpar}

\subsection{An Example Based on Minimal Abnormality}
Let $\Gamma = \{p \vee r, p \vee q, q \vee r, \neg p, \neg q\}$, and consider first a derivation $\mathbb{A}$ that ends with an application of RC.

\begin{mathpar}
    \infer*[right=RC]
        {
        \infer*[right=$\wedge$ I]
            {\infer*[right=$\vee$ E]
                {\infer*[right=PREM]
                {~}
                {\TurnOne{\Gamma; \cdot}{p \vee r}}}
                {\TurnTwo{\Gamma; \cdot}{p, r}}\\
             \infer*[right=PREM]
             {~}
             {\TurnThree{\Gamma; \cdot}{\neg p}}}
            {\TurnFour{\Gamma; \cdot}{p \wedge \neg p, r}}}
        {\TurnFive{\Gamma; (p \wedge \neg p)^-}{r}}
\end{mathpar}
Next, let $\mathbb{B}$ be the following derivation, where $\ast$ refers to the application of $\wedge$ I followed by the application of \textsc{MinDab}, and $\star$ refers to the application of \textsc{Choice} followed by the application of \textsc{MinChoice} to single out $p \wedge \neg p$ from the only minimal disjunction of abnormalities:

\begin{mathpar}
    \infer*[right=$\star$]
        {\infer*[right=$\ast$]
        {\infer*[right=$\wedge$ I]
        {
        \infer*[right=$\vee$ E]
        {\infer*[right=PREM]
        {~}
        {\TurnSix{\Gamma; \cdot}{p \vee q}}}
        {\TurnSeven{\Gamma; \cdot}{p, q}}
        \\
        \infer*[right=PREM]
        {~}
        {\TurnEight{\Gamma; \cdot}{\neg p}}
        }
        {\TurnNine{\Gamma; \cdot}{p \wedge \neg p, q}}
        \\
        \infer*[right=PREM]
        {~}
        {\TurnTen{\Gamma; \cdot}{\neg q}}
        }
        {\TurnEleven{\Gamma; \cdot}{(p \wedge \neg p, q \wedge \neg q)^{min}}}}
        {\TurnTwelve{\Gamma; \cdot}{\mathsf{choice_1}(\{\{p \wedge \neg p, q \wedge \neg q\}\})^{min}}}
\end{mathpar}
Combining both with an application of \XBox M2 gives us the derivation $\mathbb{C}$:
\begin{mathpar}
\infer*[right=\XBox M2]
    {
        \infer*[right=RC]
            {\mathbb{A}}
            {\TurnFive{\Gamma; (p \wedge \neg p)^-}{r}}\\
        \infer*[right=MinChoice]
        {\mathbb{B}}
        {\TurnTwelve{\Gamma; \cdot}{\mathsf{choice_1}(\{\{p \wedge \neg p, q \wedge \neg q\}\})^{min}}}
    }
    {\TurnMarkedMAflex{13}{\Gamma; (p \wedge \neg p)^-}{r}}
\end{mathpar}
The side-condition for the application of this marking-rule is satisfied because no other conditional derivation of $r$ occurs earlier in the proof, and \emph{a fortiori}, no deduction of $r$ on a condition that does not intersect with the choice-set ${p \wedge \neg p}$ has been given.

Derivation $\mathbb{D}$ now provides a proof of $r$ that relies on a different condition:
\begin{mathpar}
    \infer*[right=RC]
        {
        \infer*[right=$\wedge$ I]
            {\infer*[right=$\vee$ E]
                {\infer*[right=PREM]
                {~}
                {\TurnFourteen{\Gamma; \cdot}{q \vee r}}}
                {\TurnFifteen{\Gamma; \cdot}{q, r}}\\
             \infer*[right=PREM]
             {~}
             {\TurnSixteen{\Gamma; \cdot}{\neg q}}}
            {\TurnSeventeen{\Gamma; \cdot}{q \wedge \neg q, r}}}
        {\TurnEighteen{\Gamma; (q \wedge \neg q)^-}{r}}
\end{mathpar}
At this point, we can see that neither of the following attempts to mark conditional deductions of $r$ using the minimal abnormality marking-rule complies with the side-conditions for \XBox M2:
\begin{mathpar}
\infer*[right=\XBox M2]
    {
        \infer*[right=RC]
            {\mathbb{A'}}
            {\Turn{\Gamma; (p \wedge \neg p)^-}{r}}\\
        \infer*[right=MinChoice]
        {\mathbb{B'}}
        {\TurnPrime{\Gamma; \cdot}{\mathsf{choice_1}(\{\{p \wedge \neg p, q \wedge \neg q\}\})^{min}}}
    }
    {?}
\end{mathpar}
\begin{mathpar}
\infer*[right=\XBox M2]
    {
        \infer*[right=RC]
            {\mathbb{D}}
            {\TurnEighteen{\Gamma; (q \wedge \neg q)^-}{r}}\\
        \infer*[right=MinChoice]
        {\mathbb{B''}}
        {\TurnPrimePrime{\Gamma; \cdot}{\mathsf{choice_2}(\{\{p \wedge \neg p, q \wedge \neg q\}\}^{min}}}
    }
    {?}
\end{mathpar}
For the first attempt, it is the presence of judgement 18 elsewhere in the proof that shows that a derivation exists that does not depend on the underivability of $p \wedge \neg p$. For the second attempt, it is the presence of judgement 5 that (notwithstanding the marked judgement 12!) signals that a derivation of $r$ that does not depend on the underivability of $q \wedge \neg q$ exists.


\subsection{Reconstruction of linear adaptive proofs}
Before we consider the question of final derivability, which is needed to relate what is provable in dynamic proofs to a semantic consequence relation, we first show that every proof in \textsf{AdaptiveND} can, with the help of the numbering of the judgements, be mapped onto an (albeit somewhat redundant) adaptive proof in a linear format. This relates what can be derived at a stage in an \textsf{AdaptiveND}-proof to what can be derived at a stage in a linear proof.

We illustrate the procedure by reconstructing the linear proof that corresponds to the example form Sections \ref{sec:example} and \ref{sec:example2}:
\begin{center}
    \begin{tabular}{cllcl}
        (1) & $\neg p \vee q$ & Prem & $\emptyset$\\
        (2) & $\bigvee \{\neg p, q\}$ & $\vee$E, (1) & $\emptyset$\\
        (3) & $p$ & Prem & $\emptyset$\\
        (4) & $\bigvee \{p \wedge \neg p, q\}$ & $\wedge$ I, (2, 3) & $\emptyset$\\
        (5) & $q$ & RC, (4) & $\{p\}$ & $\XBox^{10}$\\
        (6) & $p$ & Prem & $\emptyset$\\
        (7) & $p \to \neg p$ & Prem & $\emptyset$\\
        (8) & $\neg p$ & $\to$ E, (6, 7) & $\emptyset$\\
        (9) & $p$ & Prem & $\emptyset$\\
        (10) & $p \wedge \neg p$ & $\wedge$ I, (8, 9) & $\emptyset$\\
    \end{tabular}
\end{center}
In this proof, the application of $\vee$E on line (2) is based on the representation of the disjunctive comma by a `super-imposed' classical disjunction (a device that effectively plays the same role in adaptive logic, see \cite[\S 2.2, 2.7]{Strasser:AdaptiveLogicsForDefeasibleReasoning:}), whereas the application of $\wedge$I is valid in virtue of the \textbf{CLuN}-validity of $\neg p \vee q, p \vdash (p \wedge \neg p) \vee q$ which warrants the application of the unconditional rule (with empty conditions). The final marking is not added as a separate line, but is instead added in the fifth place on line 5 and labelled with the number of the line or stage at which the relevant abnormality was derived.

As illustrated in a second example based on the proof from \S \ref{subsec:sel-example}, there is no guarantee that the translation of an {\sf AdaptiveND} proof into a linear adaptive proof will contain all the markings required by the latter.
\begin{center}
    \begin{tabular}{cllcl}
        (1) & $p \vee r$ & Prem & $\emptyset$\\
        (2) & $\bigvee \{p, r\}$ & $\vee$E, (1) & $\emptyset$\\
        (3) & $\neg p$ & Prem & $\emptyset$\\
        (4) & $\bigvee \{p \wedge \neg p, r\}$ & $\wedge$ I, (2, 3) & $\emptyset$\\
        (5) & $r$ & RC, (4) & $\{p\}$ & $\XBox^{??}$\\
        (6) & $p \vee q$ & Prem & $\emptyset$\\
        (7) & $\bigvee(p, q)$ & $\vee$E, (6)& $\emptyset$\\
        (8) & $\neg p$ & Prem & $\emptyset$\\
        (9) & $\bigvee(p \wedge \neg p, q)$ & $\wedge$ I, (7, 8) & $\emptyset$\\
        (10) & $\neg q$ & Prem & $\emptyset$\\
        (11) & $\bigvee(p \wedge \neg p, q \wedge \neg q)$ & $\wedge$ I, (9, 10) & $\emptyset$
    \end{tabular}
\end{center}
Indeed, the above proof is a straightforward translation of the tree-form proof up to stage {\sf 11}, but the marking of line 5, which should be added to the linear proof once $\bigvee(p \wedge \neg p, q \wedge \neg q)$ is derived, cannot be added by a mechanical translation procedure. This is because in the standard proof-format marking is governed by a definition, which simply stipulates when a line is marked, whereas in \textsf{AdaptiveND} marking is governed by rules and therefore requires the execution of additional inferential steps.\footnote{See also footnote 10 of \cite{BDVM08} for a discussion of this distinction.} To show that proofs in \textsf{AdaptiveND} correctly capture the adaptive dynamics, we will therefore have to show that such ``missing markings'' can always be obtained by a further extension of a proof.

In the next section we complete our system with the required meta-theoretical analysis needed to define derivability at stage and final derivability.

\section{Derivability}\label{sec:meta}

In the example from the previous section we have illustrated how the marking condition establishes a dynamic derivability relation, which allows to derive formulas and retract them. Whenever a certain formula is derived on some $\phi\in \Delta^{min}$ adaptive condition, it might still be marked afterwards. Consequently, a judgement of the form $\Turn{\Gamma; \Theta^-}{\psi}$ only expresses what is derived at a stage. This gives us the notion of derivability at a stage:

\begin{definition}[Derivability at stage]
A formula $\psi$ is derived at stage {\sf s}  iff $\TurnPrime{\Gamma; \phi^{-}}{\psi}$ where $\mathsf{s'\leq s}$ and it is not the case that $\TurnPrimePrimeMarked{\Gamma; \cdot}{\psi}$ for some $\mathsf{s'\leq s''\leq s}$.
\end{definition}

A more stable notion of derivability, called \emph{final derivability}, holds when marking is no longer possible. This notion is customarily defined with a reference to possible extensions of a proof.\footnote{``A is finally derived from $\Gamma$ on line $i$ of a proof at stage $s$ iff (i) $A$ is the second element of line $i$, (ii) line $i$ is not marked at stage $s$, and (iii) every extension of the proof in which line $i$ is marked may be further extended in such a way that line $i$ is unmarked.'' \cite[229]{batens07}} By only taking finite premise-sets into consideration, we can pursue a more explicit characterisation of final derivability.

To this aim, one requires that the stage {\sf s} at which a formula $\psi$ is derived remains unmarked in all the extensions of the derivation tree which can be obtained by using all \textit{relevant} abnormalities as adaptive conditions. This relevance criterion is essential if one wants to guarantee finite surveyability of the proof tree to establish whether a formula is never marked (again). We define therefore a set of \textit{abnormalities relevant to $\Gamma$}. To do so we first identify the union set of all subformulas of the premise set $\Gamma$:


\begin{definition}[Subformulas of the premise set]
$\Sf(\Gamma)=\bigcup_{\phi \in \Gamma} \{\psi \mid \psi$ is a subformula of $ \phi\}$.
\end{definition}


From $\Sf(\Gamma)$ we then construe all the possible abnormalities that can be obtained from its members:

\begin{definition}[Abnormalities relevant to the premise set]
$\Omega(\Gamma)=\{\psi\wedge \neg \psi \in \Omega \mid \psi\in \Sf(\Gamma)\}$.
\end{definition}
For \textsf{AdaptiveND}, \textbf{CLuN$^r$}, and \textbf{CLuN$^m$} the requirement that all $\psi\wedge \neg \psi$ should be in $\Omega$ is trivially satisfied. This condition becomes mandatory when $\Omega$ is based on a restricted logical form; e.g. when abnormalities are contradictions of the form $\psi \wedge \neg \psi$ with $\psi$ atomic. In that case, our definition of $\Omega(\Gamma)$ is co-extensive with the more basic $\{\psi \wedge \neg \psi \mid \psi \in \At(\Gamma)\}$.
%
\begin{theorem}\label{thm:subform}
    If $\bigvee(\Delta)$ is a minimal disjunction of abnormalities derivable from $\Gamma$, then $\Delta \subseteq \Omega(\Gamma)$.
\end{theorem}
{\color{red} check numbering}
\noindent\textsl{Proof.}~We consider the possible ways of deriving a minimal disjunction of abnormalities by examining the structure of the proof-rules of \textsf{minimalND}.
\begin{enumerate*}
    \item Applying ($\vee$I) (or \textsc{Wr}) can never result in a minimal disjunction of abnormalities. This excludes all proof-rules that can be used to deduce a judgement with a formula on the right that isn't yet a formula or sub-formula in one of the judgements it relies on.
    \item A formula of the form $\phi \wedge \neg \phi$ can be derived on the right of the turn-style if it is already a sub-formula of some premise, or the result of ($\wedge$I).
    \item If $\phi \wedge \neg \phi$ is the result of ($\wedge$I), each of its conjuncts should be derivable. We focus on the proof-paths to formulae of the form $\neg \phi$, of which there are four:
        \begin{enumerate*}
            \item $\neg \phi$ is a premise;
            \item $\neg \phi$ can be obtained by ($\wedge$E) from some $\neg \phi \wedge \psi$ on the right;
            \item $\neg \phi$ can be obtained by ($\to$E) from some $\psi \to \neg \phi$ on the right;
            \item $\neg \phi$ can be obtained by ($\neg$I) from $\phi$ on the left.
        \end{enumerate*}
        Cases (a-c) imply that $\neg \phi$ should be a positive part of a previously derived formula on the right, and hence $\phi$ should be a negative part of that formula. By induction over the length of proofs (with the rule {\sf PREM}  as the base-case), these three cases can be retraced to $\phi$ being a negative part of some premise.
    \item Case (d) requires the presence or deduction of some $\phi$ on the left, either because the left-hand side is of the form $\Gamma, \phi$ and thus the result of applying (\textsf{WL}), or because it is of the form $\Gamma$ with $\phi \in \Gamma$. In each of these cases, this can never lead to a judgement where (i) the left-side consists only of the premise-set, and (ii) the right-side has no more formulae than before the application of ($\neg$I). This implies that case (d) cannot lead to the deduction of a minimal disjunction of abnormalities.
\end{enumerate*}
From (3) and (4) it follows that every abnormality that occurs in a minimal disjunction of abnormalities obtains from a formula that occurs as the negative part of some premise. A fortiori, this means it should be formed from a member of $\Omega(\Gamma)$.\qed

The focus on positive and negative parts of formulae goes back to \cite{Sch60}, and was previously used for the development of goal-directed proof-strategies for adaptive logics \cite{Batens:LogiqueAnalyse:2001}. The fact that we should pay attention to all negative parts of the premises should also be obvious in view of the semantics for \textbf{CLuN}, as the truth-value of negative formulae does not need to depend on the truth-values of its sub-formulae.

Theorem \ref{thm:subform} helps us to characterise finite proof-trees to decide whether a formula is finally derived by identifying the abnormalities derivable in view of the syntactical form of the premises. But it can also be seen as a \textbf{CLuN}-specific variant of the \emph{Derivability Adjustment Theorem} from \cite{batens07}. This result can be stated in multiple-conclusion form as follows:
\[
    \Gamma \vdash_{\mathbf{ULL}} \phi \text{ iff } \Gamma \vdash_{\mathbf{LLL}} \phi, \Delta \text{ for some finite } \Delta \subset \Omega
\]
Or yet it can be seen as a \textbf{CLuN}-alternative of a result from \cite{Beall:TheReviewOfSymbolicLogic:2011} that relates $\mathbf{LP}^{+}$, the multiple-conclusion extension of \textbf{LP}, and $\mathbf{CPL}^{+}$, the multiple-conclusion extension of classical logic:

\[
    X \models_{\mathbf{CPL}}^+ Y \text{ iff } X \models_{\mathbf{LP}}^+ Y \cup \iota(X) \tag{LP/CPL}\label{beall}
\]
with $\iota(X) = \{p \wedge \neg p : p \in \At(X)\}$.

Here, we do not use such connections to bridge different approaches to classical recapture, but instead rely on it to  introduce the notion of a \textit{complete proof-tree} with respect to derivable relevant disjunction of abnormalities:

\begin{definition}[Completeness relative to relevant abnormalities]
Let $P$ be an {\sf AdaptiveND} proof. We say that $P$ is complete relative to $\Omega(\Gamma)$ at stage {\sf s} if for every derivable $\bigvee(\Delta^{min})$ with $\Delta\subseteq\Omega(\Gamma)$ there is an $\mathsf{s' < s}$ such that $\TurnPrime{\Gamma;\cdot}{\bigvee(\Delta^{min})}$.

%
\end{definition}


\begin{definition}[Completeness relative to marking]
Let $P$ be an {\sf AdaptiveND} proof. We say that $P$ is complete relative to marking
iff
\begin{description}
    \item[Rel] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$ occurring in a tree $T_{i}\in P$, if $P$ can be extended so that it includes a judgement $\Gamma;\cdot \vdash_{\mathsf{s'}} \Unrel(\Gamma)$ with $\bigcup \Delta(\Gamma) \cap \Theta \neq \emptyset$, then there is a tree $T_{t>i}\in P$ that ends with $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.
    \item[MinAb1] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$, if every derivable $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$ intersects with $\Theta$, then there is a tree $T_{t>i}\in P$ that ends with  $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.
    \item[MinAb2] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$, if for every other derivable $\Gamma; \Theta'^{-}\vdash_{{\sf s}} \phi$, some derivable
     $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$ intersects with each $\Theta, \dots, \Theta'$, then there is a tree $T_{t>i}\in P$ that ends with $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.

\end{description}



\end{definition}


We can now formulate our notion of final derivability:

\begin{definition}[Final Derivability]\label{def:finalder}
A formula $\psi$ is finally derived $\TurnChecked{\Gamma;\Theta^{-}}{\psi}$ iff $\Turn{\Gamma; \Theta^-}{\psi}$ occurs in an abnormality and marking complete proof $P$, where $\TurnMarkedprime{\Gamma; \Theta^-}{\psi}$ does not occur for any $\mathsf{s'} \geq \mathsf{s}$.
\end{definition}
%



\begin{theorem}
$\TurnChecked{\Gamma;\Theta^{-}}{\psi}$ in {\sf AdaptiveND} if and only if there is a final derivation of $\psi$ from $\Gamma$ in a standard linear adaptive proof.
\end{theorem}

\begin{proof}

\begin{itemize}
%\marginpar{Replaced $\mathbb{P}$ by $\mathbf{P}$ to distinguish from the previous uses of mathbb-fonts in the examples.}
\item[$\rightarrow$] By assumption $\psi$ is finally derived, therefore there is an abnormality and marking complete proof $P$ in which it is derived. Then $P$, by definition, contains unconditional judgements for every deducible minimal disjunction of abnormalities in $\Omega(\Gamma)$ and $\psi$ is not contained in any of those. Now consider the translation $\mathbf{P}$ of $P$ in a linear adaptive proof: the same minimal disjunctions of abnormalities as in $P$ are also unconditionally derived in $\mathbf{P}$. By Theorem \ref{thm:subform}, this implies that all minimal disjunctions of abnormalities are unconditionally derived in $\mathbf{P}$, and further extensions of the proof cannot lead to additional unconditionally derived minimal disjunctions of abnormalities. Because $P$ contains all possible markings, every formula which is marked in $P$ will be marked in $\mathbf{P}$, and $\mathbf{P}$ will therefore be in accordance with the standard marking definitions. As $\psi$ is not in any adaptive conditions of $P$ which induces a marking, it will be derived in $\mathbf{P}$ as well. Moreover, $\psi$ will be finally derived in $\mathbf{P}$ because further extensions would not lead to newly derived minimal disjunctions of abnormalities, and would not lead to additional marking either.

\item[$\leftarrow$] Assume that $\psi$ is finally derived in some linear adaptive proof $\mathbf{P}$; then


\begin{itemize}
  \item[for \textbf{CluN}$^{R}$]  $\Gamma\vdash_{\mathbf{CluN}}\psi\vee \phi$ with $\phi\cap U(\Gamma)=\emptyset$. By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\phi}$. By applying {\sf RC}, we obtain a judgement
  $\TurnNext{\Gamma;\phi^{-}}{\psi}$. If at some later stage {\sf s'} an abnormality complete proof is obtained, any $\bigcup \Delta (\Gamma)$ derived thereafter will, by Proposition \ref{prop:unrel} be a sub-set of $U(\Gamma)$. Consequently, any application of $\XBox$R to a judgement with condition $\phi^-$ would from that point also require $\phi \in \bigcup \Delta (\Gamma)$. Since this would contradict our assumption that $\phi\cap U(\Gamma)\neq\emptyset$, no such marking can be applied.

  \item[for \textbf{CluN}$^{M}$] $\Gamma \vdash_{\mathbf{CluN}} \psi\vee \Theta$, with $\Theta \subseteq \Omega(\Gamma)$ and

\begin{enumerate}

\item either for every $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$ we have
\[
    \Theta \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) = \emptyset.
\]
By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\Theta}$, and by repeated applications of \textsc{RC}, we obtain the judgement $\TurnNextm{\Gamma; \Theta^-}{\psi}$. Let $t \geq s+m$ be a stage at which this proof is marking and abnormality-incomplete, and assume, \emph{for reductio} that it ends with a judgement $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$. Consequently, this judgement must occur as the final node of a marking-tree for minimal abnormality:

\begin{figure}[ht!]
\begin{mathpar}
{\footnotesize
\infer*[right={\tiny$\XBox$M}]{\Turn{\Gamma; \Theta^-}{\psi} \\
\infer*[right={\tiny\textsc{MinChoice}}]{\infer*[right={\tiny\textsc{Choice}}]{\Pi}{\TurnNext{\Gamma, \cdot}{\Phi'(\Gamma)}}}
{\TurnNextNext{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta'_1, \ldots, \Delta'_m\})^{min}}}\\ \ldots\\ \infer*{\Pi'}{\vdots}\\\Theta \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}}}
\end{mathpar}
\end{figure}

To complete our argument, we rely on the fact that every choice-set derivable from $\Phi'(\Gamma)$ is used as a premise for the application of the marking-rule, but we do not need to assume that $\Phi'(\Gamma)$ is identical to $\Phi(\Gamma)$. Because $\Phi'(\Gamma)$ is derived at a stage of the proof that is already abnormality-complete, we know that $\{\Delta'_1, \ldots, \Delta'_m\} \subseteq \{\Delta_1, \ldots, \Delta_n\}$ (each $\Delta'_i$ is equal to some $\Delta_i$, but not vice-versa). Proposition \ref{prop:minchoice} then guarantees that each $\mathsf{choice_j}(\{\Delta_1, \ldots, \Delta_n\})$ is a superset of some $\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})$. Consequently, since the marking at stage $t$ required that $\Theta \cap \mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\}) \ne \emptyset$ for each choice-set in $\Phi'(\Gamma)$, it should also hold that $\Theta \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \ne \emptyset$ for each choice-set in $\Phi(\Gamma)$, which contradicts our initial assumption.

\item or for every $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$, if $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \cap \Theta \ne \emptyset$, there is a $\Theta'$ such that $\Gamma \vdash_{\mathbf{CluN}} \psi \vee \Theta'$ with
\[
    \Theta' \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) = \emptyset.
\]
By Theorem \ref{thm:clun} there are provable {\sf AdaptiveND} judgements $\Turn{\Gamma;\cdot}{\psi,\Theta}$ and $\TurnPrime{\Gamma;\cdot}{\psi,\Theta'}$ for $\Theta$ and each such $\Theta'$. As in the previous case, by repeated applications of \textsc{RC} we can derive corresponding $\TurnNextm{\Gamma;\Theta^-}{\psi}$ and $\TurnNextmPrime{\Gamma;\Theta'^-}{\psi}$. Let $t$ be a stage at which this proof is extended to an abnormality and marking complete proof, and assume, \emph{for reductio}, that it ends with a judgement $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$. Consequently, this judgement must occur as the final node of a marking-tree for minimal abnormality:

\begin{figure}[!ht]
\begin{mathpar}
{\footnotesize
\infer*[right={\tiny$\XBox$M}]{\Turn{\Gamma; \Theta^-}{\psi} \\
\ldots\\
\TurnPrime{\Gamma; \Theta'^{-}}{\psi}\\
\infer*[right={\tiny\textsc{MinCh}}]{\infer*[right={\tiny\textsc{Choice}}]{\Pi}{\TurnPrimePrime{\Gamma, \cdot}{\Phi'(\Gamma)}}}
{\TurnPrimePrimePlusOne{\Gamma; \cdot}{\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})^{min}}}
\forall(\Theta\ldots \Theta') \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedMAflex{t}{\Gamma;\Theta^-}{\psi}}}
\end{mathpar}
\end{figure}
But then, by the same reasoning as above, Proposition \ref{prop:minchoice} entails that if $\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})$ intersects with every condition $\Theta, \ldots, \Theta'$, it must also intersect with some $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \in \Phi(\Gamma)$, which contradicts our initial assumption.

\end{enumerate}
Therefore, $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$ cannot occur at any stage $t$ of a proof that is abnormality-complete at $t$.
\end{itemize}

\end{itemize}
\end{proof}

\section{Concluding remarks}\label{sec:recap}
To conclude, we would like to highlight certain distinctive features of the proposed calculus, and briefly discuss how these features can be used to reconsider the question of classical recapture. As we see it, the defeasible reasoning-forms formalised in our \textsf{AdaptiveND} system have three primary virtues:
\begin{enumerate*}
    \item they are formulated in a tree-format that forces one to state all information used in an inference-step explicitly, and this restricts the reliance on global features of a proof to a minimum (e.g. when checking that a disjunction of abnormalities is minimal);
    \item the multiple-conclusion format leads to a transparent connection between the restricted inference-rules that are valid in \textsf{minimalND} (i.e. the lower-limit-logic) and their use as a premise of the conditional rule;
    \item the explicit individuation of abnormalities as a sub-type of the well-formed formulae.
\end{enumerate*}
The explicit connection between multiple-conclusions and defeasible inferences brings a recent disagreement over the problem of classical recapture in the logic \textbf{LP} into focus.\footnote{See \cite[18ff]{Allo:Theoria:2015} for a more detailed reconstruction of this debate.} In several papers, Graham Priest has explicitly endorsed the adaptive approach to classical recapture. To that effect, he has proposed his own \emph{minimally inconsistent} \textbf{LP}: an adaptive logic based on a stronger paraconsistent logic (but without a detachable implication) and the minimal abnormality strategy \cite{GP:LPm}. This approach has been criticised by JC Beall, another prominent defender of the logic \textbf{LP}, on the ground that any all-purpose logic should at any cost prevent one to step from truth to falsehood \cite{Beall01072012}. This is a task that cannot in general be fulfilled by an adaptive logic, and indeed a task we shouldn't impute on adaptive logics in the first place \cite{Priest01102012}. By contrast, Beall's preferred take on classical recapture is that it should be handled with extra-logical means. The multiple-conclusion extensions of classical logic and \textbf{LP} already mentioned in the previous section provide formalisms in which this idea can be made precise, since (\ref{beall}) can be seen as a minimalist expression of how paraconsistent logics like \textbf{LP} incorporate classical logic in a restricted form. Given the central role of similar multiple-conclusion judgements in \textsf{AdaptiveND}, results like (\ref{beall}) should really be understood as agnostic between the different strategies for classical recapture. Indeed, whereas Beall advocates the view that \textbf{LP$^+$}
 only presents us with logically viable options, these same options work as the motor behind any defeasible inference mechanism that allows one to favour one of them in the first place. Presentations of defeasible approaches to classical recapture based on the selection of minimally abnormal models bypass references to logical options: but their use in dynamic proofs relies implicitly or explicitly on the individuation of logical options that conform to a particular logical form. When adaptive logics are formulated according to the standard format, this type of connection is already made visible (Priest's minimally inconsistent \textbf{LP} is not formulated in this generic format) through the \emph{Derivability Adjustment Theorem} mentioned in the previous section. \textsf{AdaptiveND} makes this connection even more explicit, by formulating its conditional rule with a multiple-conclusion judgement as a premise, and enforcing the condition that a logical option must have a particular logical form to be moved from the right-hand side where it is a logical possibility to the left-hand side where it is used as a negative condition.

 The formal approach taken in the development of \textsf{AdaptiveND} signals another crucial departure from the terms in which the Priest/Beall debate is carried out, namely a departure concerning the individuation of abnormalities. Within the adaptive logic tradition, abnormalities are understood as formulae of a specific logical form, and the abnormality of models (e.g. how inconsistent they are) is measured relative to the abnormal formulas they verify. When compared to the road taken by minimally inconsistent \textbf{LP}, this has certain advantages (see \cite{Batens:Synthese:2000} for a diagnosis of this problem in the first-order case). The same syntactic approach to abnormalities is integrated in \textsf{AdaptiveND} through the identification of a class of formulae of type $\Omega$ and the need to state membership of $\Omega$ when the conditional rule is applied. This approach is more general in the sense that it doesn't have to appeal to semantic concepts like \emph{gluts} in its formulation, and it can explain how we step from logical options to defeasible inferences by only taking into account the logical form of the premises at hand. From a proof-theoretic viewpoint, this could be seen as a more explicit approach, whereas from the standpoint of the broader adaptive logic programme it is definitely more flexible.



\bibliographystyle{plain}
\bibliography{primiero}

\end{document}
