\documentclass[]{article}

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{syntax}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathpartir}
\usepackage{bussproofs}
\usepackage{wasysym}
\usepackage{xcolor}
\usepackage{mdwlist}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand{\TurnADND}[2]
    { {#1}\vdash_{\textbf{\sf AdaptiveND}}  {#2}}

\newcommand{\TurnMinusn}[2]
            { {#1}\vdash_{\textbf{\sf s-n}}  {#2}}

\newcommand{\Turn}[2]
    { {#1}\vdash_{\textbf{\sf s}}  {#2}}
    \newcommand{\TurnNext}[2]
        { {#1}\vdash_{\textbf{\sf s+1}}  {#2}}

            \newcommand{\TurnMinusOne}[2]
                    { {#1}\vdash_{\textbf{\sf s-1}}  {#2}}

\newcommand{\TurnNextn}[2]
        { {#1}\vdash_{\textbf{\sf s+n}}  {#2}}
\newcommand{\TurnNextm}[2]
        { {#1}\vdash_{\textbf{\sf s+m}}  {#2}}
\newcommand{\TurnNextmPrime}[2]
        { {#1}\vdash_{\textbf{\sf s'+m'}}  {#2}}
\newcommand{\TurnNextnn}[2]
        { {#1}\vdash_{\textbf{\sf s+n+1}}  {#2}}
\newcommand{\TurnNextNext}[2]
    { {#1}\vdash_{\textbf{\sf s+2}}  {#2}}

\newcommand{\TurnNextNextNext}[2]
    { {#1}\vdash_{\textbf{\sf s+3}}  {#2}}

\newcommand{\TurnPrime}[2]
    { {#1}\vdash_{\textbf{\sf s'}}  {#2}}

\newcommand{\TurnPrimePrime}[2]
    { {#1}\vdash_{\textbf{\sf s''}}  {#2}}

    \newcommand{\TurnPrimePrimePlusOne}[2]
        { {#1}\vdash_{\textbf{\sf s''+1}}  {#2}}

        \newcommand{\TurnPrimePrimePlusTwo}[2]
            { {#1}\vdash_{\textbf{\sf s''+2}}  {#2}}


\newcommand{\TurnOne}[2]
    { {#1}\vdash_{\textbf{\sf 1}}  {#2}}
\newcommand{\TurnMarked}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox}}  {#2}}

\newcommand{\TurnMarkedREL}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox R}}  {#2}}
\newcommand{\TurnMarkedMA}[2]
    { {#1}\vdash_{\textbf{\sf s\XBox MA}}  {#2}}
\newcommand{\TurnMarkedMAflex}[3]
    { {#2}\vdash_{\textbf{\sf {#1}\XBox MA}}  {#3}}
\newcommand{\TurnPlusOneMarkedMA}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox MA}}  {#2}}


\newcommand{\TurnChecked}[2]
    { {#1}\vdash_{\textbf{\sf \checked}}  {#2}}
\newcommand{\TurnMarkedNext}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox}}  {#2}}
\newcommand{\TurnMarkedprime}[2]
    { {#1}\vdash_{\textbf{\sf s'\XBox}}  {#2}}
    \newcommand{\TurnPrimePrimeMarked}[2]
        { {#1}\vdash_{\textbf{\sf s''\XBox}}  {#2}}


\newcommand{\TurnMaxPlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s')+1}}  {#2}}

\newcommand{\TurnMaxTwoPlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s')+1}} {#2}}

\newcommand{\TurnMaxThreePlusOne}[2]
     { {#1}\vdash_{\textbf{\sf max(s,s',s'')+1}}  {#2}}

\newcommand{\TurnMarkedNextREL}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox R}}  {#2}}
\newcommand{\TurnMarkedNextNextREL}[2]
    { {#1}\vdash_{\textbf{\sf s+n+1\XBox R}}  {#2}}
\newcommand{\TurnMaxPlusOneREL}[2]
    { {#1}\vdash_{\textbf{\sf max(s,s')+1\XBox R}}  {#2}}

\newcommand{\TurnMarkedNextMA}[2]
    { {#1}\vdash_{\textbf{\sf s+1\XBox MA}}  {#2}}
\newcommand{\TurnMarkedNextNextMA}[2]
    { {#1}\vdash_{\textbf{\sf s+n+1\XBox MA}}  {#2}}
\newcommand{\TurnMaxPlusOneMA}[2]
        { {#1}\vdash_{\textbf{\sf max(s,s')+1\XBox MA}}  {#2}}
        \newcommand{\TurnMaxTwoPlusOneMA}[2]
                { {#1}\vdash_{\textbf{\sf max(s,s',s'')+1\XBox MA}}  {#2}}


%\newcommand{\TurnOne}[2]
%   { {#1}\vdash_{\textbf{\sf 1}}  {#2}}
\newcommand{\TurnTwo}[2]
    { {#1}\vdash_{\textbf{\sf 2}}  {#2}}
\newcommand{\TurnThree}[2]
    { {#1}\vdash_{\textbf{\sf 3}}  {#2}}
\newcommand{\TurnFour}[2]
    { {#1}\vdash_{\textbf{\sf 4}}  {#2}}
\newcommand{\TurnFive}[2]
    { {#1}\vdash_{\textbf{\sf 5}}  {#2}}
\newcommand{\TurnSix}[2]
    { {#1}\vdash_{\textbf{\sf 6}}  {#2}}

\newcommand{\TurnSeven}[2]
    { {#1}\vdash_{\textbf{\sf 7}}  {#2}}

\newcommand{\TurnMarkedSevenREL}[2]
    { {#1}\vdash_{\textbf{\sf 7\XBox R}}  {#2}}

\newcommand{\TurnMarkedEightREL}[2]
    { {#1}\vdash_{\textbf{\sf 8\XBox R}}  {#2}}


\newcommand{\TurnEight}[2]
    { {#1}\vdash_{\textbf{\sf 8}}  {#2}}


\newcommand{\TurnMarkedFiveMA}[2]
    { {#1}\vdash_{\textbf{\sf 5\XBox MA}}  {#2}}
\newcommand{\TurnMarkedEightMA}[2]
    { {#1}\vdash_{\textbf{\sf 8\XBox MA}}  {#2}}

\newcommand{\TurnNine}[2]
    { {#1}\vdash_{\textbf{\sf 9}}  {#2}}


\newcommand{\TurnTen}[2]
    { {#1}\vdash_{\textbf{\sf 10}}  {#2}}

\newcommand{\TurnEleven}[2]
        { {#1}\vdash_{\textbf{\sf 11}}  {#2}}


\newcommand{\TurnMarkedElevenREL}[2]
    { {#1}\vdash_{\textbf{\sf 11\XBox R}}  {#2}}

\newcommand{\TurnTwelve}[2]
        { {#1}\vdash_{\textbf{\sf 12}}  {#2}}

\newcommand{\TurnMarkedTwelveREL}[2]
    { {#1}\vdash_{\textbf{\sf 12\XBox R}}  {#2}}

\newcommand{\TurnMarkedThirteenREL}[2]
{ {#1}\vdash_{\textbf{\sf 13\XBox R}}  {#2}}


\newcommand{\TurnThirteen}[2]
        { {#1}\vdash_{\textbf{\sf 13}}  {#2}}

\newcommand{\TurnFourteen}[2]
        { {#1}\vdash_{\textbf{\sf 14}}  {#2}}

\newcommand{\TurnFifteen}[2]
        { {#1}\vdash_{\textbf{\sf 15}}  {#2}}


\newcommand{\TurnSixteen}[2]
        { {#1}\vdash_{\textbf{\sf 16}}  {#2}}

\newcommand{\TurnSeventeen}[2]
        { {#1}\vdash_{\textbf{\sf 17}}  {#2}}


\newcommand{\TurnEighteen}[2]
        { {#1}\vdash_{\textbf{\sf 18}}  {#2}}

\newcommand{\TurnMarkedEighteenREL}[2]
    { {#1}\vdash_{\textbf{\sf 18\XBox R}}  {#2}}

\newcommand{\TurnNineteen}[2]
        { {#1}\vdash_{\textbf{\sf 19}}  {#2}}

\newcommand{\TurnTwenty}[2]
{ {#1}\vdash_{\textbf{\sf 20}}  {#2}}

\newcommand{\TurnTwentyOne}[2]
{ {#1}\vdash_{\textbf{\sf 21}}  {#2}}

\newcommand{\TurnTwentyTwo}[2]
{ {#1}\vdash_{\textbf{\sf 22}}  {#2}}

\newcommand{\TurnTwentyThree}[2]
{ {#1}\vdash_{\textbf{\sf 23}}  {#2}}

\newcommand{\TurnTwentyFour}[2]
{ {#1}\vdash_{\textbf{\sf 24}}  {#2}}

\newcommand{\TurnTwentyFive}[2]
{ {#1}\vdash_{\textbf{\sf 25}}  {#2}}

\newcommand{\Sf}{\ensuremath{\mathrm{Sf}}}
\newcommand{\At}{\ensuremath{\mathrm{At}}}

%\newcommand{\TurnT}[2]
%   { \Delta_0;{#1}\vdash  {#2}}
%\newcommand{\TurnTT}[2]
%   { \Delta_0;{#1}\vdash_{\sf JC_1}  {#2}}
%\newcommand{\Turnj}[1]
%   { \Delta_0\vdash_{\sf J_0}  {#1}}
%\newcommand{\Turnjc}[3]
%    { {#1};{#2}\vdash_{\textbf{\sf JC}}  {#3}}


%opening
\title{Annotated Natural Deduction for Adaptive Reasoning}
\author{Giuseppe Primiero\\
Department of Computer Science\\
Middlesex University London\\
 \and Patrick Allo\\
 Oxford Internet Institute\\
 University of Oxford}
\date{}


\begin{document}

\maketitle

\begin{abstract}
We present a multi-conclusion natural deduction calculus characterizing the dynamic reasoning typical of adaptive logics. The resulting system {\sf AdaptiveND} is sound and complete with respect to the propositional fragment of adaptive logics based on  \textbf{CLuN}. This appears to be the first tree-format presentation of the standard linear dynamic proof system typical of Adaptive Logics. It offers the advantage of full transparency in the formulation of locally derivable rules, a connection between restricted inference rules and their adaptive counterpart, and the formulation of abnormalities as a subtype of well-formed formulas. These features of the proposed calculus are used to reconsider the question of classical recapture.
\end{abstract}

\section{Intro}

In this paper we outline a multiple-conclusion natural deduction calculus in which the dynamics of standard (Fitch-style) dynamic proofs of Adaptive Logics \cite{batens07} can be reconstructed. Adaptive logics are a family of logics that can be used to formalise a wide range of defeasible reasoning forms. Their consequence-relations rely on the standard idea of interpreting premises as normally as possible through the selection of models of its premises, but it is only at the level of its proof-theory that its distinctive approach comes to the fore. Adaptive logics, namely, reconstruct defeasible reasoning patterns as dynamic proofs; proofs in which steps performed earlier may later be retracted when the assumptions they were based on no longer hold.

The specific system we describe here is for an inconsistency adaptive logic: this is a logic that captures the paraconsistent reasoning performed to avoid triviality in the face of inconsistency, while trying to make up for its deductive weakness by provisionally applying classical inference-rules when there is no explicit indication that inconsistencies are involved in that inference. This choice brings us closer to the original motivations for the development of adaptive logic \cite{Batens:ParaconsistentLogicEssaysOnTheInconsistent:1989}, but also allows us to engage with current philosophical debates of relevance to Graham Priest's work.

The dynamics of retracting earlier lines in a proof can be captured in a rather natural way in linear proof-formats, including standard axiomatic and Fitch-style natural deduction proofs, but is much less straightforward in a tree-like proof-format. Consider, for instance, the following retraction in an application of \emph{Ex Contradictione Quodlibet}:

\begin{figure}[h!]
\centering
    \begin{tabular}{cllcl}
        (1) & $p$ & Prem & $\emptyset$\\
        (2) & $p \vee q$ & Addition & $\emptyset$\\
        (3) & $\neg p$ & Prem & $\emptyset$\\
        (4) & $q$ & DS & $\{p\}$ & $\XBox^5$\\
        (5) & $p \wedge \neg p$ & Adjunction & $\emptyset$
    \end{tabular}
\end{figure}
\noindent Here, at line (4) disjunctive syllogism (DS) is applied on the condition that $p$ behaves normally, i.e.\ that the contradiction $p \wedge \neg p$ hasn't been derived. When this contradiction is effectively derived at line (5), line (4) is marked (here and in the following by $\XBox$) and is from then on no longer assumed to be part of the proof. This type of reasoning illustrates the idea of provisional applications of classical inference-rules to paraconsistent logics that reject the disjunctive syllogism, but in which the restricted form $\phi \vee \psi, \neg \phi / \psi \vee (\phi \wedge \neg \phi)$ is retained.

Contrast this, now, with the following attempt to reconstruct a similar reasoning-process in a Gentzen-Prawitz-style proof-tree:
%\footnote{In the proposed system we will not make use of the standard discharging method for assumptions in the marking of derivation judgements.}

\begin{prooftree}
    \AxiomC{$\Gamma \vdash p$}
    \RightLabel{$\vee$I}
    \UnaryInfC{$\Gamma \vdash p \vee q$}
    \AxiomC{$\Gamma \vdash \neg p$}
    \AxiomC{$\Gamma \not\vdash p \wedge \neg p$}
    \LeftLabel{DS$^*$}
    \TrinaryInfC{$\Gamma \vdash q$}
    \AxiomC{$\Gamma \vdash p$}
    \AxiomC{$\Gamma \vdash \neg p$}
    \RightLabel{$\wedge$I}
    \BinaryInfC{$\Gamma \vdash p \wedge \neg p$}
    \BinaryInfC{?}
\end{prooftree}
When in this proof an explicit contradiction is derived in the right-hand branch, the assumption of its invalidity (stated explicitly in the left-hand branch) no longer holds. In this format, however, the order used to construct the proof cannot be read off the proof itself (an issue that could easily be fixed). But also, more importantly, it isn't even clear what it might mean to retract the line where $q$ is derived, since the result of removing that line from the proof is in itself no longer a well-formed proof.

The proof-format we propose solves this problem by making two changes: first, we add indices to judgements to keep track of stages in the construction of a proof; and second, we exploit the fact that judgements that are `marked' at a certain stage do not have to be removed, because there is simply no need to prevent their implicit re-use since every assumption or premise should explicitly be written down in the place it is used. Instead, it is the derivation of the same judgement at a later stage that is (or may be) blocked, because the original assumption that led to its initial derivation probably no longer holds. We therefore provide, for the first time, an appropriate Natural Deduction translation of adaptive reasoning, whose proofs have been so far always been presented in their linear format. We do so by formulating a general approach to Adaptive Reasoning which accommodates both strategies standardly used to retract previously made judgements. While the reformulation of the Reliability Strategy is a much easier task, Minimal Abnormality is a more daunting one given the complexity of its procedural translation in a proof tree. Nonetheless, we show that it can be done within our logic, although more efficient procedures might be devised.

Because this system uses multiple-conclusion judgements, it also explicitly captures the connection between unconditional derivations of certain disjunctions in the paraconsistent logic and the conditional deductions of one of their disjuncts in the adaptive logic. This formal feature can be used to re-assess a current debate on how one should best approach the question of \textit{classical recapture} in paraconsistent logics. The latter problem can be summarised as follows. When one adopts a logic that is strictly weaker than classical logic, the question of how one should account for epistemically useful classical inference-forms that are invalidated by one's preferred logic almost immediately arises. In the case of paraconsistent logic, this question is often deemed urgent, as the practical and epistemic usefulness of the inference-forms that are lost, like the disjunctive syllogism, is almost undisputed. Inconsistency-adaptive logics present one possible answer to this challenge under the form of defeasible inference-forms that allow one to use classical inference-steps on the condition that certain assumptions are not violated. It is also a response that Graham has endorsed \cite{GP:LPm}. His specific proposal on how this should be implemented has, in recent years, become the focus of a renewed interest in the problem of how dialetheists should account for classical recapture. We contend that the combination of a multiple-conclusion calculus with the reconstruction of the defeasible dynamics of adaptive proofs can further clarify this debate.

%When one adopts a logic that is strictly weaker than classical logic, the question of how one should account for epistemically useful classical inference-forms that are invalidated by one's preferred logic almost immediately arises. In the case of paraconsistent logic, this question is often deemed urgent, as the practical and epistemic usefulness of the inference-forms that are lost, like the disjunctive syllogism, are almost undisputed. This is the problem of \emph{classical recapture}. Inconsistency-adaptive logics present one possible answer to this challenge under the form of defeasible inference-forms that allow one to use classical inference-steps on the condition that certain assumptions are not violated. The class of adaptive logics that have been formulated since the earliest formulation of this paradigm as a response to the problem of classical recapture generalise this idea, and provide a general framework in which many types of defeasible inference-forms can be rigourously formalised.

The paper is structured as follows. We introduce in Section \ref{sec:lower} a basic natural deduction system called {\sf minimalND}, which acts as the Lower Limit Logic of our adaptive system. In Section \ref{sec:adaptive}, we extend the system to account for adaptive reasoning through the definition of an appropriate abnormal form of expressions and appropriate adaptive rules; the new system is called {\sf AdaptiveND}. In Section \ref{sec:marking} we define marking strategies to identify derivation steps that can no longer be assumed to hold in the tree. In Section \ref{sec:meta} we define basic meta-theoretical properties. We return to the challenge of classical recapture in Section \ref{sec:recap}.

% We present a multi-conclusion natural deduction calculus that mimics the dynamic reasoning at work in adaptive logics (\cite{batens07}). This is the first attempt to reconstruct the dynamics typical of adaptive logics in a natural deduction setting. The resulting system does not correspond to the usual structure known as the Standard Format for Adaptive Logics: this means that, though we \textit{do not} introduce an adaptive logic proper, we can talk of a natural deduction system for \textit{adaptive reasoning}. We characterize such a way of reasoning as having properties that identify adaptive dynamics.  To do so, the standard proof-theoretical procedure of a natural deduction system is enhanced with:

% \begin{enumerate}
% \item a rule-based ability of introducing abnormal formulas of the form $A\wedge \neg A$;\footnote{In the current format we focus on inconsistency-adaptive logics, though the generalization to the natural deduction for other adaptive formats seems possible.} the appearance of such formulas on the right-hand side of our derivability sign justifies the claim that our system is extended to a multi-conclusion setting;

% \item a rule-based ability of deriving formulas under conditions that some such abnormal formula is not true;

% \item the procedural ability of rejecting derivation steps previously obtained by way of marking in view of effectively derived abnormal formulas.
% \end{enumerate}

% These are all properties inspired by the adaptive logics approach. In view of the last property, we need moreover to annotate the derivability relation with a stage counting mechanism to keep track of the steps performed in the derivation tree (thus counting also premises rather than only rules).

% %These are all properties inspired by the adaptive logics approach. This means that, though we \textit{do not} introduce an adaptive logic proper, we can talk of a natural-deduction system for \textit{adaptive reasoning}. In fact,
% Our system is not even close to a standard format for AL. We express the standard triple $\{LLL,\Omega,STRATEGY\}$ in a system where the Lower Limit Logic is extended to include rules both for expressing the abnormal formulas in $\Omega$ and to interpret the selection Strategy. In other words, this rule-based approach allows to merge the rules and axioms of a typical $LLL$ and the rules of the $AL$ based on abnormal formulas into a single system of rules.

%\subsection*{Other Works}
%xxx


\section{{\sf minimalND}}\label{sec:lower}

We start by defining the type universe for the $\{\neg, \rightarrow, \wedge, \vee\}$ fragment of intuitionistic propositional logic corresponding to minimal logic. We call this logic {\sf minimalND} and use it as the equivalent of a Lower Limit Logic---the paraconsistent logic that governs the unconditional steps in a proof. Contrary to what is standard in an intuitionistic setting, we do not allow the deduction of $\bot$ from an explicit contradiction. Whereas $\bot$ can be eliminated via \emph{Ex Falso Quodlibet}, there is no introduction-rule for $\bot$, and this is what makes our base-logic paraconsistent. It is only when the assumption of consistency is introduced that the connection between negation-inconsistency and absolute inconsistency can provisionally be recreated.

We start by defining the syntax of our language:

\begin{definition}[{\sf minimalND}]

 Our starting language for {\sf minimalND} is defined by the following grammar:

\begin{displaymath}
\begin{array}{l}
{\sf Type}:={\sf Prop}\\
{\sf Prop}:= A \mid \bot \mid \neg \phi \mid \phi_{1} \rightarrow \phi_{2} \mid \phi_{1} \wedge \phi_{2} \mid \phi_{1} \vee \phi_{2}\\
\Gamma := \{\phi_{1}, \dots, \phi_{n}\}\\
\Delta := \{\phi_{1}, \dots, \phi_{n}\}

\end{array}
\end{displaymath}
\end{definition}

%
The type universe of reference is the set of propostions {\sf Prop}, construed by atomic formulas closed under negation, implication, conjunction, disjunction and allowing $\bot$ to express absolute contradictions. Formula formation rules are given in Figure \ref{fig:formulaconstructions}.

\begin{figure}[h!]
\begin{mathpar}
\infer*[right=Atom] { } {A \in {\sf Prop}}
\and
\infer*[right=$\bot$] { } {\bot \in {\sf Prop}}
\and
\infer*[right=$\neg$] {\phi \in {\sf Prop} } {\neg \phi \in {\sf Prop}}
\and
\infer*[right=$\rightarrow$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\rightarrow\phi_2\in {\sf Prop}}
\and
\infer*[right=$\wedge$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\wedge\phi_2\in {\sf Prop}}
\and
\infer*[right=$\vee$] {{\phi_1 \in {\sf Prop }}\\ {\phi_2 \in {\sf Prop}}} {\phi_1\vee\phi_2\in {\sf Prop}}
\end{mathpar}
\caption{Formula Formation Rules}\label{fig:formulaconstructions}
\end{figure}



\begin{definition}[Judgements]
A multiple conclusion {\sf minimalND}-judgement is of the form $\Gamma;\cdot \vdash_{\sf s} \Delta$, where: $\Gamma$ is the usual set of assumptions, $\Delta$ is a set of formulas of the language and {\sf s} is a positive integer.
\end{definition}
The set $\Gamma$ on the left-hand side of the derivability sign is to be read conjunctively. Similarly for the semi-colon symbol, which is introduced here but is only used in Section \ref{sec:adaptive} to separate standard assumptions in $\Gamma$ from conditions (in the adaptive sense). At this stage, the symbol $\cdot$ following the semi-colon is used to express an empty set of adaptive conditions. The set $\Delta$ and the comma (if it occurs) on the right-hand side of the derivability sign are both to be read disjunctively. This characterizes our calculus as multiple-conclusion. Context formation rules, for both left and right-hand side set of formulas are given in Figure \ref{fig:contextrules}. {\sf Nil} establishes the base case of a valid empty context, we use {\sf wf} as an abbreviation for `well-formed'; $\Gamma${\sf -Formation} allows extension of contexts by propositions; {\sf Prem} establishes derivability of formulas contained in context (and it defines the equivalent of the adaptive Premise rule).
%; finally, $\Delta${\sf -Formation} allows \textit{disjunctive} extension of derived sets of formulas by well-typed ones.{\color{red}this is still an open issue. G: IN WHICH WAY?}


\begin{figure}[h!]
\begin{mathpar}
\infer*[right=Nil] { } {\cdot\Turn {} {\sf wf}}
\and
\infer*[right=$\Gamma$-formation] {{\Turn {\Gamma; \cdot} {\sf wf} } \\ {\phi \in {\sf Prop}}} {\TurnNext {\Gamma , \phi; \cdot} {\sf wf}}
\end{mathpar}


\begin{mathpar}
\infer*[right=Prem] {{\Turn {\Gamma; \cdot} {\sf wf}}\\ {\phi \in \Gamma}}{\TurnNext {\Gamma; \cdot} {\phi}}
%\and
%\infer*[right=$\Delta$-formation] {{\Turn {\Gamma; \cdot} {\Delta \cup \{\phi\}}}} {\TurnNext {\Gamma; \cdot} {\Delta, \phi}}

\end{mathpar}
\caption{Context Formation Rules}\label{fig:contextrules}
\end{figure}

The derivability sign is enhanced with a signature {\sf s} that corresponds to a counter of the ordered derivation steps executed to obtain the corresponding ND-formula in a tree. This annotation only comes to use in the next extension of the calculus in Section \ref{sec:adaptive}.

The semantics of connectives is given in the standard proof-theoretic way by Introduction and Elimination Rules in Figure \ref{fig:connectives}. Introduction of $\rightarrow$ corresponds to conditional proof, while its elimination formalises Modus Ponens. Rules for $\wedge$ are standard; notice that $\vee$-Elimination makes the disjunctive reading of the comma on the right hand-side of the turnstile explicit. $\bot$ can be eliminated by \emph{Ex Falso}, but cannot be introduced. Dually, our paraconsistent negation $\neg$ can be introduced, but not eliminated.
%
%
\begin{figure}[h!]
\begin{mathpar}
\infer*[right=$\rightarrow$I] {\Turn {\Gamma, \phi_1; \cdot} {\Delta, \phi_2}} {\TurnNext {\Gamma; \cdot} {\Delta, \phi_1\rightarrow \phi_2}}
\and
\infer*[right=$\rightarrow$E] {\Turn {\Gamma; \cdot} {\Delta, \phi_1\rightarrow\phi_2}\\{\TurnPrime {\Gamma';\cdot} {\Delta', \phi_1}}} {\TurnMaxPlusOne {\Gamma; \Gamma'} {\Delta, \Delta', \phi_2}}
\end{mathpar}

\begin{mathpar}
\infer*[right=$\wedge$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_1}\\{\TurnPrime {\Gamma'; \cdot} {\Delta', \phi_2}}} {\TurnMaxPlusOne {\Gamma, \Gamma';\cdot} {\Delta, \Delta', \phi_1\wedge \phi_2}}
\and
\infer*[right=$\wedge$E]
{\Turn {\Gamma;\cdot} {\Delta, \phi_1\wedge\phi_2}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_{i \in \{1,2\}}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=$\vee$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_1}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_1\vee \phi_2}}
\and
\infer*[right=$\vee$I] {\Turn {\Gamma;\cdot} {\Delta, \phi_2}} {\TurnNext {\Gamma;\cdot} {\Delta, \phi_1\vee \phi_2}}
\and
\infer*[right=$\vee$E]
{\Turn {\Gamma;\cdot} {\Delta, \phi_1\vee\phi_2}}{\TurnNext {\Gamma;\cdot} {\Delta, \phi_{1},\phi_{2}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=$\bot$E] {\Turn {\Gamma; \cdot}{\Delta, \bot} }{\Turn {\Gamma;\cdot} {\Delta, \phi}}
%\end{mathpar}
\and
%\begin{mathpar}
\infer*[right=$\neg$I] {\Turn {\Gamma; \phi}{\Delta, \psi}}{\TurnNext {\Gamma; \cdot}{\Delta, \psi, \neg \phi}}
\end{mathpar}

\caption{Rules for I/E of connectives}\label{fig:connectives}
\end{figure}

\begin{figure}[h!]
\begin{mathpar}
\infer*[right=Wl] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}} } {\TurnNext {\Gamma, \phi_{2}; \cdot} {\Delta, \phi_{1}}}
\and
\infer*[right=Cl] {\Turn {\Gamma, \phi_{1}, \phi_{1}; \cdot} {\Delta, \phi_{2}} } {\TurnNext {\Gamma, \phi_{1}; \cdot} {\Delta, \phi_{2}}}
\and
\infer*[right=El] {\Turn {\Gamma, \phi_{1}, \phi_{2}; \cdot} {\Delta, \phi_{3}} } {\TurnNext {\Gamma, \phi_{2}, \phi_{1}; \cdot} {\Delta, \phi_{3}}}
\end{mathpar}
\begin{mathpar}

\infer*[right=Cut] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}} \\ {\TurnPrime {\Gamma', \phi_{1}; \cdot} {\Delta', \phi_{2}}}} {\TurnMaxPlusOne {\Gamma; \Gamma'; \cdot} {\Delta, \Delta', \phi_{2}}}
\end{mathpar}
\begin{mathpar}

\infer*[right=Cr] {\Turn {\Gamma; \cdot} {\Delta, \phi, \phi}} {\TurnNext {\Gamma; \cdot} {\Delta, \phi} }
\and
\infer*[right=Er] {\Turn {\Gamma; \cdot} {\Delta, \phi_{1}, \phi_{2}} } {\TurnNext {\Gamma; \cdot} {\Delta, \phi_{2}, \phi_{1}}}
\end{mathpar}
\caption{Structural Rules}\label{fig:structural}
\end{figure}

Finally, we introduce  in Figure \ref{fig:structural} a set of rules to enforce structural properties.  {\sf WL} is a Weakening on the left-hand side of the judgement: it allows the monotonic extension of assumptions preserving already derivable formulas. Notice that this rule can only work with a strictly empty set of formulas $; \cdot$ following $\Gamma$: we shall introduce in the next section this as the set of \textit{adaptive conditions}. The reason for this requirement in {\sf WL} is that the set of adaptive conditions strictly depends on the set of assumptions $\Gamma$, hence a Weakening of the latter can imply a different formulation of the former. We do not need to formulate a {\sf WR} rule for weakening of the set $\Delta$ of derivable formulas, as this can be obtained by a detour of $\vee$-Introduction and Elimination. {\sf CL} for Contraction on the left allows elimination of repeated assumptions and {\sf EL} for Exchange on the left is valid just by set construction, as there is no order. {\sf CR} and {\sf ER} do a similar job on the right-hand side of the judgement. Finally, {\sf Cut} (also known as {\sf Substitution} in some Natural Deduction Calculi) guarantees that derivations can be pasted together, and in general it requires that there are no clashes of free variables in $\Gamma, \Gamma'$.

The resulting system is equivalent to the propositional fragment of \textbf{CLuN}, the logic obtained by adding Excluded Middle to the positive fragment of classical logic. This is a very weak paraconsistent (but not paracomplete) logic that does not validate any of the usual De Morgan rules \cite{Batens:LogiqueAnalyse:1980}, and has been used as the Lower Limit Logic of one of the first adaptive logics.

\begin{theorem}\label{thm:clun}
    {\sf minimalND} is sound and complete w.r.t. to the propositional fragment of \textbf{CLuN}.
\end{theorem}
\noindent\textsl{Proof.} Soundness can be shown as usual, with the key step verifying that ($\neg$I) is sound in view of the completeness-clause for negation
\[
   \text{If } v(\phi) = \mathrm{False} \text{, then } v(\neg \phi) = \mathrm{True}\tag{C$\neg$}\label{eq:negclause}
\]
Completeness follows from the provability of all \textbf{CLuN}-axioms. Below, we only give the proofs for Excluded Middle and Peirce's Law.
    \begin{mathpar}
    \infer*[left=Cr]
        {\infer*[right=$\vee$I]
            {\infer*[right=$\vee$I]
                {\infer*[right=$\neg$I]
                    {\infer*[left=Prem]
                        { }
                        {\TurnOne {p; \cdot} {p}}}
                    {\TurnTwo {\emptyset; \cdot} {p, \neg p}}}
                {\TurnThree {\emptyset; \cdot} {p \vee \neg p, \neg p}}}
            {\TurnFour {\emptyset; \cdot} {p \vee \neg p, p \vee \neg p}}}
        {\TurnFive {\emptyset; \cdot} {p \vee \neg p}}

    \and

    \infer*[right=$\to$I]
        {\infer*[right=Cr]
            {\infer*[right=$\to$E]
                {\infer*[left=$\to$I]
                    {\infer*[left=Wr]
                        {\TurnOne {p;\cdot} {p}}
                        {\TurnTwo {p;\cdot} {p, q}}}
                    {\TurnThree {;\cdot} {p, p \to q}} \\
                \infer*[right=Prem]
                    { }
                    {\TurnFour {(p \to q) \to p; \cdot} {(p \to q) \to p}}}
                {\TurnFive {(p \to q) \to p; \cdot} {p, p}}}
            {\TurnSix {(p \to q) \to p; \cdot} {p}}}
        {\TurnSeven { ;\cdot} {((p \to q) \to p) \to  p}}
    \end{mathpar}
\qed
% % % %NOT DONE THIS

%
%We should also remember that this Minimal Logic fragment verify
%
%\begin{figure}[h]
%\begin{mathpar}
%\infer*[right=] {\Turn {\Gamma; \cdot} {\phi\rightarrow\psi}} {\TurnNext {\Gamma; \cdot} {\neg \phi,  \psi}}
%\end{mathpar}
%\end{figure}
%
%but it does not verify
%
%\begin{figure}[h]
%\begin{mathpar}
%\infer*[right=] {\Turn {\Gamma; \cdot} {\neg \phi\rightarrow\psi}} {\TurnNext {\Gamma; \cdot} {\phi,  \psi}}
%\end{mathpar}
%\end{figure}
%
%hence is not fully CLuN equivalent.
%]

\section{{\sf AdaptiveND}}\label{sec:adaptive}


We now extend {\sf minimalND} to characterize a new logic called {\sf AdaptiveND} to allow for inconsistency adaptive reasoning. To this aim one needs:
%
\begin{enumerate}
\item the explicit formulation of an $\Omega$ set of propositions;% of type $\bot$ {\color{red}why type $\bot$};
\item the formulation of judgements including an \textit{adaptive condition};
\item the formulation of a rule that allows the derivation of new formulas independent from such an adaptive condition;
\item the formulation of a rule that allows the derivation of new formulas that depend on such an adaptive condition.
\end{enumerate}
%
We offer accordingly new definitions for the syntax of this logic and the related form of judgements.

\begin{definition}[{\sf AdaptiveND}]
The language of {\sf AdaptiveND} is as follows:


\begin{displaymath}
\begin{array}{l}
{\sf Type}:={\sf Prop}\\
{\sf Prop}:= A \mid \bot \mid \neg \phi \mid \phi_{1} \rightarrow \phi_{2} \mid \phi_{1} \wedge \phi_{2} \mid \phi_{1} \vee \phi_{2}\\
% \mid \phi_{1} \vee_{\sf CL} \phi_{2}\\
\Gamma := \{\phi_{1}, \dots, \phi_{n}\}\\
\Delta := \{\phi_{1}, \dots, \phi_{n}\}\\
\Omega := \{\phi \wedge \neg \phi\mid \phi\in Prop\}\\
%Dab(\Delta) := \phi_{1} \vee_{\sf CL} \phi_{2}\mid \phi_{1},\phi_{2}\in \Omega
\end{array}
\end{displaymath}
\end{definition}




\begin{definition}[Judgements]
An {\sf AdaptiveND}-judgement is of the form $\Gamma; \Theta^{-}\vdash_{s} \Delta$, where:

\begin{enumerate}
\item the left-hand side of $\vdash_{\sf s}$ has $\Gamma$ as in {\sf minimalND};
\item the semicolon sign on the left-hand side of $\vdash_{\sf s}$ is conjunctive;
\item $\Theta$ refers to a finite subset of $\Omega$, i.e. a set of formulas of a specific inconsistent logical form; we write $\phi$ instead of $\{\phi\}$ when $\Theta$ is the singleton $\{\phi\}$; below we introduce an appropriate $\Omega$-formation rule;\footnote{As mentioned above, the current setting of {\sf AdaptiveND} is specified for an inconsistency-adaptive logic.}
\item the last place of the left-hand side context is always reserved for negated formulas of type $\Omega$; we shall use $\phi^{-}$ to refer to the negation of $\phi$, and $\Theta^-$ for $\{\phi^- \mid \phi \in \Theta\}$;
\item the right-hand side is in disjunctive form.
\end{enumerate}
\end{definition}
%
When the second place on the left-hand side of $\vdash$ is empty, we shall write $\Gamma;\cdot\vdash$, thus reducing to the form of a {\sf minimalND}-judgement.
%When $\Omega$ is empty on the right-hand side of $\vdash$, we shall write simply $\vdash A$. In view of point $4.$ above, this natural deduction calculus can also be characterized as a multiple conclusion calculus.\footnote{There is a certain similarity between $\vdash ;\Omega$ and the notion of \textit{denied formulas} in a state from \cite{restall2005}: the standard way to read the right hand side of an {\sf AdaptiveND}-formula is in fact that of an exclusive ``or".}
Moreover, in {\sf AdaptiveND}, the annotation on the proof stage {\sf s} is optionally followed by one of the following two marks:

\begin{itemize}
    \item[]  $\XBox$ to mark that at the current stage some previously derived formula is retracted; the meaning of this annotation is given for the Reliability Strategy by a corresponding rule $\XBox R$, presented in Section \ref{sec:markrel}; for the Minimal Abnormality Strategy, its meaning is given by two rules, presented in Section \ref{sec:ma};
\item[] $\checked$ to mark that at the current stage some previously derived formulas is now finally derived, i.e. will no longer be marked by $\XBox$; the use of this annotation is formally given below in Definition \ref{def:finalder}.
\end{itemize}
%These symbols will be formally introduced in Sections \ref{sec:marking} and \ref{sec:meta} respectively.
%We shall also use below the abbreviation $\neg\phi:=\phi\rightarrow\bot$.

%A feature of {\sf AdaptiveND} that we will use below is that it requires to have \textit{classical disjunctions of formulas generated under the $\Omega$-rule}. To be more precise, whenever we derive more than a formula in the set $\Omega$, we might want to establish which of those is unavoidable, and thus extend $\Gamma$ with a set of disjunctions of $\omega$'s which is classical (as we cannot infer it from any already verified $\omega$). This means that {\sf AdaptiveND} needs a new formation rule for $\vee_{\sf CL}$, which is in fact restricted to $\phi$'s that are in $\Omega$. We shall also give a formation rule for $\Omega$ and one for $\Gamma; \Omega^{-}$. Furthermore, two additional rules are introduced for deriving formulas, simply or on conditions.

%The mechanism enforced by the marking definitions (either for retracting or for stabilizing a derived content) makes a proof a sequence of derivation steps. The dynamic nature of this form of reasoning is implemented in our ND calculus by allowing extensions by derivation steps appended at the end of the current proof-tree.\footnote{That is, we will avoid the more complex approach that allows for line insertions in a given derivation.} For obtaining the stable notion of final derivability, it will be useful to allow \textit{infinite} extensions of a proof-tree.

%\subsection{Rules of {\sf AdaptiveND}}

We now introduce the rules for {\sf AdaptiveND}. In Figure \ref{fig:omega}, we describe the formation and use of formulas $\phi \in \Omega$. By $\Omega${\sf-Formation}, the explicit contradiction $\phi \wedge \neg \phi$, with $\phi$ any proposition, is a formula of the $\Omega$ type. In the Adaptive tradition a formula of type $\Omega$ is called an \textit{abnormality} or \textit{abnormal formula}. By {\sf Adaptive Condition Formation}, given a valid context $\Gamma$ and a formula $\phi$ of the $\Omega$ type, a context $\Gamma$ followed by the Adaptive Condition that expresses the defeasible assumption that $\phi$ \textit{is false}, is a well-formed context. This corresponds to the use of syntactic restrictions that are applied to the use of conditions as additional elements of a proof line in the standard linear format of adaptive logics. By {\sf Adaptive Condition Extension}, a newly constructed formula of type $\Omega$ can be added to an existing non-empty Adaptive Condition.

\begin{figure}[h!]
\begin{mathpar}
\infer*[right=$\Omega$-formation] {{\phi \in {\sf Prop}}}{(\phi \wedge \neg \phi) \in \Omega}
%
% {{\Turn {\Gamma;\cdot} {\phi}}\\ {\TurnNext {\Gamma, \phi; \cdot} {\neg\phi}}}
%{\TurnNextNext {\Gamma; \cdot} {{\sf \Omega}}}
\and
\infer*[right=Adaptive Condition-formation] {{\Turn {\Gamma;\cdot} {\sf wf} } \\ {\phi \in {\sf \Omega}}} {\TurnNext {\Gamma ; \phi^{-}} {\sf wf}}
\and
\infer*[right=Adaptive Condition-extension] {{\Turn {\Gamma;\Theta^-} {\sf wf} } \\ {\phi \in {\sf \Omega}}} {\TurnNext {\Gamma ; \Theta^-, \phi^{-}} {\sf wf}}
\end{mathpar}
\caption{$\Omega$ Formation rules}\label{fig:omega}
\end{figure}


Next, the calculus is extended by introducing the conditional rule {\sf RC} (Figure \ref{fig:adaptiverules}), which states that if a disjunction $\psi,\phi$ is derivable from $\Gamma$, with $\phi$ an abnormal formula, then $\psi$ can also be derived alone under $\Gamma$ and the Adaptive Condition that $\phi$ be false. Because the application of {\sf RC} can be delayed by keeping formulae of type $\Omega$ on the right hand-side of the turnstile, the role of the unconditional rules of the standard calculus is subsumed under the {\sf Cut} rule. The single and multi-premise versions of the unconditional rules displayed in Figure \ref{fig:adaptiverules2} can thus be treated as derived rules as shown in Figure \ref{fig:condasderived}.

% two rules, see Figure \ref{fig:adaptiverules}. $RU$ is called the unconditional rule: it says that if a formula $\phi_{1}$ is derivable in {\sf AdaptiveND}, and another formula $\phi_{2}$ is derivable from $\phi_{1}$ without additional assumptions or adaptive conditions, then $\phi_{2}$ is derivable from $\phi_{1}$ and the context $\Gamma;\phi^{-}\vdash$ in which the latter holds.  $RC$ is  called the conditional rule: it says that if a disjunction $\psi,\phi$ is derivable from $\Gamma$, with $\phi$ an abnormal formula, then $\psi$ can also be derived alone under $\Gamma$ and the Adaptive Condition that $\phi$ be false.

\begin{figure}[h!]
\begin{mathpar}
\infer*[right=RC] {\Turn {\Gamma;\Theta^{-}} {\psi,\phi}\\ {\phi \in \Omega}} {\TurnNext {\Gamma; (\Theta\cup\{\phi\})^{-}} {\psi}}
\end{mathpar}

\caption{Conditional Rule}\label{fig:adaptiverules}
\end{figure}

\begin{figure}[h!]
\begin{mathpar}
\infer*[right=RU] {\TurnPrime {\Gamma; \phi^{-}} {\phi_{1}}\\ {\TurnPrimePrime {\phi_{1};\cdot} {\phi_{2}}}}
{\TurnMaxTwoPlusOne {\Gamma; \phi^{-}} {\phi_{2}}}
\and
\infer*[right=RU2] {\Turn {\Gamma; \phi^-} {\phi_{1}}\\ {\TurnPrime {\Gamma';\phi'^-} {\phi_2}} \\ {\TurnPrimePrime {\phi_1, \phi_2;\cdot} {\phi_3}}}
{\TurnMaxThreePlusOne {\Gamma, \Gamma'; \{\phi, \phi'\}^-} {\phi_{3}}}
\end{mathpar}

\caption{Unconditional Rules}\label{fig:adaptiverules2}
\end{figure}

\begin{figure}
    \begin{mathpar}
        \infer*[right=RC]{
                        \infer*[right=Cut]{\TurnOne {\Gamma; \cdot} {\phi_{1}, \phi}\\ {\TurnTwo {\phi_{1};\cdot} {\phi_{2}}}}
                        {\TurnThree {\Gamma; \cdot} {\phi_{2}, \phi} \\ \phi \in \Omega}
                        }
                         {\TurnFour {\Gamma; \phi^-}{\phi_2}}

        \and

\infer*[right=RC]{
		\infer*[right=RC]{
        \infer*[right=Cut]{{\TurnFour {\Gamma; \cdot} {\phi_{1}, \phi}}
        	\infer*[right=Cut]{\TurnOne {\Gamma;\cdot} {\phi_2, \phi'}\\ {\TurnTwo {\phi_1, \phi_2;\cdot} {\phi_3}}}
        	{\TurnThree {\Gamma', \phi_1 ;\cdot}{\phi_3, \phi'}}
        }
        {\TurnFive {\Gamma, \Gamma'; \cdot} {\phi_{3}, \phi, \phi'} \\ \phi \in \Omega}}
    {\TurnSix {\Gamma, \Gamma'; \phi^-} {\phi_{3}, \phi'} \\ \phi' \in \Omega}}
{\TurnSeven {\Gamma, \Gamma'; \{\phi, \phi'\}^-} {\phi_{3}}}
    \end{mathpar}
    \caption{Redundancy of unconditional rules}\label{fig:condasderived}
\end{figure}

The Adaptive strategies developed in the next Section have the aim of establishing which abnormal formulas can no longer be safely considered as conditions in the application of the Conditional Rule {\sf RC}, thereby requiring a retraction of the formulas that are derivable from assuming the underivability of these abnormalities. To this aim, three elements need to be introduced:

\begin{enumerate}
\item  minimal disjunctions of formulas of type $\Omega$, denoted by $\bigvee(\Delta)^{min}$, with $\Delta \subset \Omega$;
\item the union of all abnormalities that occur as a disjunct of some $\bigvee(\Delta)^{min}$ derived up to a certain stage from $\Gamma$, denoted by $\bigcup\Delta(\Gamma)$;
\item the set of minimal choice sets of a family of sets $\Delta_1, \ldots, \Delta_n$, corresponding to all $\bigvee(\Delta_i)^{min}$ derived up to a certain stage from $\Gamma$, denoted by $\Phi(\Gamma)$.
\end{enumerate}

The rules in Figure \ref{fig:mindab} establish these three constructions.

Rule {\sf MinDab} says that a disjunctive formula of the $\Omega$ type derived at some stage {\sf s} of a derivation can be considered minimal at stage {\sf s'} if at no previous stage {\sf t} $<$ {\sf s'} a shorter one can be derived in the same context $\Gamma$. Note that the superscript $min$ works as an annotation that the disjuction $\Delta$ in the first premise of the {\sf MinDab} rule satisfies the side condition of the same rule, but it does not indicate that $\Delta^{min}$ is a different set than $\Delta$.

Rule {\sf UnRel} says that given minimal disjunctions of abnormalities, each derived according to {\sf MinDab} at stages {\sf s-n} up to {\sf s-1}, the union set of all such formulas can be derived at stage {\sf s}, defined as follows:

\begin{definition}[Set of unreliable formulas]
$\bigcup\Delta(\Gamma)$ derived at stage {\sf s} from $\Gamma$ denotes the union of some $\Delta_{1}, \dots, \Delta_{n}$ derived from $\Gamma$ up to stage {\sf s-1} according to {\sf MinDab}.
\end{definition}
Note that when the operator $\bigcup\Delta(\Gamma)$ occurs on the right-hand side of the tunrstile, its reading is not intended as disjunctive, as it is the case for normal a set $\Delta$ in the same position. The side condition for the Rule {\sf UnRel} ensures that each $\Delta_{i}^{min}$ used as a premise is still minimal at stage {\sf s-1}.

Choice sets provide selections of abnormalities that might turn out to be true at a stage {\sf s}, defined as follows:

\begin{definition}[Choice set of $\{\Delta_{1}, \dots, \Delta_{n}\}$]
$\Phi(\Gamma)$ derived at stage {\sf s} from $\Gamma$ denotes the set of choice sets of some $\{\Delta_{1}, \dots, \Delta_{n}\}$ derived from $\Gamma$ up to stage {\sf s-1} according to {\sf MinDab}. We denote each such choice set with ${\sf choice_{i}}(\{\Delta_{1}, \dots, \Delta_{n}\})$.
\end{definition}

The choice set of an empty set of minimal disjunction of abnormalities is empty. This notion of choice set is procedurally implemented by the rule {\sf Choice} in Figure \ref{fig:mindab}, where again each premise must be intended as the conclusion of a {\sf MinDab} rule and the side condition ensures that each $\Delta_{i}^{min}$ used as a premise is still minimal at stage {\sf s-1}. The Rule {\sf MinChoice} allows the selection of a \textit{minimal} choice set out of $\Phi(\Gamma)$, denoted by ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$.






\begin{figure}[t]
\begin{mathpar}
\infer*[right=MinDab] {\Turn {\Gamma; \cdot} {\Delta}\\ {\Delta \subset \Omega} \\ {\mbox { with no }\Delta'\subseteq \Delta \mbox{ s.t. } \Gamma; \cdot \vdash_{\sf t<s'} \Delta'}} {\TurnPrime {\Gamma; \cdot} {\Delta^{min}}}
\end{mathpar}


\begin{mathpar}
\infer*[right=UnRel]{\TurnMinusn{\Gamma;\cdot}{\Delta_{1}^{min}}\\
\ldots\\
\TurnMinusOne{\Gamma;\cdot}{\Delta_{n}^{min}}\\ {\mbox { with no }\Delta_{i}^{min}\subset \Delta_{j}^{min}} }
{\Turn{\Gamma;\cdot}{\bigcup\Delta(\Gamma)}}
\end{mathpar}

\begin{mathpar}
\infer*[right=Choice]{\TurnMinusn{\Gamma;\cdot}{\Delta_{1}^{min}}\\
\ldots\\
\TurnMinusOne{\Gamma;\cdot}{\Delta_{n}^{min}}\\ {\mbox { with no }\Delta_{i}^{min}\subset \Delta_{j}^{min}}}
{\Turn{\Gamma;\cdot}{\Phi(\Gamma)}}
\end{mathpar}



\begin{mathpar}
	\infer*[right=MinChoice]{{\TurnMinusOne{\Gamma;\cdot}{\Phi(\Gamma)}\\ {\mbox { with no }{\sf choice_{j}}(\{\Delta_{1},\ldots, \Delta_{n}\}) \in \Phi(\Gamma), \mbox{ s.t. } {\sf choice_{j}}\subset {\sf choice_{i}}}}}
	{\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}}
\end{mathpar}


\caption{Minimal Abnormal Formulas Rule and their Choice Set}\label{fig:mindab}
\end{figure}


The derivation of a minimal disjunction of abnormalities and the formation of their union and choice set is a process that occurs along with the development of the proof-tree, and the marking procedure depends on the derivation of these types of judgements. Unlike for the standard definitions of unreliable formulae and minimal choice-sets, the rules \textsc{UnRel} and \textsc{Choice} can be applied without taking every minimal disjunction of abnormalities derived up to that stage as a premise. The following propositions guarantee that this does not lead to further complications: if an abnormality occurs in $\bigcup\Delta(\Gamma)$, but is not unreliable according to $\Gamma$, then $\bigcup\Delta(\Gamma)$ was derived from a $\Delta_i$ that isn't minimal; similarly, if an abnormality occurs in some ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, but isn't verified by a minimally abnormal model, then it was derived from a $\Delta_i$ that isn't minimal. The key to these results is that the side-conditions that apply to the premises of \textsc{UnRel} and \textsc{Choice} refer to all disjunctions of abnormalities that occur unconditionally in the proof, and not only those used as premises.

\begin{proposition}\label{prop:unrel}
    If $\Turn{\Gamma;\cdot}{\bigcup\Delta(\Gamma)}$ occurs in a proof and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are still minimal at $s$, then $\bigcup\Delta(\Gamma) \subseteq \bigcup(\Lambda)$.
\end{proposition}
\noindent\textsl{Proof.}~Immediate from the fact that if $\Gamma; \cdot \vdash_{s - n  + (i-1)} \Delta_i^{min}$ is a premise used to derive $\Turn{\Gamma;\cdot}{\bigcup\Delta(\Gamma)}$ then (i) $\Delta_i^{min} \in \Lambda$, and (ii) $\Delta_i^{min}$ is still minimal at stage $s-1$.\qed

\begin{proposition}\label{prop:choice}
    If $\Turn{\Gamma;\cdot}{\Phi(\Gamma)}$ occurs in a proof and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are still minimal at $s$, then each $\phi \in \Phi(\Gamma)$ is a subset of some choice-set from $\Lambda$.
\end{proposition}
\noindent\textsl{Proof.}~If $\Gamma; \cdot \vdash_{s - n  + (i-1)} \Delta_i^{min}$ is a premise used to derive $\Turn{\Gamma;\cdot}{\Phi(\Gamma)}$, adding a premise $\Gamma; \cdot \vdash_{s - n  + (j-1)} \Delta_j^{min}$ for some $\Delta_j$ that was already derived at some stage $s - (n + m)$ would not lead to the violation of the side-condition for the application of \textsc{Choice}. Let $\Turn{\Gamma;\cdot}{\Phi'(\Gamma)}$ be the conclusion obtained by adding this premise, and note that each $\phi' \in \Phi'(\Gamma)$ can be obtained by extending a $\phi \in \Phi(\Gamma)$ with some member of $\Delta_j$.\qed

\begin{proposition}\label{prop:minchoice}
    If $\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}$ occurs in a proof and $\Lambda$ is the set of all $\Delta^{min}$ derived up to stage $s$ that are still minimal at $s$, then $\sf{choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$ is a subset of some minimal choice-set from $\Lambda$, and for each minimal choice-set from $\Lambda$ there is a ${\sf choice_{j}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$ it is a superset of.
\end{proposition}
\noindent\textsl{Proof.}~Assume $\Turn{\Gamma;\cdot}{{\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}}$ is obtained by an application of \textsc{MinChoice} to a judgement with $\Phi(\Gamma)$ as a consequent. Let $\Delta^{n+1}$ be minimal at stage $s$, and assume that $\Phi'(\Gamma)$ is derivable from judgements with $\Delta_1, \ldots \Delta_{n+1}$ as consequent. By Proposition \ref{prop:choice} we know that ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$ is a subset of one or more members of $\Phi'(\Gamma)$. Let $\psi_1, \ldots, \psi_m$ be an enumeration of the members of $\Delta^{n+1}$. (i) If some $\psi_i \in \Delta^{n+1}$ is a member of ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, then the latter is also minimal in $\Phi'(\Gamma)$. (ii) If no $\psi_i \in \Delta^{n+1}$ is a member of ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min}$, then, because $\Delta^{n+1}$ is not included in any $\Delta_i$, any ${\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\})^{min} \cup \{\psi_i\}$ must be minimal in $\Phi'(\Gamma)$.\qed

\subsection{A simple example}\label{sec:example}

We present here a simple derivation in {\sf AdaptiveND}, where $\Gamma=\{(\neg p \vee q),p,(p\rightarrow q), (p \to \neg p)\}$:



\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(\neg p \vee q)}}}{\TurnTwo {\Gamma;\cdot}{\neg p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{p}}}{\TurnFour {\Gamma; \cdot}{(p\wedge \neg p), q}}\\
{(p \wedge \neg p)\in \Omega}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{q}}
\end{mathpar}
\bigskip
%%
%Notice that at stage $2$, $B$ can be derived under the $\Gamma\Omega^{-}$ formation rule, as $\Gamma$ contains $\neg B$. Next, $\neg B$ can be also derived under the premise rule. We obtain a conflict between derivations at stages $2$ and $6$. Next we want to establish how to retract one.

In the above derivation, all judgements up to stage $4$ are obtained by {\sf minimalND} rules. Stage $5$ derives a formula on condition of the abnormality $(p\wedge \neg p)$ being false.  This corresponds to changing a multiple conclusion judgement at stage $4$ into a single conclusion one at stage $5$ by turning one of the conclusions into an adaptive condition. This move is justified by the syntactical form of the abnormality, stated as the side condition $(p \wedge \neg p)\in \Omega$ for the application of the {\sf RC} rule.



\section{Adaptive Proofs and Rules for Marking}\label{sec:marking}

In standard Adaptive Logics, one introduces strategies to tell which applications of the {\sf RC} rule should be retracted in view of the Minimal Disjunction of Abnormalities that have been derived. Adaptive Logics come with marking mechanisms that allow such retractions, according to different possible strategies. The two `standard' strategies and their rationale are \cite{batens01}:

\begin{itemize}
    \item \textit{Reliability}: once a $\bigcup(\Delta(\Gamma))$ is derived at some stage {\sf s}, \textit{every} formula derived at some prior stage {\sf s'} on the assumption that some $\phi\in \bigcup(\Delta(\Gamma))$ is false, needs to be retracted;

\item \textit{Minimal Abnormality}: once a $\Phi(\Gamma)$ is derived at some stage {\sf s}, a formula $\psi$ is marked if either (i) it is derived at some prior stage {\sf s'} on an assumption $\Theta$ which intersects with every minimal choice-set ${\sf choice_{i}}(\{\Delta_{i}, \ldots, \Delta_{n}\})\in \Phi(\Gamma)$; or (ii) for some minimal choice-set ${\sf choice_{i}}(\{\Delta_{i}, \ldots, \Delta_{n}\})\in \Phi(\Gamma)$, there is no derivation of $\psi$ on another condition $\Theta'$, such that the intersection of $\Theta'$ with ${\sf choice_{i}}$ is empty.
\end{itemize}
%
In this section, we extend {\sf AdaptiveND} with rules corresponding to both strategies by providing a proof-theoretic reconstruction of the standard marking conditions. The conclusion of a Marking Rule expresses a dead-end for the tree, and as such its content should not be used as premise for any other step. To preserve such information about retracted judgements within a proof,\footnote{This move also facilitates the practice of cutting and pasting proofs together without having to renumber the judgements.} we need to extend the standard formalisation of proofs as trees, and define adaptive proofs as sequences of proof-trees.

\begin{definition}[Proof Tree]
A well-formed {\sf AdaptiveND} tree is a finite proof tree obtained by deriving AdaptiveND judgements from other {\sf AdaptiveND} judgements where
\begin{enumerate}

\item the top leaves of the tree are instances of the {\sf Prem} rule and
\item each next step is obtained by applying one of the {\sf minimalND} proof rules or one of the {\sf AdaptiveND} proof rules.
\end{enumerate}
\end{definition}

\begin{definition}[Adaptive Proof]
An {\sf AdaptiveND} proof is a sequence of {\sf AdaptiveND} trees $T_{1}, \dots, T_{n}$ with consecutively numbered judgements across the trees.
\end{definition}

As a notational short-hand, we will sometimes include marked judgements as unused premises to incorporate dead-ends in a single proof-tree.


\subsection{Marking Rule for Reliability}\label{sec:markrel}

Reliability is the adaptive strategy that takes the most cautious interpretation of abnormalities: any formula that in view of the premises might behave abnormally, because it occurs in a minimal disjunction of abnormalities, is deemed unreliable and should not be assumed to behave normally. This means in practice that a formula derived on the assumption that $\phi$ behaves normally will be `marked' as soon as the unreliability of $\phi$ is established. The result of this marking is that $\psi$ should no longer be treated as a formula that was derived.
%In the following we shall introduce a new inference-rule that internalizes this process in {\sf AdaptiveND}.

%We shall first introduce a notion of union of all so-called unreliable formulas. To this aim, in the following we will refer to $\Sigma=\bigcup\{\Gamma,\Omega^{-}\}$, for some premises $\Gamma$ and some abnormal formulas $\Omega$ derivable from $\Gamma$:
%%
%%
%\begin{definition}[The set of unreliable formulas of $\Gamma$]
%$U_{{\sf s}}{(\Gamma)}$ is defined by the union of $\Delta\subseteq\Omega$, where $\Gamma\vdash\Delta_{1}, \dots, \Delta_{n}$ are respectively $Dab(\Delta)^{min}_{1}, \dots, Dab(\Delta)^{min}_{s}$.
%\end{definition}



In Figure \ref{fig:markR}, we define a new rule $\XBox$R that depends on the derivation of a set of unreliable formulas.
% {\color{red} If we only do reliability, the set of all disjuncts that occur in some minimal disjunction of abnormalities does not play a role. We can simply mark based on a single minimal disjunction of abnormalities (as the rule below correctly states).}
%The meaning of $\XBox \textsc{R}$ is the following: if at stage {\sf s} a minimal disjunction of abnormalities $\Delta^{min}$ is derived for $\Gamma$, while at a stage {\sf s'} a formula $\psi$ is derived from the same premise set by assuming a component of $\Delta^{min}$ false by an application of the conditional rule {\sf RC}, then at a next stage $\psi$ is marked as retracted.

\begin{figure}[h]
\begin{mathpar}
\infer*[right=$\XBox$R]{\Turn{\Gamma;\Theta^{-}}{\psi}\\ \TurnPrime{\Gamma; \cdot}{\bigcup\Delta(\Gamma)} \\ {\Theta \cap \bigcup\Delta(\Gamma) \ne \emptyset}}
{\TurnMaxPlusOneREL{\Gamma;\Theta^{-}}{\psi}}
\end{mathpar}
\caption{Marking for Reliability}\label{fig:markR}
\end{figure}


%\begin{mathpar}
%\infer*[right=$\XBox$R]{\Turn{\Sigma;\cdot}{\Delta^{min}}\\ \TurnPrime{\Sigma; \Psi^{-}}{\phi} \\ {\Psi \cap \Delta^{min}\neq \emptyset}}
%{\TurnMaxPlusOneREL{\Sigma}{\phi}}
%\end{mathpar}


\subsection{Extending the example}\label{sec:example2}

Let us now extend the example from Section \ref{sec:example} with a new branch to illustrate the derivation step obtained by the Marking Rule $\XBox$R. Let $\mathbb{D}$ be the derivation from our initial example that ended with the derivation at stage {\sf 5} of $q$ in context $\Gamma$ and with $(p \wedge \neg p)^-$ as a condition. We extend it now as follows:

%\begin{mathpar}
%\infer*[right=some]{\infer*[right=UR]{\infer*[]{\infer*[vdots =1.5 em]{} {\TurnOne {\Gamma; \cdot} {\Delta_{1}}}}{\Turn {\Gamma; \cdot}{\Delta_{n}}}}{\TurnNextn{\Gamma,\Gamma';\cdot}}}{B;Dab(\Delta)^{min}_{i}}
%\end{mathpar}

\begin{mathpar}
\infer*[right=\XBox R]{
{
\infer*[]{
\mathbb{D}\qquad}{\TurnFive{\Gamma; (p \wedge \neg p)^{-}}{q}}}\\
\infer*[right=$\wedge$ I]
{
\infer*[right=$\rightarrow$ E]{{\TurnSix{\Gamma;\cdot}{p}}\\{\TurnSeven{\Gamma; \cdot}{p\rightarrow \neg p}}}
{\TurnEight {\Gamma; \cdot}{\neg p}}\\{\TurnNine {\Gamma; \cdot}{p}}}{\TurnTen {\Gamma;\cdot}{p\wedge\neg p}}}
{\TurnMarkedElevenREL {\Gamma;\cdot }{q}}
\end{mathpar}
\bigskip

In this derivation a new abnormality is derived at stage {\sf 10}, namely the same that is assumed to be false at stage {\sf 5}. Note that we avoid the superflous inference step from the formula $(p \wedge \neg p)$ to the corresponding singleton set of unreliable formulas; moreover, it is essential that this abnormality be derived under an empty condition, i.e. under context $\Gamma;\cdot$, as explained above for the required strict condition on {\sf WL}. A difference between the Fitch-style proofs that are the existing standard for Adaptive Logics and the proposed Natural Deduction derivation style becomes evident here. In the former, a marking rule implies the need to proceed backwards on the derivation, to mark all lines that were derived on an assumption that is shown to be violated and thus can no longer be considered derived. In the latter, on the other hand, there is no need to remove formulas because the result obtained at stage {\sf 5} cannot be reused in an extension of this proof.  Instead a new derivation step is performed (stage {\sf 11}), where the conclusion $q$ is marked. Moreover, if we were ever to get again $\Gamma; (p \wedge \neg p)^{-} \vdash_{\sf i} q$, it would be obtained as the result of some new derivation $\mathbb{D}^{\prime}$ and be the conclusion at some stage ${\sf i>11}$, where an additional step would again be required to mark it at a later stage.


%
%where $\Delta_{i}$ can be any of abnormal formulas derived at any line {\sf s}. Then we want to mark as invalid the last step of this derivation where $B$ is derived, if it relies on $\Omega^{-}$ containing at least one minimal abnormal formula. In the following we abbreviate assumptions sets $\Gamma, \Gamma'$ in a tree by $\Sigma$. Then we define a marking rule:
%The example above is then extended with two additional derivation steps:
%
%\begin{mathpar}
%
%\infer*[right=$\XBox$R]
%{
%\infer*[right=$\XBox$R]
%{
%\infer*[right=RC]{
%\infer*[right=$\vee_{CL}$] {
%\infer*[right=RU] {\infer*[right=$\Gamma\Omega^{-}$] {\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{B}}} {\TurnTwo {\Gamma; \{B\wedge \neg B\}^{-}}{B}}\\ \infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg B}}} {\TurnFour {\Gamma; \{B\wedge \neg B\}^{-}}{\{B\wedge \neg B\}}}
%}
%{\TurnFive {\Gamma;\cdot}{\{\neg B\wedge \neg\neg B\};{\{B\wedge \neg B\}}}
%}
%}
%{\TurnSix {\Gamma; (\{\neg B\wedge \neg\neg B\}^{min}_{5})^{-}}{\neg B}}
%}
%{\TurnMarkedSevenREL{\Gamma; \cdot}{\neg B}}
%%\\ {\{B\wedge \neg B\}\}\in U_{{\sf 2}}(\Gamma)}
%}
%{\TurnMarkedEightREL{\Gamma; (\{B\wedge \neg B\}^{min}_{2})^{-}}{B}}
%%
%%
%%{\TurnTwo {\Gamma; \{B\wedge \neg B\}^{-}}{B}}\\ \infer*[right=PREM] {}{\TurnThree {\Gamma; \cdot}{B}}
%%
%%
%%{
%%\infer*[right=RC] {\TurnFour {\Gamma; \cdot}{\{B\wedge \neg B\};\{\neg B\wedge \neg\neg B\}}} {\TurnFive {\Gamma; \{\neg B\wedge \neg\neg B\}}{\neg B}}}
%\end{mathpar}
%
%The marking step  $11$ tell us the original lines at which the minimal unreliable formulas in view of the premise set at a given step were obtained (resp. $5$ and $2$;  we have skipped an additional line repeating the derivability of $B$ before its marking).\footnote{This is a sensible difference with the marking mechanism in place for standard adaptive logics, where the marking itself happens at the line whose content is retracted and the formulation of the set of unreliable formulas remains entirely outside of the derivation. Notice that a shortcoming of the standard marking mechanism is that it does not provide indications on the stage at which the marking is executed.}

\subsection{Marking Rules for Minimal Abnormality}\label{sec:ma}
%
%
Minimal abnormality is the marking strategy that reflects the following condition: a formula $\psi$ derived on a condition $\Theta^-$ is retracted if, either (i) every minimal choice-set includes some condition in $\Theta$, or (ii) there is a choice-set $\phi$ such that every derivation of $\psi$ is based on a condition that is shown to be violated by $\phi$. We offer rules for this strategy in Figure \ref{fig:MA}.
%To formulate appropriate marking rules, we first need to introduce a set of minimal choice sets of abnormalities:
%
%\begin{definition}
%We call $\Phi_{{\sf s}}(\Gamma)$ the set of minimal choice sets of $\{\Delta_{1}, \dots, \Delta_{n}\}$ at stage {\sf s}, i.e.\ where each of $\bigvee\Delta_{1}, \dots, \bigvee\Delta_{n}$ is derived from $\Gamma$ according to {\sf MinDab}.
%\end{definition}
%%
%Choice sets provide selections of abnormalities that might turn out to be true at a stage {\sf s}. The choice set of an empty set of minimal disjunction of abnormalities is empty. This notion of choice set is procedurally implemented by the rule {\sf Choice}, where each premise must be intended as the conclusion of a {\sf MinDab} rule.

The first marking rule $\XBox$M reflects the following condition: if a formula $\psi$ is derived at stage $s$ under an adaptive condition $\phi$ which is part of all minimal choice-sets in $\Phi(\Gamma)$ at stage $s'$, then at the next stage the formula $\psi$ can be retracted.

The second marking rule $\XBox$M2 reflects the following condition: if a formula $\psi$ is derived always under an adaptive condition that is part of the same minimal choice-set in $\Phi(\Gamma)$, then at the next stage the formula $\psi$ can be retracted. Here the adaptive condition of the conclusion should be intended as saying that the retraction applies to all stages where the derivation of $\psi$ was obtained under one of the listed conditions.

In Figure \ref{fig:exmarkMA} we illustrate two trees where markings for Minimal Abnormality are applied. In the first tree, it is shown how the derivation of a formula $\psi$ under an adaptive condition $\Theta$ is followed by a series of derivations $\Pi$ for all possible choice sets of minimally abnormal formulas; if the side condition that requires $\Theta$ to occur in each such set holds, then marking can be applied. In the second (dual) tree, it is shown how a formula $\psi$ derived under a series of adaptive conditions $\Theta, \dots, \Theta'$ is marked if all these conditions occur in at least one choice set of minimally abnormal formulas derived by a sub-tree $\Pi$.


\begin{figure}[t]

% \begin{mathpar}
% \infer*[right=Choice]{\TurnMinusn{\Gamma;\cdot}{\Delta_{1}^{min}}\\
% \ldots\\
% \Turn{\Gamma;\cdot}{\Delta_{n}^{min}}}
% {\TurnPrime{\Gamma;\cdot}{\Phi_{{\sf s}}(\Gamma)}}
% \end{mathpar}

\begin{mathpar}
{\scriptsize
\infer*[right={\scriptsize$\XBox$M}]{\Turn{\Gamma;\Theta^{-}}{\psi}\\
\TurnPrime{\Gamma; \cdot}{\Phi(\Gamma)} \\
{\Theta \cap {\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\}) \ne \emptyset \text{ for each } {\sf choice_{i}}(\{\Delta_{1},\ldots, \Delta_{n}\}) \in \Phi(\Gamma)}}
{\TurnMaxPlusOneMA{\Gamma;\Theta^{-}}{\psi}}}
\end{mathpar}


\begin{mathpar}
{\scriptsize
\infer*[right={\scriptsize$\XBox$M2}]{\Turn{\Gamma;\Theta^{-}}{\psi}\\
\ldots\\
\TurnPrime{\Gamma; \Theta'^{-}}{\psi}\\
\TurnPrimePrime{\Gamma; \cdot}{\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})}\\
{\forall(\Theta, \ldots, \Theta') \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \ne \emptyset}}
{\TurnMaxTwoPlusOneMA{\Gamma; \Theta^{-}}{\psi}}}
\end{mathpar}

\caption{Marking for Minimal Abnormality}\label{fig:MA}
\end{figure}

\begin{figure}
\begin{mathpar}
\infer*[right=$\XBox$M]{\Turn{\Gamma; \Theta^-}{\psi} \\
\infer*[right=\textsc{MinChoice}]{\infer*[right=\textsc{Choice}]{\Pi}{\TurnNext{\Gamma, \cdot}{\Phi(\Gamma)}}}
{\TurnNextNext{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta_1, \ldots, \Delta_n\})}}\\ \ldots\\ \infer*{\Pi}{\vdots}\\\Theta \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedNextNextMA{\Gamma;\Theta^-}{\psi}}
\end{mathpar}




\begin{mathpar}
\infer*[right=$\XBox$M]{\Turn{\Gamma; \Theta^-}{\psi} \\
\ldots\\
\TurnPrime{\Gamma; \Theta'^{-}}{\psi}\\
\infer*[right=\textsc{MinCh}]{\infer*[right=\textsc{Choice}]{\Pi}{\TurnPrimePrime{\Gamma, \cdot}{\Phi(\Gamma)}}}
{\TurnPrimePrimePlusOne{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta_1, \ldots, \Delta_n\})}}
\forall(\Theta\ldots \Theta') \cap \mathsf{choice_i} \ne \emptyset}
{\TurnPrimePrimePlusTwo{\Gamma;\Theta^-}{\psi}}
\end{mathpar}

\caption{Marking-trees for Minimal Abnormality}\label{fig:exmarkMA}
\end{figure}


%%, where again $\Sigma=\bigcup\{\Gamma, \Omega^{-}\}$:
%
%
% \begin{definition}[Marking for Minimal Abnormality]
% $\TurnMarked{\Gamma; \Omega^{-}}{B}$ iff for any $\Sigma \in \Phi_{{\sf s}}(\Gamma)$, $\Sigma\cap \Omega^{-}\neq \emptyset$ and for some $\Sigma \in \Phi_{{\sf s}}(\Gamma)$ there is no ${\sf s}$ such that  $\Turn{\Gamma;\Omega^{-}}{B}$ and $\Sigma\cap \Omega^{-}\neq \emptyset$.
% \end{definition}
%
%
%\begin{mathpar}
%\infer*[right=$\XBox$MA]{
%\Turn{\Gamma;\cdot}{\Delta^{min}_{i\dots n}}\\
%\TurnNext{\Gamma;\Psi^{-}}{\psi}\\
%{\Psi\cap \phi, \forall\phi \in\Phi_{\sf s}(\Gamma)\neq \emptyset}}
%{\TurnMarkedNextNextMA{\Gamma;\cdot}{\psi}}
%\end{mathpar}
%
%
%\begin{mathpar}
%\infer*[right=$\XBox$MA2]{
%\Turn{\Gamma;\cdot}{\Delta^{min}_{i\dots n}}\\
%\TurnNext{\Gamma;\Psi^{-}}{\psi}\\
%\TurnNext{\Gamma;\Xi^{-}}{\psi}\\
%{\Xi,\Psi\cap \phi \in\Phi_{\sf s}(\Gamma)\neq \emptyset}}
%{\TurnMarkedNextNextMA{\Gamma;\cdot}{\psi}}
%\end{mathpar}
%
%
%%
% This rule tells us that an {\sf Adaptive ND}-formula at stage {\sf s} is \textit{not} marked when:
% \begin{itemize}
% \item either $\Phi_{\sf s}(\Gamma)=\emptyset$;
% \item or $\exists\Xi\in\Phi_{\sf s}(\Gamma)$ such that $\Xi\cap\Omega^{-}=\emptyset$ possibly according to some ordering criterion;
% and $\forall \Xi\in\Phi_{\sf s}(\Gamma)$, the formula at hand is derived under some condition that does not intersect with any of the minimal choice sets.
% \end{itemize}
%
% In other words, in case $\Delta_{i}$ on the left-hand side of $\vdash$ at some stage {\sf s} has a non-empty-intersection with some $\Xi\in\Phi_{\sf s}(\Gamma)$, then the interpretation of $\Xi$ falsifies the formula derived at {\sf s}. This marking rule is further specified by establishing that not every $\Xi\in\Phi_{\sf s}(\Gamma)$ is treated in the same way. Hence, choice sets can be ordered according to some criterion (lexicographic, by counting, and so on) and relative to an inclusion relation that orders derivable formulas in view of the abnormality degree of their conditions. The most simple such ordering is done in function of the \textit{number} of abnormalities that are held as conditions of a given formula: the less the cardinality of such a set, the less abnormal the model (and hence, the more preferred the derivation).
%
%Consider the above example again and the construction of minimal choice sets:
%
%$$
%\begin{array}{l}
%\Phi_{\sf 2}(\Gamma):=\{B\wedge \neg B\}\\
%\Phi_{\sf 6}(\Gamma):=\{\neg B\wedge \neg\neg B\}\\
%\Phi_{\sf 6}(\Gamma):=\{\{B\wedge \neg B\},\{\neg B\wedge \neg\neg B\}\}\\
%\end{array}
%$$
%%
%By this ordering one would consider abnormalities at stage $2$ less serious than those at stage $6$. But notice how this result will be obtained stage by stage. Hence the previous derivation would proceed in view of: first falsifying formulas obtained under condition $\Phi_{\sf 2}(\Gamma)$; then once $\Phi_{\sf 6}(\Gamma):=\{\neg B\wedge \neg\neg B\}$ is obtained, by re-enabling the former and marking the latter )where $\Pi$ abbreviates the derivation as above:
%
%
%\begin{mathpar}
%\infer*[right=UR]
%{
%\infer*[right=$\XBox$MA]
%{
%\infer*[right=RC]{
%\infer*[right=$\vee_{CL}$] {
%\infer*[right=$\XBox$MA]{
%\infer*[right=RU]{\Pi} {\TurnFour {\Gamma; (\{B\wedge \neg B\}\in \Phi_{{\sf 2}}(\Gamma))^{-}}{\{B\wedge \neg B\}}}
%}
%{\TurnMarkedFiveMA{\Gamma; (\{B\wedge \neg B\})^{-}}{B}}
%}
%{\TurnSix {\Gamma;\cdot}{\{\neg B\wedge \neg\neg B\};{\{B\wedge \neg B\}}}}
%}
%{\TurnSeven {\Gamma; (\{\neg B\wedge \neg\neg B\}\in \Phi_{{\sf 6}}(\Gamma))^{-}}{\neg B}}}
%{\TurnMarkedEightMA{\Gamma; \cdot}{\neg B}}
%}
%{\TurnNine{\Gamma;(\{\neg B\wedge \neg\neg B\})^{-}}{B}}
%\end{mathpar}
%%
%%
%%\infer*[right=RU] {\infer*[right=$\Gamma\Omega^{-}$] {\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{B}}} {\TurnTwo {\Gamma; \{B\wedge \neg B\}^{-}}{B}}\\ \infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg B}}}
%
%In this case the formula $B$ derived at stage $2$ is first marked, when the only derived $\Delta$ is the one on whose condition $B$ is derived; later, it get unmarked because the condition on which it is derived is safe when one considers $\Delta_{6}=\{\neg B\wedge \neg\neg B\}$ and it can be further derived under such condition.

\subsection{An example with $\bigvee(\Delta^{min})$-selection}\label{subsec:sel-example}


The previous example is rather simple, in that it shows a formula that is first derived under an adaptive condition (referring to an abnormal formula assumed to be false), and then retracted after that condition is validated again.

Let us consider now a slightly more complex example. We want to show a situation in which a disjunction of two abnormalities can be derived: accordingly, there might be more than one formula to be marked. Let us start with a premise set $\Gamma=\{(p \vee r),\neg p,(p\vee q), \neg q, (\neg p \rightarrow q)\}$. Now consider the following derivation, dubbed $\mathbb{D}$:



\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(p \vee r)}}}{\TurnTwo {\Gamma;\cdot}{p, r}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg p}}}{\TurnFour {\Gamma; \cdot}{(p\wedge \neg p), r}}\\
{(p \wedge \neg p)\in \Omega}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{r}}
\end{mathpar}
\bigskip

At stage {\sf 4} a disjunction of an abnormality with $r$ is derived, and by {\sf RC} at stage {\sf 6} the formula $r$ is derived alone, assuming the relevant abnormality to be false. Consider now a second derivation, dubbed $\mathbb{D}^{'}$:

\begin{mathpar}
\infer*[right=UnRel]{
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnSix {\Gamma; \cdot}{(p \vee q)}}}{\TurnSeven {\Gamma;\cdot}{p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnEight {\Gamma; \cdot}{\neg p}}}{\TurnNine {\Gamma; \cdot}{(p\wedge \neg p), q}}\\
\infer*[]{\phantom{xxxx}}{\TurnTen{\Gamma; \cdot}{\neg q}}}{\TurnEleven {\Gamma;\cdot}{((p \wedge \neg p), (q\wedge \neg q))^{min}}}}
{\TurnTwelve {\Gamma; \cdot}{\bigcup(\{p\wedge \neg p,q\wedge \neg q\})}}
\end{mathpar}
\bigskip

Here the previously derived abnormality $(p \wedge \neg p)$ is unconditionally derived in disjunctive form with a new abnormality $(q \wedge \neg q)$ at stage {\sf 11}, where the latter is obtained by $\wedge$I from stages {\sf 7-9}. If we join now the two branches $\mathbb{D,D^{\prime}}$ to form $\mathbb{E}$, we can apply the marking-rule (where $\bigcup(\Delta(\Gamma))$ is the set $\{(p \wedge \neg p), (q\wedge \neg q)\}$):

\begin{mathpar}
\infer*[right=\XBox]{
{\infer*[]{\mathbb{D}}{\TurnFive {\Gamma;(p \wedge \neg p)^{-}}{r}}}\\{\infer*[]{\mathbb{D^{\prime}}}{\TurnTwelve {\Gamma; \cdot}{\bigcup(\{p\wedge \neg p,q\wedge \neg q\})}}}\\{(p \wedge \neg p)\in \bigcup(\Delta(\Gamma))}
}{\TurnMarkedThirteenREL{\Gamma; (p\wedge \neg p)^{-}}{r}}
\end{mathpar}
\bigskip

At stage {\sf 13} the formula $r$ is no longer valid, because its adaptive condition is in the set of unreliable formulas derived at stage \textsf{12}. Now we can provide a further extension of this derivation dubbed $\mathbb{D^{\prime\prime}}$:

\begin{mathpar}
\infer*[right=$\wedge$ I]{
\infer*[right=$\rightarrow$ I]{
\infer*[right=PREM]{\phantom{xxx}}{\TurnFourteen{\Gamma;\cdot}{\neg p}}\\ \infer*[right=PREM]{\phantom{xxx}}{\TurnFifteen{\Gamma;\cdot}{\neg p\rightarrow q}}}
{\TurnSixteen{\Gamma; \cdot}{q}}\\{
\infer*[right=PREM]{\phantom{xx}}{\TurnSeventeen{\Gamma;\cdot}{\neg q}}}}{\TurnEighteen {\Gamma; \cdot}{(q \wedge \neg q)}}

\end{mathpar}
\bigskip

$\mathbb{D^{\prime\prime}}$ derives a single abnormality at stage {\sf 18}, for which, as before, we skip the redundant step of deriving the singleton set of unreliable formulas. This also means that if we obtain a copy of derivation $\mathbb{D}$, where each step is re-numbered consecutively, and join it to $\mathbb{D^{\prime\prime}}$ and $\mathbb{E}$, it is possible to deduce $r$ anew with $(p \wedge \neg p)$ as its adaptive condition and accordingly leave this judgement unmarked at stage {\sf 20}:


\begin{mathpar}
\infer*[right=RC*]{
\infer*[]{\mathbb{E}}{\TurnMarkedThirteenREL{\Gamma; \cdot}{r}}\\
\infer*[]{\mathbb{D^{\prime\prime}}}{\TurnEighteen {\Gamma; \cdot}{(q \wedge \neg q)^{min}}}\\
\infer*[]{\mathbb{D}}{\TurnNineteen {\Gamma;\cdot}{(p\wedge \neg p), r}}
%\\{(p\wedge \neg p)\in \Omega}
}
{\TurnTwenty {\Gamma;(p \wedge \neg p)^{-}}{r}}

\end{mathpar}
\bigskip

where $*$ is the side condition that $(p\wedge \neg p)\in \Omega$. If the derivation is no longer extended, the formula $r$ can be considered finally derived.

\subsection{Another Example}

In the following example, we do not derive a set of unreliable formulas when there is a single minimal disjunction of abnormalities. Let

\begin{align*}
\Gamma = & \{p \vee q, \neg q,\\
& (q \wedge \neg q) \vee (r \wedge \neg r), (q \wedge \neg q) \to (r \wedge \neg r),\\
& (q \wedge \neg q) \vee (s \wedge \neg s), (q \wedge \neg q) \to (s \wedge \neg s)\}\\
\end{align*}

Consider first the derivation $\mathbb{D}$:

\begin{mathpar}
\infer*[right=RC]{
\infer*[right=$\wedge$I] {
\infer*[right=$\vee$E]{
\infer*[right=PREM] {\phantom{xx}} {\TurnOne {\Gamma; \cdot}{(p \vee q)}}}{\TurnTwo {\Gamma;\cdot}{p, q}}\\
\infer*[right=PREM] {\phantom{xx}} {\TurnThree {\Gamma; \cdot}{\neg q}}}{\TurnFour {\Gamma; \cdot}{(q \wedge \neg q), p}}\\
{(q \wedge \neg q)\in \Omega}}{\TurnFive {\Gamma;(q \wedge \neg q)^{-}}{p}}
\end{mathpar}

Which we extend as follows to form $\mathbb{E}$:

\begin{mathpar}
\infer*[right=\XBox] {
    {
\infer*[] {\mathbb{D}} {\TurnFive {\Gamma;(q \wedge \neg q)^{-}}{p}}
    }\\
{
\infer*[right=$\vee$E] {
\infer*[right=PREM] {~} {\TurnSix {\Gamma} {(q \wedge \neg q) \vee (r \wedge \neg r)}}}
                    {\TurnSeven {\Gamma;\cdot}{((q \wedge \neg q), (r \wedge \neg r))^{min}}}}\\
{(q \wedge \neg q)\in \Delta^{min}}
                    }
{\TurnMarkedEightREL{\Gamma; (q \wedge \neg q)^-}{p}}
\end{mathpar}

We then construct $\mathbb{F}$ to show that a shorter disjunction of abnormalities can be derived:

\begin{mathpar}
\infer*[right=Cr]{
    \infer*[right=$\to$E]{\infer*[right=$\vee$E] {\infer*[right=PREM] {~} {\TurnNine {\Gamma; \cdot} {(q \wedge \neg q) \vee (r \wedge \neg r)}}} {\TurnTen {\Gamma; \cdot} {(q \wedge \neg q), (r \wedge \neg r)}} \\ \infer*[right=PREM] {~} {\TurnEleven {\Gamma; \cdot} {(q \wedge \neg q) \to (r \wedge \neg r)}}}
    {\TurnTwelve {\Gamma; \cdot} {(r \wedge \neg r), (r \wedge \neg r)}}
                 }
                 {
\TurnThirteen {\Gamma; \cdot} {r \wedge \neg r}
                 }

\end{mathpar}

As the previous example, we put these branches together (and re-use a renumbered copy of $\mathbb{D}$) to to obtain $\mathbb{G}$ and re-derive $p$ on condition $(q \wedge \neg q)^-$:

\begin{mathpar}
\infer*[right=RC*]{
    \infer*[]{\mathbb{E}}{\TurnMarkedEightREL{\Gamma; (q \wedge \neg q)^-}{p}}\\
    \infer*[]{\mathbb{F}}{\TurnThirteen {\Gamma; \cdot}{(r \wedge \neg r)^{min}}}\\
\infer*[]{\mathbb{D}}{\TurnFourteen {\Gamma;\cdot}{(q\wedge \neg q), p}}
}
{\TurnFifteen {\Gamma;(q \wedge \neg q)^{-}}{p}}

\end{mathpar}

But consider then the following variant of $\mathbb{E}$, denoted by $\mathbb{E}^*$:

\begin{mathpar}
\infer*[right=\XBox] {
    {
\infer*[] {\mathbb{G}} {\TurnFifteen {\Gamma;(q \wedge \neg q)^{-}}{p}}
    }\\
{
\infer*[right=$\vee$E] {
\infer*[right=PREM] {~} {\TurnSixteen {\Gamma} {(q \wedge \neg q) \vee (s \wedge \neg s)}}}
                    {\TurnSeventeen {\Gamma;\cdot}{((q \wedge \neg q), (s \wedge \neg s))^{min}}}}\\
{(q \wedge \neg q)\in \Delta^{min}}
                    }
{\TurnMarkedEighteenREL{\Gamma; (q \wedge \neg q)^-}{p}}
\end{mathpar}

and then a variant $\mathbb{F}^*$ of $\mathbb{F}$:
%{\color{red}some renumbering to be done}:

\begin{mathpar}
\infer*[right=Cr]{
    \infer*[right=$\to$E]{\infer*[right=$\vee$E] {\infer*[right=PREM] {~} {\TurnNineteen {\Gamma; \cdot} {(q \wedge \neg q) \vee (s \wedge \neg s)}}} {\TurnTwenty {\Gamma; \cdot} {(q \wedge \neg q), (s \wedge \neg s)}} \\ \infer*[right=PREM] {~} {\TurnTwentyOne {\Gamma; \cdot} {(q \wedge \neg q) \to (s \wedge \neg s)}}}
    {\TurnTwentyTwo {\Gamma; \cdot} {(s \wedge \neg s), (s \wedge \neg s)}}
                 }
                 {
\TurnTwentyThree {\Gamma; \cdot} {s \wedge \neg s}
                 }

\end{mathpar}

which can once more be used to re-derive $p$:

\begin{mathpar}
\infer*[right=RC*]{
    \infer*[]{\mathbb{E}^*}{\TurnMarkedEighteenREL{\Gamma; (q \wedge \neg q)^-}{p}}\\
    \infer*[right=UnRel]{
    \infer*[]{\mathbb{F}}{\TurnThirteen {\Gamma; \cdot}{(r \wedge \neg r)}}\\
    \infer*[]{\mathbb{F}^*}{\TurnTwentyThree {\Gamma; \cdot}{(s \wedge \neg s)}}}{\TurnTwentyFour {\Gamma;\cdot}{\bigcup(\{r \wedge \neg r, s \wedge \neg s\})}}\\
\infer*[]{\mathbb{D}}{\TurnFourteen {\Gamma;\cdot}{(q\wedge \neg q), p}}
}
{\TurnTwentyFive {\Gamma;(q \wedge \neg q)^{-}}{p}}

\end{mathpar}


\subsection{Reconstruction of linear adaptive proofs}
Before we consider the question of final derivability, which is needed to relate what is provable in dynamic proofs to a semantic consequence relation, we first show that every proof in \textsf{AdaptiveND} can, with the help of the numbering of the judgements, be mapped onto an (albeit somewhat redundant) adaptive proof in a linear format. This relates what can be derived at a stage in an \textsf{AdaptiveND}-proof to what can be derived at a stage in a linear proof.

We illustrate the procedure by reconstructing the linear proof that corresponds to the example form Sections \ref{sec:example} and \ref{sec:example2}:
\begin{center}
    \begin{tabular}{cllcl}
        (1) & $\neg p \vee q$ & Prem & $\emptyset$\\
        (2) & $\bigvee \{\neg p, q\}$ & $\vee$E, (1) & $\emptyset$\\
        (3) & $p$ & Prem & $\emptyset$\\
        (4) & $\bigvee \{p \wedge \neg p, q\}$ & $\wedge$ I, (2, 3) & $\emptyset$\\
        (5) & $q$ & RC, (4) & $\{p\}$ & $\XBox^{10}$\\
        (6) & $p$ & Prem & $\emptyset$\\
        (7) & $p \to \neg p$ & Prem & $\emptyset$\\
        (8) & $\neg p$ & $\to$ E, (6, 7) & $\emptyset$\\
        (9) & $p$ & Prem & $\emptyset$\\
        (10) & $p \wedge \neg p$ & $\wedge$ I, (8, 9) & $\emptyset$\\
    \end{tabular}
\end{center}
In this proof, the application of $\vee$E on line (2) is based on the representation of the disjunctive comma by a `super-imposed' classical disjunction (a device that effectively plays the same role in adaptive logic, see \cite[\S 2.2, 2.7]{Strasser:AdaptiveLogicsForDefeasibleReasoning:}), whereas the application of $\wedge$I is valid in virtue of the \textbf{CLuN}-validity of $\neg p \vee q, p \vdash (p \wedge \neg p) \vee q$ which warrants the application of the unconditional rule (with empty conditions). The final marking is not added as a separate line, but is instead added in the fifth place on line 5 and labelled with the number of the line or stage at which the relevant abnormality was derived.

As illustrated in a second example based on the proof from \S \ref{subsec:sel-example}, there is no guarantee that the translation of an {\sf AdaptiveND} proof into a linear adaptive proof will contain all the markings required by the latter.
\begin{center}
    \begin{tabular}{cllcl}
        (1) & $p \vee r$ & Prem & $\emptyset$\\
        (2) & $\bigvee \{p, r\}$ & $\vee$E, (1) & $\emptyset$\\
        (3) & $\neg p$ & Prem & $\emptyset$\\
        (4) & $\bigvee \{p \wedge \neg p, r\}$ & $\wedge$ I, (2, 3) & $\emptyset$\\
        (5) & $r$ & RC, (4) & $\{p\}$ & $\XBox^{??}$\\
        (6) & $p \vee q$ & Prem & $\emptyset$\\
        (7) & $\bigvee(p, q)$ & $\vee$E, (6)& $\emptyset$\\
        (8) & $\neg p$ & Prem & $\emptyset$\\
        (9) & $\bigvee(p \wedge \neg p, q)$ & $\wedge$ I, (7, 8) & $\emptyset$\\
        (10) & $\neg q$ & Prem & $\emptyset$\\
        (11) & $\bigvee(p \wedge \neg p, q \wedge \neg q)$ & $\wedge$ I, (9, 10) & $\emptyset$
    \end{tabular}
\end{center}
Indeed, the above proof is a straightforward translation of the tree-form proof up to stage {\sf 11}, but the marking of line 5, which should be added to the linear proof once $\bigvee(p \wedge \neg p, q \wedge \neg q)$ is derived, cannot be simply in virtue of a mechanical translation procedure. This is because in the standard proof-format marking is governed by a definition, which simply stipulates when a line is marked, whereas in \textsf{AdaptiveND} marking is governed by rules and therefore requires the execution of additional inferential steps.\footnote{See also footnote 10 of \cite{BDVM08} for a discussion of this distinction.} To show that proofs in \textsf{AdaptiveND} correctly capture the adaptive dynamics, we will therefore have to show that such ``missing markings'' can always be obtained by a further extension of a proof.

In the next section we complete our system with the required meta-theoretical analysis needed to define derivability at stage and final derivability.

\section{Derivability}\label{sec:meta}

In the example from the previous section we have illustrated how the marking condition establishes a dynamic derivability relation, which allows to derive formulas and retract them. Whenever a certain formula is derived on some $\phi\in \Delta^{min}$ adaptive condition, it might still be marked afterwards according to $\XBox R$. Consequently, a judgement of the form $\Turn{\Gamma; \Theta^-}{\psi}$ only expresses what is derived at a stage. This gives us the notion of derivability at a stage:

\begin{definition}[Derivability at stage]
%$\Turn{\Gamma; \phi^{-}}{\psi}$ iff at ${\sf s}$ it is not the case that $\TurnMarked{\Gamma}{\psi}$.
A formula $\psi$ is derived at stage {\sf s}  iff $\TurnPrime{\Gamma; \phi^{-}}{\psi}$ where $\mathsf{s'\leq s}$ and it is not the case that $\TurnPrimePrimeMarked{\Gamma; \cdot}{\psi}$ for some $\mathsf{s'\leq s''\leq s}$.
%$\Turn{\Gamma; \cdot}{\phi,\psi}$
\end{definition}

A more stable notion of derivability, called \emph{final derivability}, holds when marking is no longer possible. This notion is customarily defined with a reference to possible extensions of a proof.\footnote{``A is finally derived from $\Gamma$ on line $i$ of a proof at stage $s$ iff (i) $A$ is the second element of line $i$, (ii) line $i$ is not marked at stage $s$, and (iii) every extension of the proof in which line $i$ is marked may be further extended in such a way that line $i$ is unmarked.'' \cite[229]{batens07}} By only taking finite premise-sets into consideration, we can pursue a more explicit characterisation of final derivability.
% Because our left-hand side of the $\vdash$ sign, hence our ``premise set'', includes the $\vee_{\sf CL}$ connective for $Dab$-formulas, it is in principle possible to generate trees where some standard adaptive consequences (i.e. consequences of a corresponding AL in standard format) would be first marked and then never get unmarked.\footnote{These results are based on the generic format for Adaptive logics presented in \cite{strasservandeputte12}. A different approach that allows insertion of lines in proof trees is given for the Standard Format in \cite{batens2009}.}
%
%[HERE AN EXAMPLE]
%
%To get around this problem and provide a stable notion of derivability
To this aim, one requires that the stage {\sf s} at which a formula $\phi$ is derived remains unmarked in all the extensions of the derivation tree which can be obtained by using all \textit{relevant} abnormalities as adaptive conditions. This relevance criterion is essential if one wants to guarantee finite surveyability of the proof tree to establish whether a formula is never marked (again). We define therefore a set of \textit{abnormalities relevant to $\Gamma$}. To do so we first identify the union set of all subformulas of the premise set $\Gamma$:


%\begin{definition}
%$Sf(\phi)=\{\psi \mid \psi$ is a subformula of $ \phi\}$
%\end{definition}


\begin{definition}[Subformulas of the premise set]
$\Sf(\Gamma)=\bigcup_{\phi \in \Gamma} \{\psi \mid \psi$ is a subformula of $ \phi\}$.
\end{definition}


From $\Sf(\Gamma)$ we then construe all the possible abnormalities that can be obtained from its members:

\begin{definition}[Abnormalities relevant to the premise set]
$\Omega(\Gamma)=\{\psi\wedge \neg \psi \in \Omega \mid \psi\in \Sf(\Gamma)\}$.
\end{definition}
For \textsf{AdaptiveND} and \textbf{CLuN$^r$} the requirement that all $\psi\wedge \neg \psi$ should be in $\Omega$ is trivially satisfied. This condition becomes mandatory when $\Omega$ is based on a restricted logical form; e.g. when abnormalities are contradictions of the form $\psi \wedge \neg \psi$ with $\psi$ atomic. In that case, our definition of $\Omega(\Gamma)$ is co-extensive with the more basic $\{\psi \wedge \neg \psi \mid \psi \in \At(\Gamma)\}$.
%\begin{definition}
%Let $P$ be a proof-tree of {\sf AdaptiveND}. We call $P^{\sf ext}$ the complete extension of $P$ at stage {\sf s} if $P^{\sf ext}$ extends $P$ by a possibly infinite number of derivation steps such that
%
%\begin{enumerate}
%\item if $\Turn{\Gamma;\cdot}{Dab(\Delta)}$, then for all derivable $\Delta^{'}\subseteq\Delta$, it holds $\TurnPrime{\Gamma;\cdot}{Dab(\Delta^{'})}$, for some $s'<s$;
%\item if $\Turn{\Gamma;\Delta^{-}}{\phi}$, then $\Turn{\Gamma;\Delta'^{-}}{\phi}$
%\end{enumerate}
%\end{definition}
%
\begin{theorem}\label{thm:subform}
    If $\bigvee(\Delta)$ is a minimal disjunction of abnormalities derivable from $\Gamma$, then $\Delta \subseteq \Omega(\Gamma)$.
\end{theorem}
\noindent\textsl{Proof.}~We consider the possible ways of deriving a minimal disjunction of abnormalities by examining the structure of the proof-rules of \textsf{minimalND}.
\begin{enumerate*}
    \item Applying ($\vee$I) (or \textsc{Wr}) can never result in a minimal disjunction of abnormalities. This excludes all proof-rules that can be used to deduce a judgement with a formula on the right that isn't yet a formula or sub-formula in one of the judgements it relies on.
    \item A formula of the form $\phi \wedge \neg \phi$ can be derived on the right of the turn-style if it is already a sub-formula of some premise, or the result of ($\wedge$I).
    \item If $\phi \wedge \neg \phi$ is the result of ($\wedge$I), each of its conjuncts should be derivable. We focus on the proof-paths to formulae of the form $\neg \phi$, of which there are four:
        \begin{enumerate*}
            \item $\neg \phi$ is a premise;
            \item $\neg \phi$ can be obtained by ($\wedge$E) from some $\neg \phi \wedge \psi$ on the right;
            \item $\neg \phi$ can be obtained by ($\to$E) from some $\psi \to \neg \phi$ on the right;
            \item $\neg \phi$ can be obtained by ($\neg$I) from $\phi$ on the left.
        \end{enumerate*}
        Cases (a-c) imply that $\neg \phi$ should be a positive part of a previously derived formula on the right, and hence $\phi$ should be a negative part of that formula. By induction over the length of proofs (with the rule {\sf PREM}  as the base-case), these three cases can be retraced to $\phi$ being a negative part of some premise.
    \item Case (d) requires the presence or deduction of some $\phi$ on the left, either because the left-hand side is of the form $\Gamma, \phi$ and thus the result of applying (\textsf{WL}), or because it is of the form $\Gamma$ with $\phi \in \Gamma$. In each of these cases, this can never lead to a judgement where (i) the left-side consists only of the premise-set, and (ii) the right-side has no more formulae than before the application of ($\neg$I). This implies that case (d) cannot lead to the deduction of a minimal disjunction of abnormalities.
\end{enumerate*}
From (3) and (4) it follows that every abnormality that occurs in a minimal disjunction of abnormalities obtains from a formula that occurs as the negative part of some premise. A fortiori, this means it should be formed from a member of $\Omega(\Gamma)$.\qed

The focus on positive and negative parts of formulae goes back to \cite{Sch60}, and was previously used for the development of goal-directed proof-strategies for adaptive logics \cite{Batens:LogiqueAnalyse:2001}. The fact that we should pay attention to all negative parts of the premises should also be obvious in view of the semantics for \textbf{CLuN}, as the truth-value of negative formulae does not need to depend on the truth-values of its sub-formulae.

Theorem \ref{thm:subform} helps us to characterise finite proof-trees to decide whether a formula is finally derived by identifying the abnormalities derivable in view of the syntactical form of the premises. But it can also be seen as a \textbf{CLuN}-specific variant of the \emph{Derivability Adjustment Theorem} from \cite{batens07}. This result can be stated in multiple-conclusion form as follows:
\[
    \Gamma \vdash_{\mathbf{ULL}} \phi \text{ iff } \Gamma \vdash_{\mathbf{LLL}} \phi, \Delta \text{ for some finite } \Delta \subset \Omega
\]
Or yet it can be seen as a \textbf{CLuN}-alternative of a result from \cite{Beall:TheReviewOfSymbolicLogic:2011} that relates $\mathbf{LP}^{+}$, the multiple-conclusion extension of \textbf{LP}, and $\mathbf{CPL}^{+}$, the multiple-conclusion extension of classical logic:

\[
    X \models_{\mathbf{CPL}}^+ Y \text{ iff } X \models_{\mathbf{LP}}^+ Y \cup \iota(X) \tag{LP/CPL}\label{beall}
\]
with $\iota(X) = \{p \wedge \neg p : p \in \At(X)\}$.

Here, we do not use such connections to bridge different approaches to classical recapture, but instead rely on it to  introduce the notion of a \textit{complete proof-tree} with respect to derivable relevant disjunction of abnormalities:

\begin{definition}[Completeness relative to relevant abnormalities]
Let $P$ be an {\sf AdaptiveND} proof. We say that $P$ is complete relative to $\Omega(\Gamma)$ at stage {\sf s} if for every derivable $\bigvee(\Delta^{min})$ with $\Delta\subseteq\Omega(\Gamma)$ there is an $\mathsf{s' < s}$ such that $\TurnPrime{\Gamma;\cdot}{\bigvee(\Delta^{min})}$.

%
%\begin{enumerate}
%\item  it holds $\TurnPrime{\Gamma;\cdot}{Dab(\Delta^{'})}$, for some $s'<s$;
%\item there is a $\psi$ such that $\TurnPrimePrime{\Gamma;\Delta'^{-}}{\psi}$, for some $s<s''<s$.
%\end{enumerate}
\end{definition}


\begin{definition}[Completeness relative to marking]
Let $P$ be an {\sf AdaptiveND} proof. We say that $P$ is complete relative to marking
iff
\begin{description}
    \item[Rel] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$ occurring in a tree $T_{i}\in P$, if some $\bigcup \Delta(\Gamma)$ such that $\bigcup \Delta(\Gamma) \cap \Theta \neq \emptyset$ is derivable at that stage, then there is a tree $T_{j>i}\in P$ such that ends with $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.
    \item[MinAb1] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$, if every derivable $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$ intersects with $\Theta$, then there is a tree $T_{j>i}\in P$ such that ends with $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.
%    \item[MinAb2] {\color{red} this will break down. The second clause for MinAb-marking does not only depend on knowing all the minimal choice-sets, but also depends on having obtained all the alternative conditional deductions of $\phi$.}
    \item[MinAb2] for every $\Gamma; \Theta^{-}\vdash_{{\sf s}} \phi$, if for every other derivable $\Gamma; \Theta'^{-}\vdash_{{\sf s}} \phi$, some derivable
     $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$ intersects with each $\Theta, \dots, \Theta'$, then there is a tree $T_{j>i}\in P$ such that ends with $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$.

\end{description}


% if $\Gamma; \cdot\vdash_{{\sf s'>s}} \Delta^{min}$ with $\Theta^{-}\cap \Delta^{min}\neq \emptyset$ and no $\Gamma; \cdot\vdash_{{\sf s''>s'}} \Delta'^{min}$ with $\Delta'^{min}\subset \Delta^{min}$, there is a tree $T_{j>i}\in P$ such that $\Gamma; \Theta^{-}\vdash_{{\sf t\XBox}} \phi$ occurs in it.

\end{definition}
%\marginpar{This is adequate for reliability, but almost certainly not for minimal abnormality.}

%By the first requirement all possible $Dab(\Delta)^{min}_{s}$ have been derived for  {\sf s}; and by the second requirement every formula is derived under the minimal conditions (hence any relevant condition for the marking has been obtained already).

We can now formulate our notion of final derivability:

\begin{definition}[Final Derivability]\label{def:finalder}
A formula $\psi$ is finally derived $\TurnChecked{\Gamma;\phi^{-}}{\psi}$ if it is derived at the final stage {\sf s} of an abnormality and marking complete proof.
\end{definition}
%
%The definition guarantees final derivability for any derived formula {\color{red}whose adaptive condition is not minimal. WHAT DOES THIS MEAN?}
%Notice that the second requirement might not be satisfied at any finite stage {\sf s}, hence it might be guaranteed only at meta-theoretical level.



\begin{theorem}
$\TurnChecked{\Gamma;\phi^{-}}{\psi}$ in {\sf AdaptiveND} if and only if there is a final derivation of $\phi$ from $\Gamma$ in a standard linear adaptive proof.
\end{theorem}

\begin{proof}

\begin{itemize}
%\marginpar{Replaced $\mathbb{P}$ by $\mathbf{P}$ to distinguish from the previous uses of mathbb-fonts in the examples.}
\item[$\rightarrow$] By assumption $\psi$ is finally derived, therefore there is an abnormality and marking complete proof $P$ in which it is derived. Then $P$, by definition, contains unconditional judgements for every deducible minimal disjunction of abnormalities in $\Omega(\Gamma)$ and $\psi$ is not contained in any of those. Now consider the translation $\mathbf{P}$ of $P$ in a linear adaptive proof: the same minimal disjunctions of abnormalities as in $P$ are also unconditionally derived in $\mathbf{P}$. By Theorem \ref{thm:subform}, this implies that all minimal disjunctions of abnormalities are unconditionally derived in $\mathbf{P}$, and further extensions of the proof cannot lead to additional unconditionally derived minimal disjunctions of abnormalities. Because $P$ contains all possible markings, every formula which is marked in $P$ will be marked in $\mathbf{P}$, and $\mathbf{P}$ will therefore be in accordance with the standard marking definitions. As $\psi$ is not in any adaptive conditions of $P$ which induces a marking, it will be derived in $\mathbf{P}$ as well. Moreover, $\psi$ will be finally derived in $\mathbf{P}$ because further extensions would not lead to newly derived minimal disjunctions of abnormalities, and would not lead to additional marking either.

\item[$\leftarrow$] Assume that $\psi$ is finally derived in some linear adaptive proof $\mathbf{P}$; then


\begin{itemize}
  \item[for \textbf{CluN}$^{R}$]  $\Gamma\vdash_{\mathbf{CluN}}\psi\vee \phi$ with $\phi\cap U(\Gamma)=\emptyset$. By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\phi}$. By applying {\sf RC}, we obtain a judgement
  $\TurnNext{\Gamma;\phi^{-}}{\psi}$. If at some later stage {\sf s'} an abnormality complete proof is obtained, any $\bigcup \Delta (\Gamma)$ derived thereafter will, by Proposition \ref{prop:unrel} be a sub-set of $U(\Gamma)$. Consequently, any application of $\XBox$R to a judgement with condition $\phi^-$ would from that point also require $\phi \in \bigcup \Delta (\Gamma)$. Since this would contradict our assumption that $\phi\cap U(\Gamma)\neq\emptyset$, no such marking can be applied.

  \item[for \textbf{CluN}$^{M}$] $\Gamma \vdash_{\mathbf{CluN}} \psi\vee \Theta$, with $\Theta \subseteq \Omega(\Gamma)$ and
% $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \cap \Theta = \emptyset$
%or every $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$,  . Then

\begin{enumerate}
% {\color{gray}\item there is a $\phi\in \Theta$ such that $\phi$ is not a member of every
%  $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$. By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\phi}$ and by applying {\sf RC}, we obtain a judgement $\TurnNext{\Gamma;\phi^{-}}{\psi}$. If $\TurnMarkedMA{\Gamma;\phi^{-}}{\psi}$ in some abnormality and marking complete proof $P$ by $\XBox$M, then
% $\phi\in \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$, $\forall \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$, which contradicts the assumption, hence $\TurnChecked{\Gamma;\phi^{-}}{\psi}$ in $P$.}

\item either for every $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$ we have
\[
    \Theta \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) = \emptyset.
\]
By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\Theta}$, and by repeated applications of \textsc{RC}, we obtain the judgement $\TurnNextm{\Gamma; \Theta^-}{\psi}$. Let $t \geq s+m$ be a stage at which this proof is marking and abnormality-incomplete, and assume, \emph{for reductio} that it ends with a judgement $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$. Consequently, this judgement must occur as the final node of a marking-tree for minimal abnormality:

\begin{figure}[h!]
\begin{mathpar}
{\footnotesize
\infer*[right={\tiny$\XBox$M}]{\Turn{\Gamma; \Theta^-}{\psi} \\
\infer*[right={\tiny\textsc{MinChoice}}]{\infer*[right={\tiny\textsc{Choice}}]{\Pi}{\TurnNext{\Gamma, \cdot}{\Phi'(\Gamma)}}}
{\TurnNextNext{\Gamma; \cdot}{\mathsf{choice_1}(\{\Delta'_1, \ldots, \Delta'_m\})}}\\ \ldots\\ \infer*{\Pi}{\vdots}\\\Theta \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}}}
\end{mathpar}
\end{figure}

To complete our argument, we rely on the fact that every choice-set derivable from $\Phi'(\Gamma)$ is used as a premise for the application of the marking-rule, but we do not need to assume that $\Phi'(\Gamma)$ is identical to $\Phi(\Gamma)$. Because $\Phi'(\Gamma)$ is derived at a stage of the proof that is already abnormality-complete, we know that $\{\Delta'_1, \ldots, \Delta'_m\} \subseteq \{\Delta_1, \ldots, \Delta_n\}$ (each $\Delta'_i$ is equal to some $\Delta_i$, but not vice-versa). Proposition \ref{prop:minchoice} then guarantees that each $\mathsf{choice_j}(\{\Delta_1, \ldots, \Delta_n\})$ is a superset of some $\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})$. Consequently, since the marking at stage $t$ required that $\Theta \cap \mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\}) \ne \emptyset$ for each choice-set in $\Phi'(\Gamma)$, it should also hold that $\Theta \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \ne \emptyset$ for each choice-set in $\Phi(\Gamma)$, which contradicts our initial assumption.

\item or for every $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$, if $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \cap \Theta \ne \emptyset$, there is a $\Theta'$ such that $\Gamma \vdash_{\mathbf{CluN}} \psi \vee \Theta'$ with
\[
    \Theta' \cap \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) = \emptyset.
\]
By Theorem \ref{thm:clun} there are provable {\sf AdaptiveND} judgements $\Turn{\Gamma;\cdot}{\psi,\Theta}$ and $\TurnPrime{\Gamma;\cdot}{\psi,\Theta'}$ for $\Theta$ and each such $\Theta'$. As in the previous case, by repeated applications of \textsc{RC} we can derive corresponding $\TurnNextm{\Gamma;\Theta^-}{\psi}$ and $\TurnNextmPrime{\Gamma;\Theta'^-}{\psi}$. Let $t$ be a stage at which this proof is extended to an abnormality and marking complete proof, and assume, \emph{for reductio}, that it ends with a judgement $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$. Consequently, this judgement must occur as the final node of a marking-tree for minimal abnormality:

\begin{figure}[!ht]
\begin{mathpar}
{\footnotesize
\infer*[right={\tiny$\XBox$M}]{\Turn{\Gamma; \Theta^-}{\psi} \\
\ldots\\
\TurnPrime{\Gamma; \Theta'^{-}}{\psi}\\
\infer*[right={\tiny\textsc{MinCh}}]{\infer*[right={\tiny\textsc{Choice}}]{\Pi}{\TurnPrimePrime{\Gamma, \cdot}{\Phi'(\Gamma)}}}
{\TurnPrimePrimePlusOne{\Gamma; \cdot}{\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})}}
\forall(\Theta\ldots \Theta') \cap \mathsf{choice_i} \ne \emptyset}
{\TurnMarkedMAflex{t}{\Gamma;\Theta^-}{\psi}}}
\end{mathpar}
\end{figure}
But then, by the same reasoning as above, Proposition \ref{prop:minchoice} entails that if $\mathsf{choice_i}(\{\Delta'_1, \ldots, \Delta'_m\})$ intersects with every condition $\Theta, \ldots, \Theta'$, it must also intersect with some $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}) \in \Phi(\Gamma)$, which contradicts our initial assumption.

% {\color{gray}\item or for every $\phi\in \Theta$, $\phi$ is not in some $\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})\in \Phi(\Gamma)$. By Theorem \ref{thm:clun} there is a provable {\sf AdaptiveND} judgement $\Turn{\Gamma;\cdot}{\psi,\Theta}$ and by applying {\sf RC}, we obtain  $\TurnNext{\Gamma;\phi^{-}}{\psi}$ and $\TurnNextNext{\Gamma;\phi'^{-}}{\psi}$, for every $\phi,\phi'\in \Theta$. If
% $\TurnMarkedMA{\Gamma;\phi^{-}}{\psi}$ or $\TurnMarkedMA{\Gamma;\phi'^{-}}{\psi}$ in some abnormality and marking complete proof $P$ by $\XBox$M2, then there is a $(\mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\}))\in \Phi(\Gamma)$ such that $\phi,\phi'\in \mathsf{choice_i}(\{\Delta_1, \ldots, \Delta_n\})$ which contradicts the assumption, hence $\TurnChecked{\Gamma;\phi^{-}}{\psi}$ in $P$, for every $\phi \in \Theta$.}
\end{enumerate}
Therefore, $\TurnMarkedMAflex{t}{\Gamma;\Theta^{-}}{\psi}$ cannot occur at any stage $t$ of a proof that is abnormality-complete at $t$.
\end{itemize}

\end{itemize}
\end{proof}

\section{Concluding remarks}\label{sec:recap}
To conclude, we would like to highlight certain distinctive features of the proposed calculus, and briefly discuss how these features can be used to reconsider the question of classical recapture. As we see it, the defeasible reasoning-forms formalised in our \textsf{AdaptiveND} system have three primary virtues:
\begin{enumerate*}
    \item they are formulated in a tree-format that forces one to state all information used in an inference-step explicitly, and this restricts the reliance on global features of a proof to a minimum (e.g. when checking that a disjunction of abnormalities is minimal);
    \item the multiple-conclusion format leads to a transparent connection between the restricted inference-rules that are valid in \textsf{minimalND} (i.e. the lower-limit-logic) and their use as a premise of the conditional rule;
    \item the explicit individuation of abnormalities as a sub-type of the well-formed formulae.
\end{enumerate*}
The explicit connection between multiple-conclusions and defeasible inferences brings a recent disagreement over the problem of classical recapture in the logic \textbf{LP} into focus.\footnote{See \cite[18ff]{Allo:Theoria:2015} for a more detailed reconstruction of this debate.} In several papers, Graham Priest has explicitly endorsed the adaptive approach to classical recapture. To that effect, he has proposed his own \emph{minimally inconsistent} \textbf{LP}: an adaptive logic based on a stronger paraconsistent logic (but without a detachable implication) and the minimal abnormality strategy \cite{GP:LPm}. This approach has been criticised by JC Beall, another prominent defender of the logic \textbf{LP}, on the ground that any all-purpose logic should at all cost prevent one to step from truth to falsehood \cite{Beall01072012}. This is a task that cannot in general be fulfilled by an adaptive logic, and indeed a task we shouldn't impute on adaptive logic in the first place \cite{Priest01102012}. By contrast, Beall's preferred take on classical recapture is that it should be handled with extra-logical means. The multiple-conclusion extensions of classical logic and \textbf{LP} already mentioned in the previous section provide one of the formalisms in which this idea can be made precise, since (\ref{beall}) can be seen as a minimalist expression of how paraconsistent logics like \textbf{LP} incorporate classical logic in a restricted form. Given the central role of similar multiple-conclusion judgements in \textsf{AdaptiveND}, results like (\ref{beall}) should really be understood as agnostic between the different strategies for classical recapture. Indeed, whereas Beall advocates the view that \textbf{LP$^+$} only presents us with logically viable options, these same options are the motor behind any defeasible inference mechanism that allows one to favour one of them in the first place. One advantage of \textsf{AdaptiveND} is that it doesn't artificially widen the gap between defeasible and multiple-conclusion accounts of classical recapture.

The formal approach taken in the development of \textsf{AdaptiveND} signals another crucial departure from the terms in which the Priest/Beall debate is carried out, namely a departure concerning the individuation of abnormalities. Within the adaptive logic tradition, abnormalities are understood as formulae of a specific logical form, and the abnormality of models (e.g. how inconsistent they are) is measured relative to the abnormal formulas it verifies. When compared to the road taken by minimally inconsistent \textbf{LP}, this has certain advantages \cite{Batens:Synthese:2000}. The same syntactical approach to abnormalities is integrated in \textsf{AdaptiveND} through the identification of a class of formulae of type $\Omega$ and the need to state membership of $\Omega$ when the conditional rule is applied. This approach is more general in the sense that it doesn't have to appeal to semantic concepts like \emph{gluts} in its formulation, and can explain how we step from logical options to defeasible inferences by only taking into account the logical form of the premises at hand. From a proof-theoretic viewpoint, this could be seen as a more explicit approach, whereas from the standpoint of the broader adaptive logic programme it is definitely more flexible.

%\begin{definition}
%$\TurnADND{\Gamma}{\phi}$ iff $\TurnChecked{\Gamma;\Omega^{-}}{\phi}$
%\end{definition}
%
%Notice that not every finally derivable formula of {\sf AdaptiveND} is derivable at stage. In particular, for the case of {\sf AdaptiveND}, where the disjunction used to construct $Dab$-formulas is explicitly admitted in the premise set by the extension of a given $\Gamma$ with $Dab(\Delta)^{-}$ on the left-hand side of $\vdash$, one can conceive of cases in which a finally derivable $\phi$ is not derivable at any finite stage of a proof-tree.\footnote{For an example see Strasser PHD, ch.2.8.}

%\section{Structural Rules}
%
%
%\begin{theorem}[Restricted Weakening]
%{\sf AdaptiveND} satisfies Weakening:
%
%\begin{enumerate}
%\item If $\Turn{\Gamma;\cdot}{\phi_{1}}$, then $\Turn{\Gamma, \phi_{2};\cdot}{\phi_{1}}$, with $\phi_{2}$ fresh.
%\item If $\Turn{\Gamma; \Omega^{-}}{\phi_{1}}$, then $\Turn{\Gamma, \phi_{2}; \Omega^{-}}{\phi_{1}}$, with $\phi_{2}$ fresh, and there is no $\phi_{3}\in\Gamma$ such that $\phi_{2},\phi_{3}\in Dab(\Delta)$.
%\item If $\Turn{\Gamma;\Omega^{-}}{\phi}$, then $\Turn{\Gamma;\Omega^{-},\Delta_{i}}{\phi}$, iff $\Delta_{i}\supseteq\Delta_{j}$, for every $\Delta_{j}\in \Omega$.
%\end{enumerate}
%\end{theorem}
%
%\begin{proof}
%The first and second item are justified simply by $\Gamma$-formation rule with the appropriate restrictions on, respectively, freshness of the formula and $Dab$-formation by $\vee_{\sf CL}$-rule. The third item works as weakening on $\Omega^{-}$ by $\Gamma\Omega^{-}$-formation rule, with the appropriate restriction on minimal $Dab$-formulas in the already present set of abnormal premises.
%\end{proof}
%
%
%\begin{theorem}[Contraction]
%{\sf AdaptiveND} satisfies Contraction:
%
%\begin{enumerate}
%\item If $\Turn{\Gamma, \phi_{1}, \phi_{1};\cdot}{\phi_{2}}$, then $\Turn{\Gamma, \phi_{1};\cdot}{\phi_{2}}$.
%\item If $\Turn{\Gamma, \phi_{1}, \phi_{1}; \Omega^{-}}{\phi_{2}}$, then $\Turn{\Gamma, \phi_{1};\Omega^{-}}{\phi_{2}}$.
%\item If $\Turn{\Gamma;\Omega^{-}, \Delta_{i}, \Delta_{i}}{\phi}$, and $\Delta_{i}\supseteq\Delta_{j}$, for every $\Delta_{j}\in \Omega$,  then $\Turn{\Gamma;\Omega^{-},\Delta_{i}}{\phi}$.
%\end{enumerate}
%\end{theorem}
%
%\begin{proof}
%First and second item by induction on formulas (where $\Omega^{-}$ being empty is irrelevant). For the third item, it is essential that the sets of abnormal formulas involved in the contraction operation be irrelevant with respect to minimal $Dab$-formulas formation
%\end{proof}
%
%
%
%\begin{theorem}[Exchange]
%{\sf AdaptiveND} satisfies exchange up to context well-formedness:
%
%\begin{enumerate}
%\item If $\Turn{\Gamma, \phi_{1}, \phi_{2}; \cdot}{\phi_{3}}$, then $\Turn{\Gamma, \phi_{2}, \phi_{1};\cdot}{\phi_{3}}$.
%\item If $\Turn{\Gamma, \phi_{1}, \phi_{2}; \Omega^{-}}{\phi_{3}}$, then $\Turn{\Gamma, \phi_{2}, \phi_{1};\Omega^{-}}{\phi_{3}}$.
%\item If $\Turn{\Gamma; \Omega^{-}, \Delta_{i}, \Delta_{j}}{\phi}$, and $\Delta_{i}, \Delta_{j}\supseteq\Delta_{k}$, for every $\Delta_{k}\in \Omega$,  then\\ $\Turn{\Gamma; \Omega^{-},\Delta_{j}, \Delta_{i}}{B}$.
%\end{enumerate}
%\end{theorem}
%
%\begin{proof}
%First and second item by induction on formulas (where $\Omega^{-}$ being empty is irrelevant). For the third item, it is again essential that the sets of abnormal formulas involved in the exchange operation be irrelevant with respect to minimal $Dab$-formulas formation.
%\end{proof}

\bibliographystyle{plain}
\bibliography{primiero}

\end{document}
